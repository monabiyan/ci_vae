{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhV9PPiQKgSg"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77NKQyeKwIj"
   },
   "source": [
    "# Download CI-VAE, other necessary packages and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cAuVLcUNETr7"
   },
   "outputs": [],
   "source": [
    "! rm -rf ci_vae\n",
    "! rm bb.pt\n",
    "! rm bb_residuals.pkl\n",
    "! rm df_reconstructed.csv\n",
    "! rm df_reconstructed_decoder.csv\n",
    "! rm residuals.pdf\n",
    "! rm results_dict.pkl\n",
    "! rm df_latent.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "18S0saDPLp0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ci_vae'...\n",
      "remote: Enumerating objects: 386, done.\u001b[K\n",
      "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
      "remote: Compressing objects: 100% (102/102), done.\u001b[K\n",
      "remote: Total 386 (delta 67), reused 112 (delta 38), pack-reused 245\u001b[K\n",
      "Receiving objects: 100% (386/386), 47.85 MiB | 12.35 MiB/s, done.\n",
      "Resolving deltas: 100% (223/223), done.\n",
      "Requirement already satisfied: umap-learn in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.51.0rc1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.23.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.5.2)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.33 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (0.33.0+1.g022ab0f)\n",
      "Requirement already satisfied: setuptools in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (49.2.0.post20200714)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mnabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/monabiyan/ci_vae.git\n",
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kzwk1I17VAQx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from ci_vae import ivae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Input Data\n",
    "\n",
    "Here we are generating a sample data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9HIiKcw_PCm5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "np.random.seed(0)\n",
    "n_samples=1000\n",
    "n_features = 5\n",
    "df_XY=pd.DataFrame(data = np.random.normal(0,1, size=(n_samples, n_features)), columns = ['A','B','C','D','E'])\n",
    "# Cell Type\n",
    "df_XY['Y']=list(np.random.randint(2, size=n_samples))\n",
    "# Healthy to Cancer (State 0 to State 1)\n",
    "df_XY['YY']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY\n",
    "\n",
    "##############################################################   \n",
    "df_XY.shape\n",
    "df_XY.head()\n",
    "df_XY.to_csv('df_XY.csv',index=False)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can load your custom data. But make sure the data is normalized and has to have one column 'Y' representing celltype with numbers {0,1,2,3,...} and 'YY' representing states {0,1} commonly 0 for healthy and 1 for cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000099139</th>\n",
       "      <th>ENSG00000269028</th>\n",
       "      <th>ENSG00000225840</th>\n",
       "      <th>ENSG00000166710</th>\n",
       "      <th>ENSG00000198804</th>\n",
       "      <th>ENSG00000167996</th>\n",
       "      <th>ENSG00000198727</th>\n",
       "      <th>ENSG00000130066</th>\n",
       "      <th>ENSG00000122862</th>\n",
       "      <th>ENSG00000231500</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000213639</th>\n",
       "      <th>ENSG00000156508</th>\n",
       "      <th>ENSG00000070756</th>\n",
       "      <th>ENSG00000063046</th>\n",
       "      <th>ENSG00000135968</th>\n",
       "      <th>ENSG00000198888</th>\n",
       "      <th>ENSG00000090104</th>\n",
       "      <th>ENSG00000147604</th>\n",
       "      <th>Y</th>\n",
       "      <th>YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.142779</td>\n",
       "      <td>-0.232256</td>\n",
       "      <td>-0.510939</td>\n",
       "      <td>-0.577439</td>\n",
       "      <td>-0.500473</td>\n",
       "      <td>-0.363331</td>\n",
       "      <td>0.762842</td>\n",
       "      <td>-0.144618</td>\n",
       "      <td>-0.099679</td>\n",
       "      <td>0.158276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506358</td>\n",
       "      <td>0.450110</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>-0.108881</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>0.396830</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.137531</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.143735</td>\n",
       "      <td>-0.275919</td>\n",
       "      <td>-0.589931</td>\n",
       "      <td>-0.682540</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>-0.401349</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>-0.199836</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.097916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352869</td>\n",
       "      <td>1.375910</td>\n",
       "      <td>-0.250887</td>\n",
       "      <td>-0.136809</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.206215</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.140000</td>\n",
       "      <td>-0.369762</td>\n",
       "      <td>-0.523606</td>\n",
       "      <td>-0.449770</td>\n",
       "      <td>-0.172116</td>\n",
       "      <td>-0.444671</td>\n",
       "      <td>0.207073</td>\n",
       "      <td>-0.199836</td>\n",
       "      <td>-0.215888</td>\n",
       "      <td>-0.030897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278965</td>\n",
       "      <td>0.449511</td>\n",
       "      <td>-0.378783</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>-0.064337</td>\n",
       "      <td>-0.131046</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.295940</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.144317</td>\n",
       "      <td>-0.274877</td>\n",
       "      <td>-0.641722</td>\n",
       "      <td>-0.171661</td>\n",
       "      <td>-0.151140</td>\n",
       "      <td>-0.421610</td>\n",
       "      <td>-0.072508</td>\n",
       "      <td>-0.077442</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>-0.217924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.694927</td>\n",
       "      <td>-0.877009</td>\n",
       "      <td>-0.359729</td>\n",
       "      <td>-0.070910</td>\n",
       "      <td>-0.055551</td>\n",
       "      <td>-0.075671</td>\n",
       "      <td>-0.292218</td>\n",
       "      <td>-0.326305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.145319</td>\n",
       "      <td>-0.238402</td>\n",
       "      <td>-0.533023</td>\n",
       "      <td>-0.715673</td>\n",
       "      <td>-0.110848</td>\n",
       "      <td>-0.447279</td>\n",
       "      <td>1.318530</td>\n",
       "      <td>-0.096442</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.138932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.805869</td>\n",
       "      <td>1.290240</td>\n",
       "      <td>0.064943</td>\n",
       "      <td>-0.017243</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>-0.091883</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.164941</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.145319</td>\n",
       "      <td>-0.168706</td>\n",
       "      <td>-0.115022</td>\n",
       "      <td>-0.881533</td>\n",
       "      <td>-0.480855</td>\n",
       "      <td>-0.455569</td>\n",
       "      <td>0.555907</td>\n",
       "      <td>-0.147432</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.228157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.806292</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>-0.199385</td>\n",
       "      <td>-0.098722</td>\n",
       "      <td>-0.085594</td>\n",
       "      <td>0.204897</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.386954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.141199</td>\n",
       "      <td>-0.119064</td>\n",
       "      <td>-0.517628</td>\n",
       "      <td>-0.573159</td>\n",
       "      <td>0.101448</td>\n",
       "      <td>-0.459228</td>\n",
       "      <td>1.009727</td>\n",
       "      <td>-0.197269</td>\n",
       "      <td>-0.213005</td>\n",
       "      <td>0.179193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547844</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>-0.348224</td>\n",
       "      <td>-0.134901</td>\n",
       "      <td>-0.074083</td>\n",
       "      <td>0.171302</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.138767</td>\n",
       "      <td>-0.277695</td>\n",
       "      <td>-0.405305</td>\n",
       "      <td>-0.850223</td>\n",
       "      <td>-0.282392</td>\n",
       "      <td>-0.437460</td>\n",
       "      <td>0.505407</td>\n",
       "      <td>-0.177188</td>\n",
       "      <td>-0.222749</td>\n",
       "      <td>0.328891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263950</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>0.190735</td>\n",
       "      <td>-0.136810</td>\n",
       "      <td>-0.080291</td>\n",
       "      <td>-0.141683</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.017815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.143859</td>\n",
       "      <td>-0.341971</td>\n",
       "      <td>-0.518436</td>\n",
       "      <td>-0.412762</td>\n",
       "      <td>-0.309528</td>\n",
       "      <td>-0.435564</td>\n",
       "      <td>-0.496546</td>\n",
       "      <td>-0.199700</td>\n",
       "      <td>-0.214390</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.806049</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-0.386615</td>\n",
       "      <td>-0.137079</td>\n",
       "      <td>-0.085386</td>\n",
       "      <td>0.450344</td>\n",
       "      <td>-0.099152</td>\n",
       "      <td>-0.233118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.145248</td>\n",
       "      <td>-0.238803</td>\n",
       "      <td>-0.592046</td>\n",
       "      <td>-0.444801</td>\n",
       "      <td>0.858058</td>\n",
       "      <td>-0.442341</td>\n",
       "      <td>-0.169489</td>\n",
       "      <td>0.075389</td>\n",
       "      <td>-0.221574</td>\n",
       "      <td>0.063786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620067</td>\n",
       "      <td>0.097621</td>\n",
       "      <td>-0.306195</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>-0.412636</td>\n",
       "      <td>-0.055385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000099139  ENSG00000269028  ENSG00000225840  ENSG00000166710  \\\n",
       "0        -0.142779        -0.232256        -0.510939        -0.577439   \n",
       "1        -0.143735        -0.275919        -0.589931        -0.682540   \n",
       "2        -0.140000        -0.369762        -0.523606        -0.449770   \n",
       "3        -0.144317        -0.274877        -0.641722        -0.171661   \n",
       "4        -0.145319        -0.238402        -0.533023        -0.715673   \n",
       "5        -0.145319        -0.168706        -0.115022        -0.881533   \n",
       "6        -0.141199        -0.119064        -0.517628        -0.573159   \n",
       "7        -0.138767        -0.277695        -0.405305        -0.850223   \n",
       "8        -0.143859        -0.341971        -0.518436        -0.412762   \n",
       "9        -0.145248        -0.238803        -0.592046        -0.444801   \n",
       "\n",
       "   ENSG00000198804  ENSG00000167996  ENSG00000198727  ENSG00000130066  \\\n",
       "0        -0.500473        -0.363331         0.762842        -0.144618   \n",
       "1         0.166552        -0.401349         0.022919        -0.199836   \n",
       "2        -0.172116        -0.444671         0.207073        -0.199836   \n",
       "3        -0.151140        -0.421610        -0.072508        -0.077442   \n",
       "4        -0.110848        -0.447279         1.318530        -0.096442   \n",
       "5        -0.480855        -0.455569         0.555907        -0.147432   \n",
       "6         0.101448        -0.459228         1.009727        -0.197269   \n",
       "7        -0.282392        -0.437460         0.505407        -0.177188   \n",
       "8        -0.309528        -0.435564        -0.496546        -0.199700   \n",
       "9         0.858058        -0.442341        -0.169489         0.075389   \n",
       "\n",
       "   ENSG00000122862  ENSG00000231500  ...  ENSG00000213639  ENSG00000156508  \\\n",
       "0        -0.099679         0.158276  ...        -0.506358         0.450110   \n",
       "1        -0.223550         0.097916  ...        -0.352869         1.375910   \n",
       "2        -0.215888        -0.030897  ...        -0.278965         0.449511   \n",
       "3        -0.223550        -0.217924  ...        -0.694927        -0.877009   \n",
       "4        -0.223550         0.138932  ...        -0.805869         1.290240   \n",
       "5        -0.223550         0.228157  ...        -0.806292         0.124092   \n",
       "6        -0.213005         0.179193  ...        -0.547844         0.006449   \n",
       "7        -0.222749         0.328891  ...        -0.263950         0.185749   \n",
       "8        -0.214390         0.029604  ...        -0.806049        -0.051556   \n",
       "9        -0.221574         0.063786  ...        -0.620067         0.097621   \n",
       "\n",
       "   ENSG00000070756  ENSG00000063046  ENSG00000135968  ENSG00000198888  \\\n",
       "0         0.085160        -0.108881        -0.089319         0.396830   \n",
       "1        -0.250887        -0.136809        -0.089319         0.026834   \n",
       "2        -0.378783         0.279081        -0.064337        -0.131046   \n",
       "3        -0.359729        -0.070910        -0.055551        -0.075671   \n",
       "4         0.064943        -0.017243        -0.089319        -0.091883   \n",
       "5        -0.199385        -0.098722        -0.085594         0.204897   \n",
       "6        -0.348224        -0.134901        -0.074083         0.171302   \n",
       "7         0.190735        -0.136810        -0.080291        -0.141683   \n",
       "8        -0.386615        -0.137079        -0.085386         0.450344   \n",
       "9        -0.306195         0.053265        -0.089319        -0.037091   \n",
       "\n",
       "   ENSG00000090104  ENSG00000147604  Y   YY  \n",
       "0        -0.447621        -0.137531  0  0.0  \n",
       "1        -0.447621        -0.206215  0  0.0  \n",
       "2        -0.447621        -0.295940  0  0.0  \n",
       "3        -0.292218        -0.326305  0  0.0  \n",
       "4        -0.447621        -0.164941  0  0.0  \n",
       "5        -0.447621        -0.386954  0  0.0  \n",
       "6        -0.447621         0.012308  0  0.0  \n",
       "7        -0.447621        -0.017815  0  0.0  \n",
       "8        -0.099152        -0.233118  0  0.0  \n",
       "9        -0.412636        -0.055385  0  0.0  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XY=pd.read_csv(\"/Users/mnabian/Desktop/CIVAE_Studies/colorectal/final_data_colorectal.csv\")\n",
    "df_XY=df_XY.drop(columns=['sample_id'])\n",
    "df_XY.to_csv('df_XY.csv',index=False)\n",
    "df_XY.head(n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPVcV9thL8SP"
   },
   "source": [
    "# Set Necessary Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KAVb-irtbIpc"
   },
   "outputs": [],
   "source": [
    "model_init=True\n",
    "model_tobe_trained=True\n",
    "save_address=\"bb\"\n",
    "kl_coef = 0.0001\n",
    "reconst_coef = 2\n",
    "classifier_coef = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRIfHjpSMKF5"
   },
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ll0w2DunMJei"
   },
   "outputs": [],
   "source": [
    "obj1 = ivae.IVAE(df_XY = df_XY,\n",
    "               latent_size = 8,\n",
    "               reconst_coef = reconst_coef,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "if model_init:\n",
    "    obj1.model_initialiaze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT-AB_8-M-tD"
   },
   "source": [
    "## See The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJXlsM8Uk2Ry",
    "outputId": "129babee-fc0e-48c5-9262-08404a9b89c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVAE_ARCH(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=10, out_features=16, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=20, out_features=30, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=3, bias=True)\n",
      "    (1): Dropout(p=0.8, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSj9WT_mNHNl"
   },
   "source": [
    "## See the Initialized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUFyVcu4ldFH",
    "outputId": "5a470060-626d-4602-dcf4-78d601febd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0415, -0.0471, -0.1371,  0.1272,  0.1784,  0.0297, -0.0964, -0.0250,\n",
      "         -0.1777,  0.1647, -0.1488, -0.0486, -0.0393, -0.0739, -0.1395, -0.1684,\n",
      "         -0.1745,  0.0462,  0.0883, -0.1749, -0.0924,  0.1239,  0.1443,  0.0314,\n",
      "         -0.0618, -0.0804,  0.1079,  0.0100, -0.0245,  0.1214],\n",
      "        [-0.0794,  0.1034,  0.1363,  0.0853, -0.0648, -0.0258,  0.0531, -0.1611,\n",
      "         -0.0747,  0.1045, -0.0658, -0.0079, -0.0901,  0.1483,  0.0404, -0.1319,\n",
      "          0.1546, -0.0405, -0.1298, -0.0360,  0.0193, -0.1135,  0.0524, -0.0876,\n",
      "          0.1447,  0.0922, -0.0300,  0.0162,  0.1110, -0.0238],\n",
      "        [ 0.1550, -0.0783, -0.0555,  0.1220, -0.0622, -0.0383,  0.0039,  0.1797,\n",
      "          0.0954, -0.0969, -0.1114, -0.0136, -0.0591, -0.0983, -0.0139, -0.1521,\n",
      "          0.0718, -0.0064, -0.0990, -0.1238,  0.1425,  0.1214, -0.0681,  0.0891,\n",
      "         -0.1212,  0.0493,  0.1180, -0.0557,  0.1522,  0.0273],\n",
      "        [ 0.0264, -0.0429,  0.1387, -0.0064, -0.0788,  0.0129,  0.0760, -0.1255,\n",
      "         -0.0950,  0.1213,  0.1417,  0.1283, -0.0489, -0.1742,  0.0030,  0.0200,\n",
      "          0.0925, -0.0441,  0.0190,  0.1470,  0.0085,  0.1606, -0.0323,  0.0369,\n",
      "         -0.1439, -0.0863, -0.0192, -0.0426, -0.0079, -0.1012],\n",
      "        [-0.0551,  0.1732, -0.0553, -0.0792,  0.0229,  0.0375, -0.0124,  0.1495,\n",
      "         -0.0999, -0.0916,  0.1186, -0.0619, -0.0249,  0.0881,  0.1572,  0.1040,\n",
      "         -0.1324, -0.0575,  0.0450,  0.1651, -0.0972, -0.0038,  0.1174,  0.0520,\n",
      "          0.0411,  0.1822,  0.1701, -0.1640, -0.0120,  0.1763],\n",
      "        [-0.1801, -0.0083, -0.1119,  0.0554, -0.0199,  0.0777,  0.1186, -0.1024,\n",
      "         -0.1202, -0.0700,  0.0914, -0.1050, -0.1191,  0.1206, -0.0024,  0.1786,\n",
      "          0.0908, -0.1654,  0.0995, -0.1301, -0.0595, -0.0796, -0.0294,  0.0743,\n",
      "          0.0572,  0.0402,  0.1281,  0.1542,  0.0810, -0.0583],\n",
      "        [ 0.1589,  0.1741,  0.1191, -0.0162,  0.0903,  0.0315,  0.1653,  0.0713,\n",
      "          0.0346,  0.0677, -0.1261, -0.0344, -0.0335, -0.0369, -0.1029,  0.1498,\n",
      "         -0.0269,  0.1776,  0.1398,  0.1340,  0.1771,  0.1420, -0.1298, -0.1668,\n",
      "          0.1185,  0.1158,  0.0927, -0.1201, -0.0209, -0.1450],\n",
      "        [-0.0170,  0.0546,  0.0251,  0.1677, -0.0360, -0.0414, -0.0169, -0.0827,\n",
      "          0.0723, -0.0735,  0.1561,  0.0845,  0.1096,  0.1462,  0.0842,  0.0327,\n",
      "          0.0334,  0.0238,  0.0236,  0.0334, -0.0284,  0.0129, -0.1485, -0.0172,\n",
      "         -0.0504, -0.1517,  0.0459, -0.0406, -0.1222,  0.0482],\n",
      "        [ 0.0448, -0.1678, -0.0986,  0.1643, -0.0637,  0.1781, -0.1036,  0.1561,\n",
      "          0.1291, -0.1518,  0.0224, -0.0028, -0.0637,  0.0680, -0.1653, -0.0762,\n",
      "         -0.0376,  0.0735, -0.0609,  0.1670,  0.1145, -0.0050, -0.1224, -0.0498,\n",
      "          0.0500, -0.0733,  0.0121,  0.1697, -0.1581, -0.0613],\n",
      "        [ 0.0176,  0.1003, -0.0241,  0.1172,  0.1596, -0.0210, -0.0848,  0.0055,\n",
      "         -0.0029,  0.1524, -0.0080, -0.0728,  0.0034,  0.1220, -0.1018, -0.1821,\n",
      "          0.1152,  0.1000, -0.1562,  0.0816,  0.1504,  0.1448, -0.0519,  0.0296,\n",
      "          0.0913, -0.0156, -0.0034, -0.1684, -0.0796,  0.1528],\n",
      "        [-0.0053,  0.0502, -0.0673, -0.1476,  0.0385,  0.1750,  0.0167,  0.0744,\n",
      "          0.1313, -0.1439,  0.0787,  0.0876, -0.0100, -0.0228, -0.0131,  0.0241,\n",
      "          0.1426, -0.0366,  0.0639,  0.0987, -0.0709, -0.0940, -0.0599,  0.1384,\n",
      "          0.0651, -0.0305, -0.1158,  0.0814,  0.0847,  0.1212],\n",
      "        [ 0.1693,  0.0267, -0.0993,  0.1625, -0.1161,  0.0937,  0.1300, -0.1045,\n",
      "          0.0431,  0.0791,  0.1580,  0.1118,  0.0199, -0.0838, -0.0203,  0.1173,\n",
      "          0.0337, -0.1797, -0.0765,  0.1417,  0.1239,  0.0320,  0.0688,  0.0535,\n",
      "          0.1546, -0.1744, -0.1516, -0.0767, -0.1345,  0.0535],\n",
      "        [ 0.0350, -0.1067,  0.1794, -0.0526, -0.0650, -0.0450,  0.1746, -0.1777,\n",
      "         -0.0753,  0.0787, -0.1298, -0.0082,  0.0309,  0.0205, -0.0617, -0.1184,\n",
      "          0.0201,  0.1656,  0.0747, -0.0820,  0.1689, -0.1375,  0.1353, -0.1456,\n",
      "          0.0295, -0.1437,  0.1176,  0.0637,  0.1298,  0.1633],\n",
      "        [-0.1074,  0.1116,  0.0626, -0.0542, -0.1332,  0.0144,  0.0071, -0.0491,\n",
      "          0.0295, -0.1038,  0.1700, -0.0803, -0.0412,  0.1604, -0.1621,  0.0248,\n",
      "          0.0614, -0.0727, -0.1360,  0.0554,  0.1547, -0.0072,  0.1022, -0.1127,\n",
      "         -0.0742,  0.1482, -0.0110,  0.0615, -0.1792,  0.0540],\n",
      "        [ 0.1003,  0.1157,  0.0428,  0.0893, -0.0805,  0.1698, -0.0240, -0.0066,\n",
      "          0.1258, -0.1148,  0.1044, -0.1156, -0.0514,  0.0074, -0.1261, -0.1821,\n",
      "          0.1359,  0.0105,  0.1508,  0.0242, -0.0157,  0.1007,  0.1306, -0.0696,\n",
      "         -0.0925,  0.0443,  0.0557,  0.1798,  0.1573,  0.1215],\n",
      "        [-0.1256, -0.0905, -0.0395, -0.0854,  0.1357, -0.1706, -0.0566, -0.0861,\n",
      "         -0.0314,  0.1537,  0.0004,  0.1346,  0.0004, -0.1256, -0.1282, -0.1543,\n",
      "         -0.0461,  0.1522, -0.1738,  0.1078, -0.0239,  0.1112,  0.0501, -0.1806,\n",
      "         -0.1747,  0.0549,  0.0516, -0.1313,  0.1484,  0.0411],\n",
      "        [ 0.0573, -0.0938, -0.0409, -0.0646, -0.0303,  0.0065, -0.1781,  0.0658,\n",
      "          0.1681, -0.1671, -0.0879,  0.0047,  0.1054,  0.0451,  0.0779,  0.1364,\n",
      "         -0.0619,  0.1572,  0.0360,  0.0830, -0.1177, -0.1658, -0.1258, -0.0795,\n",
      "         -0.1095, -0.1008,  0.0963, -0.0303, -0.1460, -0.0759],\n",
      "        [ 0.0684, -0.0925, -0.1396,  0.0476,  0.1816, -0.0489, -0.0277, -0.0328,\n",
      "         -0.1703, -0.0414, -0.1650, -0.1358, -0.0709, -0.0155, -0.0258,  0.1372,\n",
      "          0.1772,  0.1773, -0.1098, -0.1444,  0.0697,  0.1504,  0.1437,  0.0439,\n",
      "         -0.0506,  0.0735,  0.0478, -0.0884,  0.1582, -0.0612],\n",
      "        [-0.1146,  0.0770, -0.0252,  0.0181, -0.0339,  0.0054,  0.0518,  0.1402,\n",
      "         -0.1101,  0.1640, -0.1162, -0.0282,  0.1024,  0.0216,  0.1072,  0.1736,\n",
      "         -0.0620, -0.1474, -0.1694,  0.1254,  0.1411,  0.0594,  0.1743, -0.1766,\n",
      "         -0.1108,  0.1797, -0.0365, -0.1144, -0.1375, -0.1538],\n",
      "        [-0.1047, -0.1423, -0.0928, -0.0803,  0.1633, -0.0170, -0.1600,  0.0285,\n",
      "          0.0472, -0.1423,  0.0246, -0.1270, -0.0824, -0.1575, -0.0237,  0.0490,\n",
      "         -0.0629,  0.1202,  0.0074,  0.1583,  0.0882, -0.0413, -0.1734,  0.0337,\n",
      "          0.0690, -0.0144, -0.0078,  0.1717,  0.0399, -0.0639]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0332,  0.1216,  0.0217, -0.0446,  0.0948,  0.1191, -0.0361, -0.1435,\n",
      "        -0.1429, -0.0370,  0.0559,  0.0038,  0.1439,  0.0147, -0.0767, -0.1810,\n",
      "        -0.0020, -0.1225, -0.0598,  0.0365], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0645,  0.1868,  0.2058,  0.1972,  0.1604,  0.1541, -0.1343, -0.0149,\n",
      "          0.0378, -0.2050, -0.0990, -0.0941,  0.1560,  0.0265, -0.0972,  0.1629,\n",
      "         -0.0080,  0.0708, -0.0361,  0.0950],\n",
      "        [-0.2149, -0.0101, -0.1414,  0.1213, -0.0200,  0.1355,  0.0516, -0.0068,\n",
      "          0.1074, -0.0302, -0.2179,  0.2082, -0.0674, -0.2153, -0.0672,  0.1571,\n",
      "         -0.1936, -0.0921,  0.0568, -0.0598],\n",
      "        [ 0.0746,  0.0528, -0.0814, -0.1744, -0.1944,  0.2005, -0.1617, -0.0466,\n",
      "          0.1578, -0.0337, -0.0079,  0.0098, -0.1027,  0.2025, -0.1366,  0.1677,\n",
      "         -0.0555,  0.1633,  0.0532,  0.0041],\n",
      "        [ 0.1680, -0.1463, -0.0786, -0.0643, -0.0740, -0.0117,  0.1540, -0.1992,\n",
      "          0.1393,  0.0967,  0.0908, -0.1454,  0.0305,  0.1496,  0.1617,  0.0905,\n",
      "         -0.1299, -0.1951, -0.1896,  0.2146],\n",
      "        [-0.1525, -0.0966, -0.0195,  0.0279,  0.0471,  0.1141,  0.1380,  0.1430,\n",
      "          0.0928,  0.0429, -0.0018, -0.2028, -0.1851, -0.1621, -0.0707,  0.2090,\n",
      "          0.1758, -0.0517, -0.2028,  0.2026],\n",
      "        [ 0.0518,  0.0415,  0.1816,  0.2128,  0.1938, -0.0288, -0.0938, -0.1570,\n",
      "         -0.1956, -0.2035, -0.0527,  0.0957,  0.1902, -0.1985, -0.1470,  0.0025,\n",
      "          0.1523, -0.1450,  0.1428, -0.0755],\n",
      "        [ 0.0068, -0.1620,  0.0473,  0.0063, -0.1398,  0.0646, -0.2019, -0.0961,\n",
      "          0.1989,  0.1793, -0.0666, -0.2213,  0.1889, -0.0920, -0.2109,  0.2050,\n",
      "          0.1605,  0.1544, -0.1737, -0.0925],\n",
      "        [-0.1918,  0.1138, -0.0010,  0.1866, -0.1534, -0.1004, -0.0509, -0.1318,\n",
      "          0.1743,  0.0757,  0.1913,  0.1969, -0.1992,  0.1808,  0.0195, -0.1572,\n",
      "         -0.1491,  0.0649,  0.1612,  0.2203],\n",
      "        [-0.1473, -0.1141,  0.0178, -0.0430,  0.2101, -0.2163,  0.0197, -0.1040,\n",
      "         -0.0339, -0.1208, -0.1664, -0.1056, -0.0259, -0.0470, -0.0705, -0.1810,\n",
      "         -0.0543,  0.1457, -0.1718, -0.1976],\n",
      "        [-0.1937, -0.0576,  0.1276,  0.1298,  0.0613,  0.0221,  0.0707,  0.1545,\n",
      "         -0.1038,  0.1417, -0.0787,  0.1492, -0.1251,  0.1970,  0.0122, -0.1735,\n",
      "          0.0942, -0.2171,  0.0309,  0.1804],\n",
      "        [-0.1099, -0.0147,  0.0719,  0.0452,  0.1326,  0.0627,  0.1340, -0.0981,\n",
      "          0.1485, -0.1839,  0.1257,  0.0670,  0.0034,  0.2088, -0.1883,  0.0402,\n",
      "          0.1129,  0.0379,  0.2169,  0.1254],\n",
      "        [ 0.1044, -0.0781,  0.1271, -0.1904, -0.1778, -0.1907, -0.0630, -0.0302,\n",
      "          0.0649, -0.0132, -0.1298, -0.0850, -0.1274, -0.1809,  0.0902,  0.1605,\n",
      "         -0.1467, -0.2156, -0.1085, -0.0716],\n",
      "        [ 0.1754,  0.2040, -0.0741,  0.2158,  0.0789, -0.0319, -0.1838, -0.2100,\n",
      "          0.1683, -0.0629,  0.1232,  0.0100, -0.0650,  0.0309,  0.0204,  0.2045,\n",
      "          0.0240, -0.1778, -0.0653, -0.1988],\n",
      "        [-0.0132, -0.0751, -0.2142,  0.0031,  0.1197,  0.1325,  0.1421, -0.0667,\n",
      "          0.0257,  0.1803, -0.0312,  0.1150,  0.0141, -0.0745,  0.0268,  0.0693,\n",
      "          0.1520, -0.2175, -0.1840,  0.0233],\n",
      "        [ 0.0682,  0.0915,  0.1101,  0.1147,  0.1893, -0.1572, -0.0847, -0.1597,\n",
      "          0.2001, -0.2146,  0.0623, -0.1903, -0.1457,  0.0123,  0.2171,  0.1393,\n",
      "          0.0223, -0.1099, -0.1121,  0.0840],\n",
      "        [-0.0656, -0.1212, -0.0111,  0.2123,  0.1201,  0.0480,  0.1059, -0.0514,\n",
      "          0.2090,  0.0195,  0.1966, -0.1038,  0.0494, -0.0612, -0.1533, -0.1354,\n",
      "          0.1428,  0.1732,  0.1344, -0.1243],\n",
      "        [ 0.1538,  0.1011,  0.1008,  0.1264, -0.1439,  0.1956,  0.1865,  0.0107,\n",
      "          0.0781,  0.0734, -0.0486,  0.0909,  0.1778,  0.0315, -0.0942,  0.1690,\n",
      "          0.0564,  0.1924, -0.0453,  0.1099],\n",
      "        [-0.1985,  0.0239, -0.1073,  0.0284,  0.0769, -0.0434,  0.1567,  0.0306,\n",
      "          0.0094, -0.0415,  0.1949,  0.1322, -0.1077, -0.1211, -0.1927,  0.1954,\n",
      "          0.1953, -0.0173, -0.2230,  0.0818],\n",
      "        [-0.0844,  0.1360, -0.0777, -0.1566, -0.1202,  0.0756, -0.0320, -0.0953,\n",
      "          0.2005,  0.1690,  0.1891,  0.0333,  0.0708, -0.1453,  0.1145, -0.0329,\n",
      "         -0.1409,  0.0352,  0.1343, -0.0667],\n",
      "        [-0.0345, -0.1462,  0.0825, -0.1116, -0.1828,  0.0186,  0.0336, -0.0506,\n",
      "         -0.1104, -0.0608,  0.1529,  0.2142, -0.1720,  0.1830,  0.0059,  0.1698,\n",
      "          0.0788, -0.0422,  0.0700, -0.0116]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0289,  0.0142,  0.0077, -0.0046,  0.2229, -0.1278,  0.1693, -0.2083,\n",
      "         0.0133,  0.1184,  0.1838,  0.2108,  0.1407,  0.0850,  0.1326, -0.0900,\n",
      "        -0.0093, -0.1268, -0.1632,  0.1277], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2139,  0.0801,  0.2193, -0.0991,  0.1373, -0.0257,  0.0281,  0.0932,\n",
      "          0.1670,  0.2108,  0.0867,  0.2164,  0.0226, -0.0898, -0.1524,  0.0607,\n",
      "         -0.0022, -0.1256,  0.2106,  0.1074],\n",
      "        [-0.0915, -0.2116, -0.0769, -0.0941, -0.1786, -0.2126,  0.2160, -0.0356,\n",
      "          0.0133,  0.0019,  0.1958, -0.1952, -0.2218,  0.1587, -0.0316, -0.0799,\n",
      "         -0.1281,  0.0805,  0.1686,  0.1616],\n",
      "        [ 0.2118,  0.2071, -0.0994, -0.1482,  0.0747,  0.0651,  0.1646, -0.1609,\n",
      "          0.2135, -0.0706, -0.0609, -0.0075, -0.0832,  0.1097, -0.1298,  0.1059,\n",
      "          0.1688, -0.2120, -0.1931, -0.1766],\n",
      "        [ 0.1219, -0.1383, -0.2011,  0.1507,  0.2224, -0.1437, -0.0612,  0.1569,\n",
      "         -0.2200, -0.2066,  0.0263,  0.0894, -0.0076,  0.1004, -0.1449, -0.0848,\n",
      "          0.1405, -0.0550,  0.0476,  0.1666],\n",
      "        [ 0.1439,  0.0446,  0.1516,  0.0039, -0.0794,  0.0888,  0.1952, -0.1226,\n",
      "         -0.1733, -0.0419,  0.1440, -0.1429, -0.1084, -0.2094, -0.0683, -0.1290,\n",
      "          0.1054, -0.1088, -0.0431, -0.1486],\n",
      "        [-0.1874, -0.0261,  0.0286,  0.1692,  0.1819, -0.0487, -0.1192,  0.1962,\n",
      "         -0.2084, -0.1746,  0.1620,  0.1399,  0.2212, -0.0368,  0.1931, -0.0377,\n",
      "         -0.0788, -0.0348,  0.0372, -0.1035],\n",
      "        [ 0.0576, -0.0131,  0.1715, -0.0609,  0.0800,  0.1930,  0.1225,  0.0398,\n",
      "         -0.2092, -0.1753, -0.1580,  0.0686, -0.1212,  0.1906,  0.0323,  0.0371,\n",
      "         -0.1017, -0.1298, -0.0166, -0.0379],\n",
      "        [-0.2074, -0.1463, -0.2011,  0.1027, -0.0277,  0.0155,  0.2128, -0.0429,\n",
      "         -0.2007, -0.0539,  0.0602, -0.1601,  0.1933,  0.1254, -0.1523,  0.1056,\n",
      "         -0.1006,  0.0987,  0.0829, -0.1738],\n",
      "        [-0.0291, -0.1930,  0.1312, -0.0510,  0.0725,  0.1573,  0.0180,  0.1226,\n",
      "         -0.0895, -0.0929,  0.2198, -0.1858,  0.1900,  0.0780, -0.2178,  0.1877,\n",
      "          0.0239, -0.1717,  0.1926,  0.0242],\n",
      "        [ 0.0819,  0.0177, -0.0890,  0.1189, -0.0322, -0.0819,  0.1489,  0.0867,\n",
      "          0.1586,  0.0062,  0.0462, -0.1469, -0.1668,  0.0784, -0.1681,  0.1897,\n",
      "          0.2061, -0.1415,  0.0565, -0.0917],\n",
      "        [-0.0326, -0.1124,  0.1981, -0.1859, -0.0925,  0.1807,  0.0050,  0.1739,\n",
      "         -0.2151, -0.0864,  0.1238,  0.0269,  0.0412,  0.1632,  0.0136,  0.1362,\n",
      "          0.1409, -0.0253, -0.0521, -0.0806],\n",
      "        [ 0.0416,  0.0338, -0.1553, -0.0963, -0.2166,  0.0375,  0.0045,  0.2147,\n",
      "          0.0923,  0.1328, -0.0417,  0.1474, -0.2192, -0.1969, -0.1121, -0.0638,\n",
      "         -0.1335,  0.1238,  0.1521,  0.1303],\n",
      "        [ 0.1624, -0.0331,  0.0051, -0.1226,  0.0381,  0.0570, -0.0350, -0.0714,\n",
      "         -0.0681, -0.0044, -0.0267,  0.1743,  0.1237,  0.1915,  0.0158,  0.2024,\n",
      "         -0.0598,  0.1821, -0.1478,  0.2137],\n",
      "        [ 0.0027, -0.0578,  0.0398,  0.1041, -0.0894, -0.1245,  0.1319, -0.0032,\n",
      "          0.0196, -0.1756, -0.0034, -0.1009, -0.0808,  0.1324, -0.0834, -0.0196,\n",
      "          0.1257,  0.0781, -0.1216, -0.2075],\n",
      "        [-0.1295, -0.0156,  0.0733,  0.2182, -0.2036,  0.0368, -0.0659,  0.1430,\n",
      "         -0.0273, -0.1459, -0.0871, -0.0607,  0.2171,  0.0294, -0.0557,  0.1211,\n",
      "         -0.0787,  0.0140, -0.1222, -0.1400],\n",
      "        [ 0.1882,  0.0504,  0.1461, -0.0437,  0.1678, -0.1411,  0.0991, -0.0900,\n",
      "         -0.0952,  0.0287, -0.1205,  0.1343,  0.1007,  0.1984,  0.0931,  0.2112,\n",
      "          0.1083, -0.2012, -0.1266,  0.0634],\n",
      "        [ 0.1856, -0.1148, -0.0116, -0.1869, -0.1556, -0.0101, -0.0609, -0.1585,\n",
      "         -0.0396, -0.2109, -0.1009,  0.0626, -0.1597,  0.1307, -0.0946,  0.0949,\n",
      "         -0.1092,  0.1151, -0.0978,  0.1795],\n",
      "        [ 0.0455,  0.0541, -0.1262,  0.1073, -0.2235,  0.1622, -0.0149, -0.1474,\n",
      "         -0.1612,  0.0831,  0.0177, -0.0363,  0.1209,  0.0328,  0.1602, -0.1014,\n",
      "         -0.1677,  0.1888, -0.1393,  0.1569],\n",
      "        [-0.0449,  0.0287, -0.1705,  0.0661, -0.1269, -0.1023, -0.0287,  0.2022,\n",
      "          0.0808, -0.1564, -0.0444,  0.0763, -0.0547, -0.0933,  0.1219, -0.1895,\n",
      "         -0.0346,  0.0633,  0.0672, -0.1523],\n",
      "        [ 0.0555,  0.0999,  0.1947, -0.1805,  0.1281,  0.1849, -0.2115, -0.1391,\n",
      "          0.0176,  0.0858, -0.1365, -0.1207,  0.0700,  0.0731, -0.1276,  0.0283,\n",
      "          0.1401,  0.1792, -0.0750, -0.2058]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1687,  0.0462,  0.0450,  0.1735,  0.2110, -0.0721,  0.0845,  0.0112,\n",
      "         0.1868, -0.1773, -0.1245,  0.2014,  0.1484, -0.0816, -0.2079, -0.1331,\n",
      "         0.1086, -0.0606,  0.1196, -0.0514], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.5056e-01,  1.1221e-01,  2.9948e-02,  8.2028e-02,  2.0723e-01,\n",
      "          2.6044e-02,  1.2598e-01,  1.6957e-01,  2.1372e-01,  1.4326e-01,\n",
      "          9.7593e-02, -1.8491e-01,  2.0845e-01,  1.6293e-01,  2.1515e-01,\n",
      "         -2.2183e-01, -5.0575e-02, -1.7053e-01, -5.8743e-02,  2.0022e-01],\n",
      "        [ 2.0686e-01, -2.5510e-02, -9.3129e-02, -1.5144e-01, -1.1458e-01,\n",
      "         -1.7089e-01, -4.2839e-02,  1.8101e-01, -6.8672e-02,  1.3605e-01,\n",
      "          2.7372e-02,  1.1575e-01, -6.9859e-02,  1.2686e-01, -1.2811e-01,\n",
      "          1.5757e-01, -1.1042e-01,  1.1764e-01,  9.2762e-02,  1.8593e-01],\n",
      "        [ 1.8752e-01,  3.1002e-02, -2.9684e-02, -1.5331e-01, -1.6625e-01,\n",
      "         -1.1359e-01, -1.9752e-01,  1.0078e-01, -2.0624e-01, -1.0852e-01,\n",
      "         -7.1670e-02,  1.1592e-01,  3.9934e-02, -4.7556e-02, -1.9721e-01,\n",
      "          4.8859e-02,  1.8127e-01, -1.1409e-02, -2.1056e-01,  6.6457e-02],\n",
      "        [ 1.3815e-01, -1.6854e-01, -6.1696e-02,  1.4855e-01,  1.9207e-01,\n",
      "         -1.4178e-01,  2.9519e-02,  5.9239e-02, -7.4530e-04,  1.1853e-01,\n",
      "         -1.9950e-01, -1.5664e-01, -9.0351e-02,  6.7573e-02,  1.2316e-01,\n",
      "         -1.8605e-01, -5.8923e-02,  2.0190e-02, -1.5186e-01,  1.0894e-01],\n",
      "        [ 1.7984e-01,  1.7722e-02,  1.5839e-01, -3.9898e-02,  2.3476e-02,\n",
      "         -2.1090e-01,  2.8718e-02,  6.7129e-02, -7.3359e-02, -1.9848e-01,\n",
      "         -1.5989e-01,  1.5872e-01,  1.4071e-01, -1.0465e-01,  3.1532e-02,\n",
      "         -8.1927e-02, -7.5079e-03,  1.8606e-01, -1.6414e-01, -3.0827e-02],\n",
      "        [ 1.2411e-01, -7.0343e-02,  1.6236e-01,  1.1629e-01,  2.0189e-01,\n",
      "         -2.3697e-02,  1.2502e-01, -5.7759e-02, -1.3328e-01,  1.6451e-01,\n",
      "         -1.6310e-01, -3.4575e-02,  1.2362e-01,  1.9456e-01,  1.0914e-01,\n",
      "         -4.0426e-02, -6.8939e-03,  1.1192e-01, -1.1343e-01, -1.2686e-01],\n",
      "        [ 1.7773e-01,  2.0655e-01, -1.6992e-01, -1.9303e-01, -9.8909e-02,\n",
      "         -9.8637e-02, -8.6228e-02, -3.6889e-02,  7.9429e-02, -5.2288e-02,\n",
      "          1.6223e-01,  1.8721e-01,  1.6939e-01,  2.0963e-01, -1.5437e-01,\n",
      "         -1.1178e-02,  8.7746e-02,  1.5967e-01, -9.2008e-02,  1.3764e-01],\n",
      "        [-1.4253e-01,  2.2098e-01, -1.3277e-01, -1.5041e-01,  2.4331e-02,\n",
      "          7.1697e-02, -1.4380e-01,  7.4604e-02,  2.1286e-01,  1.4956e-02,\n",
      "         -5.0775e-02, -1.0614e-01, -1.5540e-01, -1.8117e-01, -1.8826e-01,\n",
      "          1.0030e-01, -7.4675e-02,  2.9777e-02, -3.7594e-02, -6.1785e-02],\n",
      "        [ 1.9982e-01,  3.1320e-02,  1.8428e-01,  9.4675e-02, -1.4570e-01,\n",
      "         -5.0689e-02,  2.3745e-02,  3.5871e-02, -1.4123e-01,  5.3727e-02,\n",
      "          4.9849e-02,  1.0835e-01,  1.9278e-02, -1.0652e-01, -5.2024e-02,\n",
      "         -1.6558e-02, -1.9872e-01, -7.1076e-02, -1.8150e-01, -1.4899e-01],\n",
      "        [ 1.5361e-01,  1.2090e-01, -1.3196e-01, -1.8849e-01, -3.2501e-02,\n",
      "          1.5307e-01, -1.3074e-02,  8.4525e-02,  2.0851e-01,  1.5722e-01,\n",
      "         -1.5795e-01, -1.6616e-02,  1.2089e-01, -1.7682e-01,  1.5841e-01,\n",
      "         -1.0503e-01,  2.3621e-03,  3.0335e-02,  1.7504e-01, -4.2389e-02],\n",
      "        [-2.0152e-01,  1.8413e-02,  1.9620e-01,  1.3966e-02,  2.1123e-01,\n",
      "          2.1023e-01, -1.8217e-01,  1.7547e-01, -9.0474e-02, -7.9791e-02,\n",
      "         -1.3176e-02,  1.2728e-01, -9.4504e-02, -1.8874e-01,  3.6424e-03,\n",
      "          1.1796e-01,  1.4150e-01, -1.0141e-01, -6.0012e-02,  1.1025e-02],\n",
      "        [ 5.8272e-02, -7.7487e-02,  1.6156e-03, -1.1005e-01,  1.9744e-02,\n",
      "          5.5321e-02,  5.3665e-03, -1.9065e-01, -5.9701e-02, -2.6964e-02,\n",
      "          1.2633e-02, -2.2220e-01, -1.9953e-01, -1.6738e-01, -2.1424e-01,\n",
      "         -8.9133e-02,  1.8834e-01,  1.2658e-01,  1.7486e-01, -2.1485e-01],\n",
      "        [ 1.5886e-01, -2.7666e-02,  1.5031e-01,  2.0391e-01, -1.1024e-01,\n",
      "          4.1959e-02,  1.9760e-01, -9.0056e-02, -1.6172e-01, -6.3857e-02,\n",
      "          1.4187e-01,  2.0163e-02, -4.6222e-02,  8.9014e-03,  1.3681e-01,\n",
      "          1.4411e-02, -6.4193e-02, -2.1573e-01, -1.8697e-02,  1.7362e-01],\n",
      "        [-7.8293e-02,  3.7541e-02,  4.1729e-02, -4.6209e-02, -4.8724e-04,\n",
      "         -1.5504e-01,  6.5284e-02,  1.8896e-01,  1.4661e-01,  1.4119e-01,\n",
      "          3.6969e-02,  1.1000e-01, -2.0484e-01,  1.1877e-01, -1.1980e-01,\n",
      "         -4.1698e-02,  1.1435e-01, -5.9146e-02,  3.9870e-02, -5.1731e-02],\n",
      "        [ 1.9429e-01, -1.8042e-01,  1.6792e-02, -2.1862e-01, -1.0422e-01,\n",
      "         -3.7153e-02,  1.2688e-01, -7.9675e-02,  1.7998e-01, -1.7108e-01,\n",
      "         -6.7004e-02,  2.8261e-02,  9.8531e-02, -1.6089e-01, -1.4279e-01,\n",
      "         -1.8491e-01,  5.6534e-02,  9.3106e-02, -2.2214e-01, -5.8375e-02],\n",
      "        [ 2.1761e-02,  1.3587e-02,  4.4347e-02,  6.8175e-02, -4.7307e-03,\n",
      "         -1.7731e-01,  1.6329e-01,  1.6836e-01, -1.6574e-01, -1.5466e-02,\n",
      "          1.3560e-01,  4.4979e-02, -2.1722e-01, -7.1178e-03, -1.0931e-02,\n",
      "          9.5232e-02, -5.9943e-02, -5.0425e-02, -4.2187e-03,  8.2104e-02],\n",
      "        [-5.0641e-02, -1.8891e-01,  4.3647e-02,  1.0525e-01,  1.4370e-01,\n",
      "         -7.4474e-02, -1.1218e-02,  1.9248e-02, -2.5749e-02, -1.2328e-02,\n",
      "         -1.6711e-01,  1.2223e-01,  1.1709e-01, -1.6633e-01, -1.1900e-01,\n",
      "         -1.1272e-02,  1.6576e-02, -1.0573e-01, -4.7211e-02, -5.9583e-02],\n",
      "        [-1.2228e-01, -1.0740e-01,  6.2957e-02, -1.8158e-01, -4.7399e-03,\n",
      "          6.0823e-02,  8.3369e-02, -7.6129e-02,  1.7871e-01,  6.6041e-02,\n",
      "          5.8792e-02, -1.4814e-02,  2.2159e-01,  1.9384e-03, -1.6373e-01,\n",
      "          2.2278e-01,  1.1165e-01, -1.6140e-01, -1.7203e-01, -1.0319e-03],\n",
      "        [ 1.8170e-02, -9.9711e-02, -1.8853e-01,  3.5073e-03, -1.6794e-01,\n",
      "         -8.5019e-02, -3.6591e-02,  2.0982e-01, -2.0417e-02,  8.7547e-02,\n",
      "         -1.1802e-01,  7.8934e-02,  7.0687e-02, -1.7202e-01, -2.2350e-01,\n",
      "         -1.8971e-01,  1.3674e-01, -1.6901e-01, -6.7267e-02, -2.2071e-01],\n",
      "        [ 3.7077e-02, -1.6957e-01, -1.2933e-01,  1.1808e-01,  1.0291e-01,\n",
      "         -8.6419e-02, -3.3041e-02, -2.0073e-01, -1.1302e-01, -9.2035e-02,\n",
      "          1.0338e-01,  1.9583e-01, -2.0130e-01,  1.0087e-04,  7.3695e-02,\n",
      "          2.2783e-02, -1.3372e-01, -1.8820e-01,  1.4215e-01, -2.1733e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0047,  0.1716,  0.0680, -0.2190,  0.0183, -0.0505,  0.1639,  0.1711,\n",
      "         0.1150,  0.1435,  0.1817, -0.1388, -0.1351, -0.0817,  0.0521,  0.1842,\n",
      "        -0.0686,  0.1667,  0.0770,  0.0035], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6905e-01,  1.8496e-01,  3.7061e-02,  1.1746e-01, -9.2010e-02,\n",
      "          8.8906e-02, -1.1532e-01,  6.8394e-02,  1.7159e-01,  1.1611e-01,\n",
      "          1.0295e-01, -6.2431e-02,  8.2908e-02,  2.0891e-01,  5.4695e-03,\n",
      "          1.5009e-01, -2.2088e-01, -2.3244e-02,  1.0587e-01,  2.0034e-01],\n",
      "        [ 9.5080e-03, -1.7181e-01,  9.8322e-02, -7.6595e-02,  1.1164e-01,\n",
      "          7.9680e-02, -1.8082e-02,  1.4898e-01,  1.7282e-01,  8.2926e-02,\n",
      "         -9.8808e-02,  7.0789e-02, -1.1069e-01, -1.0125e-01,  5.5676e-02,\n",
      "          1.8960e-01,  6.7842e-02, -9.9687e-02, -1.6598e-01, -2.0542e-01],\n",
      "        [ 3.9242e-02, -1.4351e-01, -5.9182e-02,  1.7131e-01, -1.6957e-01,\n",
      "         -5.1250e-02, -8.2598e-02, -1.9484e-02,  5.6477e-02, -3.0213e-02,\n",
      "         -1.1950e-01,  1.8666e-01,  1.5613e-01, -4.8602e-02, -9.6152e-03,\n",
      "         -6.4893e-02, -1.0619e-01,  4.4175e-02, -7.8524e-02, -1.7693e-01],\n",
      "        [-1.4207e-01, -5.4280e-02,  2.0919e-01,  1.2090e-01,  2.1857e-01,\n",
      "          2.1978e-01,  1.5185e-01,  1.5741e-01,  1.7732e-01,  7.5800e-02,\n",
      "          2.3743e-03,  1.2077e-01, -1.7967e-02, -8.4226e-03,  1.6149e-01,\n",
      "         -9.5680e-03,  1.5427e-01,  1.5392e-01,  1.5358e-01,  1.7966e-01],\n",
      "        [-7.8436e-02, -1.6988e-01,  1.0593e-01, -3.2302e-02,  8.1548e-02,\n",
      "          1.5406e-01, -7.4906e-02, -6.3484e-02,  1.7854e-01,  1.3954e-01,\n",
      "          2.1482e-01, -7.8831e-02,  1.0697e-02,  1.1997e-01,  9.0263e-02,\n",
      "         -3.5979e-02, -2.0796e-01,  3.8448e-04, -1.1297e-01, -1.6853e-01],\n",
      "        [-2.1043e-01,  1.7655e-01,  1.1098e-01,  4.4355e-02,  1.1082e-01,\n",
      "          2.0190e-01, -9.4407e-02, -5.2976e-02, -1.4862e-01,  1.4805e-02,\n",
      "          2.0711e-01, -1.2630e-01,  1.0193e-01, -6.8152e-02,  1.0695e-01,\n",
      "          1.1542e-02,  6.8230e-03, -7.5555e-02,  3.5431e-02,  1.5445e-01],\n",
      "        [ 1.2808e-01, -1.3324e-01, -9.3972e-02,  3.1598e-02, -1.4971e-01,\n",
      "         -6.7390e-02,  1.1384e-01, -1.7261e-01,  6.5265e-02, -1.7706e-01,\n",
      "         -9.6759e-02,  1.3732e-01,  1.6351e-01,  1.3465e-01, -8.5700e-02,\n",
      "          7.1694e-02, -8.9236e-02,  1.0136e-01, -2.1993e-01, -3.5061e-02],\n",
      "        [-1.9249e-01, -1.6540e-01,  1.9579e-01,  8.4986e-02,  2.7162e-02,\n",
      "          3.0971e-03, -2.0291e-01,  1.9543e-01,  1.1519e-02,  1.7025e-01,\n",
      "          1.7695e-01,  1.1161e-01,  1.9399e-01, -8.3734e-02, -1.7236e-01,\n",
      "          2.0498e-01,  1.3464e-02, -2.1237e-01,  2.1299e-01, -1.1077e-02],\n",
      "        [-2.6579e-02,  8.7605e-02, -5.3006e-02,  1.6782e-01, -4.6393e-02,\n",
      "          1.3003e-01,  2.0654e-01,  3.0745e-02,  7.6571e-02, -1.1441e-01,\n",
      "         -2.2639e-03, -2.1359e-01, -3.1942e-02, -1.2482e-01, -1.2105e-01,\n",
      "         -1.4491e-01, -9.5076e-02,  1.4067e-01,  6.7766e-02,  4.4255e-02],\n",
      "        [-1.9329e-01, -6.4837e-02, -1.5165e-01, -5.7394e-03, -2.2095e-01,\n",
      "         -2.2240e-01,  6.5957e-02, -1.5871e-01, -5.1961e-02,  1.3303e-01,\n",
      "          8.8001e-02, -8.3908e-02,  2.4934e-02,  1.6567e-01,  1.3838e-02,\n",
      "         -1.8358e-01,  8.7634e-02,  1.9635e-01,  1.5705e-01, -9.8939e-02],\n",
      "        [-1.7883e-01, -8.0375e-02,  8.1902e-02,  2.2153e-01, -6.6237e-02,\n",
      "         -1.8697e-01, -5.2316e-02,  4.6505e-02,  8.2335e-02, -3.3471e-02,\n",
      "         -1.6172e-01,  1.1686e-01,  2.0432e-01, -1.4213e-01,  1.3782e-01,\n",
      "          1.1237e-01, -1.0222e-03,  1.6907e-01,  2.0279e-01, -1.0472e-01],\n",
      "        [-6.0808e-02, -1.8833e-01,  8.8181e-02,  7.6433e-02, -5.1281e-02,\n",
      "         -4.3099e-02,  2.0179e-04, -1.2118e-01,  3.8007e-02, -1.2885e-01,\n",
      "          1.9007e-01, -1.6078e-01,  6.1494e-02, -5.7015e-02,  2.1618e-01,\n",
      "         -1.3186e-01,  6.7600e-03, -2.1941e-01, -1.9511e-02,  6.8857e-02],\n",
      "        [ 3.9238e-02,  1.1890e-01,  5.6688e-03, -7.1844e-02,  2.1395e-01,\n",
      "         -1.4654e-01,  1.4117e-01,  1.8377e-01, -2.1931e-01, -1.0086e-01,\n",
      "         -1.5075e-01,  1.3463e-03, -1.6868e-01,  4.3722e-02,  1.8746e-01,\n",
      "         -2.2442e-02,  1.2491e-02,  6.5598e-02, -1.8203e-01, -1.9392e-01],\n",
      "        [-1.1852e-01,  1.0079e-01,  1.0719e-01,  3.4989e-02,  1.7976e-01,\n",
      "         -3.3587e-02,  7.6158e-02, -1.4550e-01, -1.9175e-01,  1.9036e-01,\n",
      "          1.9728e-01, -2.1604e-02,  8.8399e-02, -7.9087e-02, -1.0094e-01,\n",
      "          1.8559e-01, -1.2420e-01,  8.2034e-02,  4.1268e-02,  1.2630e-01],\n",
      "        [ 9.7648e-03,  6.7965e-02,  1.3741e-01, -4.6005e-02,  5.9469e-02,\n",
      "         -6.2965e-02,  7.0284e-03, -8.1563e-02,  1.3920e-01,  1.5511e-01,\n",
      "         -2.1529e-01,  1.5381e-01, -1.6706e-01, -1.8321e-01, -1.5385e-01,\n",
      "         -1.9454e-01,  1.0659e-01, -2.4375e-02, -1.6593e-01, -1.5638e-01],\n",
      "        [ 8.9900e-02,  2.1987e-01, -1.8437e-02, -4.4419e-02,  3.6391e-02,\n",
      "          1.4833e-02, -8.1401e-02, -1.1949e-01,  1.9889e-01, -1.3033e-01,\n",
      "         -1.2015e-01,  1.5711e-01, -1.0055e-01,  5.2589e-02, -1.2050e-01,\n",
      "         -2.1472e-01,  8.0822e-02, -4.6695e-02,  7.5025e-02, -1.2551e-02],\n",
      "        [-1.2253e-01, -1.3838e-01, -2.3406e-02, -4.0567e-02, -1.8411e-02,\n",
      "         -1.2360e-01,  7.6634e-02,  1.3956e-01,  1.6729e-01,  1.1134e-02,\n",
      "          1.3680e-02, -1.8027e-01,  4.8941e-02, -1.0225e-01,  8.1831e-02,\n",
      "          1.0689e-01,  2.3919e-02, -4.9783e-02, -8.4022e-02,  1.6125e-01],\n",
      "        [ 4.9263e-02,  1.2076e-01, -2.2480e-03, -1.5538e-01,  4.5167e-03,\n",
      "          2.2877e-02, -8.4120e-02,  4.9159e-02,  1.9634e-01,  1.5215e-01,\n",
      "          6.5532e-02, -1.4837e-01, -1.2357e-01, -1.9105e-01,  2.1585e-01,\n",
      "          1.9180e-01, -1.0922e-02, -1.4062e-01, -3.7563e-02, -7.9644e-02],\n",
      "        [ 1.4459e-01, -1.8147e-01,  3.6661e-02, -1.7634e-01, -1.6880e-01,\n",
      "         -2.2269e-01,  5.1746e-02, -1.4933e-01,  1.5896e-01,  1.2999e-01,\n",
      "          2.4072e-02,  8.6091e-02, -4.7489e-02, -1.8928e-01,  9.6227e-02,\n",
      "         -1.0907e-01,  1.6354e-01, -2.1703e-01, -1.2954e-01,  4.5627e-02],\n",
      "        [-1.8150e-01,  1.3446e-01, -1.2253e-01, -2.1879e-01, -2.3740e-02,\n",
      "          9.0830e-02, -1.1038e-01,  2.3797e-02,  6.0176e-02,  7.5978e-02,\n",
      "          4.7835e-02, -1.4296e-04, -8.2447e-02,  4.0150e-02, -1.4164e-01,\n",
      "          5.0712e-02,  1.3758e-01,  6.3193e-02,  1.9681e-01, -7.5572e-03]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1637,  0.1393, -0.1053,  0.0963,  0.1021,  0.1561, -0.0282, -0.0908,\n",
      "         0.1457, -0.1529,  0.0475, -0.2220,  0.0900, -0.0829, -0.1132,  0.1894,\n",
      "         0.0548, -0.0601, -0.1073,  0.1946], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1437,  0.1806, -0.1753,  0.0509,  0.1491, -0.1187,  0.0056,  0.1091,\n",
      "          0.0446, -0.1974, -0.1245,  0.0789,  0.0561, -0.1139,  0.1180,  0.1018,\n",
      "         -0.0734, -0.0199,  0.1844, -0.1832],\n",
      "        [-0.1978,  0.1785,  0.1379, -0.1530, -0.0988, -0.1636,  0.1049, -0.0869,\n",
      "          0.1228,  0.1291, -0.0707,  0.0087, -0.0237,  0.2042, -0.1760,  0.1984,\n",
      "          0.0291,  0.1799, -0.0595,  0.1795],\n",
      "        [ 0.2163,  0.0625,  0.1395,  0.1773, -0.1669, -0.2062,  0.0656,  0.1966,\n",
      "         -0.0847, -0.0456, -0.0059,  0.0100,  0.2068, -0.1782,  0.0534, -0.0311,\n",
      "          0.0584, -0.0243, -0.2226,  0.1691],\n",
      "        [ 0.1927, -0.1650, -0.0722, -0.0076,  0.1274, -0.1976,  0.1050,  0.1725,\n",
      "         -0.0442,  0.1208, -0.0044, -0.2045, -0.0754, -0.1068, -0.2196,  0.1058,\n",
      "          0.1017, -0.0387,  0.1236,  0.1977],\n",
      "        [-0.0074, -0.0426,  0.1806,  0.1166, -0.1935, -0.1061,  0.1599, -0.1421,\n",
      "          0.1390, -0.1978, -0.1227,  0.0422, -0.2037,  0.0576, -0.0246,  0.0238,\n",
      "          0.0762, -0.2035,  0.1002, -0.1498],\n",
      "        [-0.0163,  0.0606, -0.1304, -0.0199, -0.1361,  0.0113, -0.1263, -0.1152,\n",
      "         -0.0911, -0.1202,  0.0959,  0.1328, -0.0317,  0.0284, -0.0078, -0.1417,\n",
      "         -0.0603, -0.1434,  0.1589,  0.2090],\n",
      "        [-0.2086, -0.0176,  0.0918, -0.1152,  0.0156, -0.1295,  0.2221,  0.1958,\n",
      "          0.1771,  0.0458,  0.2151, -0.1168,  0.1713, -0.0357, -0.2078,  0.0734,\n",
      "          0.0459,  0.1373, -0.1784,  0.0754],\n",
      "        [-0.0738,  0.1571,  0.0941, -0.1158, -0.2177, -0.1995, -0.1698,  0.0407,\n",
      "         -0.1218, -0.2228, -0.2131,  0.2112, -0.1889, -0.1333, -0.1873, -0.1120,\n",
      "          0.0567, -0.0691, -0.1552,  0.0772],\n",
      "        [-0.0035, -0.1269,  0.0232, -0.0402,  0.1662,  0.0602, -0.1542, -0.2181,\n",
      "         -0.0792,  0.0429, -0.1246,  0.1144,  0.0335,  0.1644,  0.1310,  0.0471,\n",
      "         -0.2053, -0.0252, -0.0259, -0.0595],\n",
      "        [-0.1566, -0.1836, -0.0107, -0.0884, -0.2164,  0.2141, -0.0598, -0.0643,\n",
      "          0.0257, -0.2008, -0.1516,  0.1730,  0.1349,  0.0693, -0.0245, -0.0977,\n",
      "         -0.1259,  0.2160, -0.0834,  0.0588],\n",
      "        [-0.0132,  0.0119, -0.0547,  0.1126,  0.1143,  0.1752, -0.0312,  0.1714,\n",
      "          0.1907,  0.0762, -0.1990,  0.1529, -0.0528, -0.1925, -0.2080, -0.1362,\n",
      "         -0.0442, -0.1409,  0.0081, -0.2076],\n",
      "        [-0.0976,  0.0967,  0.1166, -0.1881, -0.0411,  0.1008, -0.1319,  0.1904,\n",
      "          0.0462, -0.1529,  0.1473,  0.1479, -0.0764,  0.1511,  0.1532,  0.0268,\n",
      "         -0.1792,  0.1449,  0.0970, -0.2063],\n",
      "        [ 0.1594, -0.1676,  0.1790, -0.1387,  0.1321,  0.1625, -0.1534,  0.1633,\n",
      "          0.0043,  0.2019, -0.1977,  0.0809,  0.1452, -0.1738, -0.1718, -0.0668,\n",
      "          0.2054,  0.2110,  0.0670,  0.2010],\n",
      "        [ 0.0777, -0.2185, -0.0105, -0.0556,  0.1100, -0.0638, -0.1791,  0.0727,\n",
      "          0.0017, -0.1507,  0.1116,  0.1086,  0.1724, -0.0362, -0.2055,  0.0870,\n",
      "         -0.1537,  0.0354, -0.0139,  0.2220],\n",
      "        [ 0.0298,  0.2228, -0.1747, -0.0754,  0.1955,  0.0108,  0.0353, -0.2067,\n",
      "         -0.1608, -0.1491,  0.0588, -0.2163, -0.1399, -0.1068,  0.0205,  0.0239,\n",
      "          0.1567,  0.2008,  0.1985, -0.0378],\n",
      "        [-0.1312,  0.0343, -0.1758,  0.1792, -0.0578,  0.0325, -0.1843, -0.1685,\n",
      "         -0.1698, -0.0238, -0.1440, -0.0139, -0.1540,  0.0834,  0.0221,  0.0580,\n",
      "         -0.0671, -0.1375, -0.0780, -0.1639],\n",
      "        [-0.0849, -0.2218,  0.1513, -0.1407,  0.0051, -0.1899,  0.0149,  0.0711,\n",
      "         -0.0892,  0.2232, -0.1249, -0.0421,  0.2004,  0.0580, -0.0667,  0.1687,\n",
      "         -0.1892, -0.1599,  0.0577, -0.1110],\n",
      "        [ 0.0360, -0.2131,  0.1378, -0.1107,  0.1998, -0.0162,  0.0994,  0.0354,\n",
      "          0.2015, -0.0281, -0.0049, -0.1045,  0.1390,  0.2101, -0.0852,  0.1457,\n",
      "          0.1201,  0.0244,  0.0336, -0.0119],\n",
      "        [ 0.1007,  0.0724, -0.0017,  0.1301, -0.2154, -0.0184, -0.1997,  0.1871,\n",
      "          0.1137,  0.0733,  0.2166,  0.0146,  0.1763, -0.0670, -0.1057,  0.2204,\n",
      "         -0.1264,  0.1662,  0.1515,  0.0661],\n",
      "        [-0.2118, -0.1894,  0.0236,  0.2182,  0.0713,  0.1563,  0.0812,  0.0110,\n",
      "          0.0568, -0.2159, -0.1190,  0.0238,  0.0735, -0.1688, -0.1878,  0.2105,\n",
      "         -0.0820,  0.0997,  0.0505, -0.0966]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1516, -0.2114,  0.0931, -0.0151, -0.0896,  0.2036, -0.0244,  0.0018,\n",
      "         0.0601, -0.2073, -0.2050, -0.2133, -0.1005, -0.0462, -0.1630, -0.1118,\n",
      "        -0.1485, -0.1745, -0.0296,  0.1227], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0346,  0.2049,  0.1580,  0.2128, -0.1225, -0.0378,  0.0215, -0.1989,\n",
      "          0.1846,  0.1763, -0.1794,  0.2115, -0.0595, -0.0316,  0.0989,  0.0067,\n",
      "          0.0693,  0.0930, -0.1273, -0.1021],\n",
      "        [-0.2079,  0.2020,  0.2110, -0.2025, -0.2112,  0.0116,  0.1217, -0.1996,\n",
      "          0.0410,  0.1455, -0.1831,  0.0106,  0.2117, -0.2223,  0.0717, -0.2057,\n",
      "          0.1824,  0.2091, -0.0257, -0.1187],\n",
      "        [-0.1830,  0.0790,  0.0478, -0.0317,  0.0656,  0.2189,  0.1592, -0.0038,\n",
      "         -0.0394, -0.1815, -0.1734,  0.0764,  0.1405,  0.0394, -0.0144,  0.0526,\n",
      "          0.0382,  0.0530,  0.2230, -0.0926],\n",
      "        [ 0.2112, -0.0681,  0.1594,  0.0702, -0.1193,  0.0221, -0.0030,  0.1930,\n",
      "         -0.1496,  0.0124, -0.0211,  0.2062,  0.0007,  0.1876, -0.0180, -0.1335,\n",
      "          0.1092, -0.0894, -0.1415,  0.0733],\n",
      "        [ 0.1959,  0.1423,  0.1833, -0.0940, -0.1294, -0.0199, -0.0921,  0.2143,\n",
      "         -0.1717, -0.0222, -0.1028,  0.1089,  0.1219, -0.0795,  0.0545, -0.1233,\n",
      "          0.1044,  0.0931, -0.2148,  0.0103],\n",
      "        [-0.2129,  0.0387, -0.0890, -0.0633,  0.1731, -0.2018, -0.1511, -0.1880,\n",
      "         -0.0835,  0.0483, -0.0508, -0.1866,  0.0344, -0.1114,  0.2126, -0.1474,\n",
      "         -0.1197,  0.1986, -0.0238, -0.0804],\n",
      "        [-0.1065,  0.1339, -0.1407, -0.1433,  0.0260,  0.0033,  0.1808, -0.1863,\n",
      "          0.0145,  0.0427,  0.0834,  0.1095,  0.1757,  0.2207, -0.0235,  0.0765,\n",
      "         -0.2000,  0.1562, -0.0964,  0.1410],\n",
      "        [-0.0449, -0.1081, -0.0939,  0.1370,  0.0433,  0.1188,  0.1113,  0.1655,\n",
      "         -0.0296,  0.0925,  0.0091, -0.2140, -0.1841,  0.0225,  0.1196,  0.0876,\n",
      "         -0.2152,  0.0876, -0.1926,  0.1828],\n",
      "        [ 0.0685, -0.0777,  0.1750, -0.1503, -0.1365,  0.1369,  0.1905,  0.0617,\n",
      "          0.0552, -0.0682, -0.1107, -0.0769,  0.0258,  0.2141,  0.0435,  0.0833,\n",
      "          0.0485, -0.0758,  0.0881,  0.0273],\n",
      "        [-0.0926,  0.0230, -0.0475, -0.0682,  0.0970, -0.1547,  0.0577, -0.2007,\n",
      "          0.0632, -0.1924,  0.2117,  0.0789, -0.0559,  0.1353,  0.0318,  0.1744,\n",
      "          0.0902, -0.0372,  0.1599, -0.0954],\n",
      "        [ 0.1051,  0.1622, -0.2049, -0.0081, -0.1314, -0.1084, -0.1793, -0.1836,\n",
      "          0.0667, -0.1944, -0.0231,  0.0894, -0.2048,  0.1610, -0.0257, -0.1927,\n",
      "         -0.2176,  0.1081, -0.1568,  0.0821],\n",
      "        [ 0.0459, -0.1968, -0.1094,  0.0279, -0.0423,  0.1260,  0.1600, -0.0114,\n",
      "          0.1859,  0.0363, -0.0142,  0.1045,  0.0185,  0.1229,  0.0883, -0.0106,\n",
      "          0.1370,  0.1149, -0.1464,  0.0747],\n",
      "        [-0.1683,  0.1255, -0.0965,  0.0476,  0.2197, -0.0226, -0.2005, -0.0830,\n",
      "         -0.0210, -0.1892,  0.0458, -0.0732,  0.2161,  0.0138,  0.1355, -0.1042,\n",
      "         -0.1615, -0.0855, -0.1369, -0.1762],\n",
      "        [-0.2177,  0.0409, -0.1694, -0.1776, -0.1618, -0.2131, -0.0806,  0.0115,\n",
      "         -0.0465,  0.1208,  0.0947,  0.0936, -0.2231,  0.2100,  0.0041, -0.0068,\n",
      "          0.0356, -0.0066,  0.2110,  0.0721],\n",
      "        [-0.0461,  0.0538, -0.1526, -0.0809,  0.0630, -0.1295, -0.1365,  0.1299,\n",
      "          0.2166,  0.2066, -0.1786,  0.1998,  0.0414, -0.0527,  0.1221,  0.1712,\n",
      "         -0.0119, -0.0792, -0.1423,  0.0707],\n",
      "        [ 0.0745,  0.0465,  0.1531,  0.0478, -0.1045, -0.0671,  0.0366,  0.2224,\n",
      "          0.0909, -0.1490,  0.1774,  0.0664,  0.0521,  0.0403,  0.0097,  0.0299,\n",
      "         -0.1471, -0.1306, -0.0160, -0.1622],\n",
      "        [-0.0362, -0.2098, -0.2085,  0.0508,  0.1822,  0.0472, -0.1867, -0.0464,\n",
      "         -0.1248,  0.1843, -0.0675,  0.1817,  0.1209,  0.0171,  0.1986, -0.0383,\n",
      "         -0.2025,  0.1534, -0.1359,  0.1569],\n",
      "        [-0.1284,  0.1751, -0.0032,  0.0419,  0.1029, -0.1147,  0.0042,  0.1004,\n",
      "          0.0069,  0.1512, -0.1669,  0.1021,  0.0289,  0.1365,  0.2063, -0.0664,\n",
      "          0.2031, -0.1099, -0.0088,  0.1096],\n",
      "        [-0.0424, -0.2188, -0.1940, -0.1160, -0.0432, -0.0443, -0.0217,  0.1119,\n",
      "          0.1612, -0.0415, -0.1444,  0.0423, -0.0482,  0.1012,  0.1908, -0.0350,\n",
      "          0.1408,  0.0162, -0.0700, -0.1923],\n",
      "        [-0.1559, -0.1647, -0.1828, -0.1053, -0.0733,  0.1973, -0.1331,  0.0995,\n",
      "         -0.1414,  0.1525, -0.0095, -0.2153, -0.2086, -0.1318, -0.0327, -0.1267,\n",
      "          0.1946, -0.1967,  0.0255,  0.0338]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1780,  0.0633, -0.0982,  0.0723, -0.1915, -0.1531, -0.1975,  0.1215,\n",
      "        -0.0282, -0.0328,  0.0008,  0.1149,  0.1177, -0.1304,  0.0952, -0.0514,\n",
      "         0.1645,  0.1018, -0.1882, -0.1807], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-6.5896e-02, -2.0158e-01, -1.9241e-03, -1.4217e-01,  1.6650e-01,\n",
      "          8.9796e-02, -4.9180e-02,  1.9721e-01, -1.2195e-01,  1.8720e-01,\n",
      "          1.3075e-01,  1.8075e-02, -1.7059e-01,  1.9682e-01,  6.6683e-02,\n",
      "          6.0688e-02,  4.3499e-02,  1.3293e-01,  3.9510e-02,  5.2297e-02],\n",
      "        [-1.3095e-01, -1.0021e-01, -2.1726e-01,  6.2033e-02,  2.1474e-01,\n",
      "          1.6321e-01, -1.0467e-01,  7.9400e-02,  7.7668e-02, -2.1786e-02,\n",
      "          1.8442e-01,  1.2194e-01, -1.4357e-01, -1.1243e-01, -1.6561e-01,\n",
      "         -1.4964e-01,  8.9176e-02,  2.6525e-02,  3.1162e-03,  1.1627e-01],\n",
      "        [-5.3665e-02,  9.8895e-02,  1.7871e-01, -3.5561e-02,  1.0651e-01,\n",
      "         -5.7072e-02,  7.9185e-02,  1.2824e-01, -7.4834e-02,  6.6331e-02,\n",
      "         -6.6311e-02,  3.6848e-03,  2.7115e-02,  1.8102e-01, -1.6954e-01,\n",
      "         -1.0052e-01, -1.8205e-01, -4.0275e-02, -1.5018e-01,  7.7766e-03],\n",
      "        [ 1.1379e-01,  1.0438e-01,  1.5638e-01, -4.5063e-02, -2.3300e-02,\n",
      "         -1.6784e-02,  3.5314e-02, -2.1139e-01, -1.2568e-01, -1.4795e-01,\n",
      "         -1.0847e-01, -1.0711e-02,  4.1327e-04,  1.6779e-01,  1.2177e-01,\n",
      "          1.2549e-01,  1.8650e-01, -4.9899e-02, -9.8297e-02, -1.4428e-01],\n",
      "        [-1.4865e-01,  1.6996e-01, -1.8773e-01,  8.6542e-02,  1.2564e-01,\n",
      "          1.3300e-01,  3.2629e-02, -2.0894e-01, -8.3592e-02, -1.9141e-01,\n",
      "          8.3132e-02,  1.5659e-01,  9.5724e-02, -1.4970e-01,  1.2870e-01,\n",
      "          9.8218e-02, -2.0875e-01,  2.0229e-01,  2.6172e-02, -1.0832e-01],\n",
      "        [ 2.0658e-01,  2.1256e-02, -3.2020e-03, -3.3980e-02, -1.4683e-01,\n",
      "          1.5837e-02,  2.0438e-01, -7.1738e-02, -6.8711e-03,  2.0446e-01,\n",
      "         -1.4120e-01, -6.3644e-02, -4.7950e-02, -6.9593e-02,  3.9679e-02,\n",
      "          1.4180e-01, -1.1158e-01,  9.4970e-02, -3.9206e-02,  2.1728e-01],\n",
      "        [ 9.8301e-02,  2.1416e-01,  8.3968e-02, -1.1066e-01, -2.0749e-01,\n",
      "         -1.6930e-03, -1.1294e-01, -8.1977e-03,  1.1565e-01, -7.4295e-02,\n",
      "          9.8483e-02, -1.0975e-01,  6.6567e-02,  1.7039e-01,  1.8830e-01,\n",
      "          6.6762e-02,  2.1905e-01, -1.3019e-03,  2.0522e-01, -4.9887e-02],\n",
      "        [-1.3369e-01,  4.8660e-02,  1.8682e-01,  3.5912e-02, -3.5556e-02,\n",
      "         -9.8543e-02,  6.7370e-02, -9.1919e-03, -4.3483e-02,  3.7842e-02,\n",
      "         -9.7006e-02, -1.5957e-02, -2.2225e-01, -1.0144e-01, -1.7591e-02,\n",
      "         -1.4669e-01, -1.3473e-01,  1.3534e-01, -1.5641e-01,  3.9915e-02],\n",
      "        [-2.1851e-01, -6.8724e-02,  3.7685e-02, -1.0946e-01, -2.1357e-01,\n",
      "         -1.7442e-01,  1.6091e-01,  2.0950e-02, -2.6132e-02, -1.1359e-01,\n",
      "          4.4894e-02,  1.9142e-01, -1.2905e-01, -8.0968e-02,  1.9042e-01,\n",
      "          1.2630e-01,  1.4689e-01,  8.7236e-02,  1.4360e-01, -9.3321e-02],\n",
      "        [ 1.2107e-01,  7.9054e-02, -6.7361e-02, -1.4457e-02,  5.1897e-02,\n",
      "          9.7607e-02,  9.7914e-03,  3.8241e-02,  1.4118e-01, -8.2526e-02,\n",
      "         -1.1690e-02,  3.3650e-02, -1.9177e-01,  8.1742e-03, -1.8653e-01,\n",
      "          1.6235e-01, -4.9626e-02, -1.7940e-02, -1.6133e-01,  4.2689e-02],\n",
      "        [-4.7900e-02, -1.0284e-01, -7.4869e-02,  1.8735e-01, -3.7095e-02,\n",
      "         -1.2466e-01, -1.1469e-01, -2.1809e-01,  1.6872e-01, -1.6513e-01,\n",
      "         -1.0822e-01, -2.0548e-01, -3.3536e-03,  1.9944e-01,  2.0456e-02,\n",
      "         -1.7172e-01,  1.1478e-01, -2.0818e-01, -8.7895e-02, -2.2312e-01],\n",
      "        [ 3.2181e-02, -9.2289e-02,  6.6450e-02, -1.8156e-01,  2.0766e-01,\n",
      "          3.0903e-02, -3.8449e-02,  1.4388e-01,  1.3167e-02, -2.0262e-01,\n",
      "         -1.0517e-01,  2.2103e-01,  1.8534e-01, -3.5715e-02, -1.8229e-01,\n",
      "          1.3880e-01, -1.7816e-01,  1.3759e-01,  2.2142e-01,  1.0527e-01],\n",
      "        [ 8.9655e-02, -8.6767e-02,  3.1753e-02,  9.4050e-02, -1.6407e-01,\n",
      "          4.1161e-02, -1.0749e-01, -6.1640e-02, -7.4453e-02, -2.1433e-01,\n",
      "         -7.8959e-02, -1.8290e-03,  2.2339e-01,  7.2015e-02,  1.6184e-01,\n",
      "          1.7029e-01, -5.9658e-02,  2.2048e-01,  3.1354e-02,  2.1745e-02],\n",
      "        [-1.9672e-01,  7.8704e-02,  7.3419e-02,  1.1789e-01,  7.5584e-02,\n",
      "          3.4257e-02,  2.0351e-01,  7.9451e-02, -1.8229e-01,  2.6867e-02,\n",
      "          1.0588e-01,  6.2786e-03, -1.8074e-01, -1.0954e-02,  1.6104e-01,\n",
      "         -1.0505e-01,  9.8641e-02, -1.7042e-03,  1.1775e-01,  1.4847e-01],\n",
      "        [ 2.4379e-02, -6.6835e-02,  1.5249e-01,  1.7751e-01,  1.7701e-01,\n",
      "         -1.0405e-01,  2.0300e-01,  8.6852e-02,  1.7909e-01,  1.3138e-01,\n",
      "         -1.6396e-01, -1.6673e-01,  3.8763e-02,  8.7705e-02, -3.1407e-02,\n",
      "         -5.7723e-02,  6.7919e-02, -3.7436e-04, -1.3491e-01,  1.9501e-01],\n",
      "        [ 4.5141e-02, -1.6166e-01,  1.4874e-01, -5.5996e-02, -3.5768e-02,\n",
      "         -1.8795e-01, -1.3937e-01, -6.7379e-03, -1.8472e-01, -1.7974e-01,\n",
      "          1.0797e-01, -7.7662e-02, -2.0143e-02,  2.2044e-01,  1.8630e-01,\n",
      "          1.0384e-01,  1.3682e-01,  6.8450e-02, -2.6599e-02,  5.0187e-03],\n",
      "        [ 6.0559e-02, -1.6064e-01, -1.9466e-01, -1.8405e-02,  2.3904e-02,\n",
      "          1.0178e-01,  8.0755e-03, -1.5188e-02, -1.1599e-01, -2.1596e-01,\n",
      "          1.0047e-01,  6.5837e-02,  1.2473e-01,  1.0943e-01, -1.2091e-01,\n",
      "          2.1294e-01,  1.6577e-01, -5.2736e-02,  9.1868e-02, -1.0822e-01],\n",
      "        [ 1.5899e-01, -9.9413e-02, -2.3958e-02, -1.7817e-01, -1.8190e-01,\n",
      "          1.8817e-01, -1.6573e-01, -1.8037e-01,  1.9528e-01, -9.2633e-02,\n",
      "          2.4275e-02, -9.0724e-02, -2.1944e-01,  2.9018e-02,  6.4378e-02,\n",
      "         -4.8088e-02, -1.3503e-02, -6.4344e-02, -1.4306e-01,  1.6540e-01],\n",
      "        [-1.9227e-01,  7.3662e-02,  2.2006e-02, -2.1579e-01, -6.7163e-02,\n",
      "         -4.0052e-02, -1.1112e-01,  1.3576e-01, -9.7168e-02,  1.2516e-01,\n",
      "         -1.2648e-01,  8.5011e-05, -1.3640e-01,  3.0252e-03,  7.7501e-02,\n",
      "         -1.1261e-01,  2.2221e-01,  5.9021e-02, -6.7246e-02,  1.7052e-01],\n",
      "        [ 1.6099e-01,  2.1976e-01,  7.2298e-02, -1.9986e-01, -1.2053e-01,\n",
      "         -1.9707e-01, -2.1508e-01,  6.1677e-02, -2.2120e-01, -1.3876e-01,\n",
      "          2.2744e-02,  1.5995e-01, -1.8856e-01, -2.1657e-01,  1.4649e-01,\n",
      "         -2.0149e-01,  1.5991e-01,  9.6441e-02,  1.6041e-01,  5.8343e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1290, -0.0200, -0.0317,  0.0367, -0.0830, -0.1512, -0.1730,  0.1287,\n",
      "        -0.0076,  0.0107,  0.1447, -0.2004, -0.1618, -0.0015,  0.0516,  0.1732,\n",
      "         0.2068,  0.1922, -0.1949, -0.2121], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1395,  0.1860, -0.1037,  0.0113, -0.1256, -0.1013, -0.0196,  0.1821,\n",
      "          0.0910, -0.1702,  0.1485, -0.2133, -0.0423, -0.2068, -0.1199,  0.2008,\n",
      "         -0.0987, -0.1313,  0.0576,  0.2092],\n",
      "        [-0.1373, -0.1199,  0.1175, -0.1890, -0.1593,  0.0084,  0.0229, -0.0077,\n",
      "         -0.1855,  0.1777,  0.0367, -0.2175,  0.1465, -0.0895,  0.0457, -0.2059,\n",
      "          0.1827,  0.0465, -0.1241, -0.0771],\n",
      "        [-0.1141,  0.2227,  0.1005, -0.1204,  0.1471,  0.1231, -0.1653,  0.0151,\n",
      "         -0.1339, -0.1984, -0.0564,  0.1233,  0.0517, -0.1564, -0.1077, -0.2170,\n",
      "          0.0697, -0.1716,  0.0585,  0.1314],\n",
      "        [-0.0258,  0.1675,  0.0326, -0.1202, -0.1383,  0.0267,  0.1213, -0.1755,\n",
      "         -0.1026,  0.0695, -0.1836,  0.0829, -0.0566, -0.0724, -0.1431, -0.1750,\n",
      "         -0.1410, -0.0840, -0.1004, -0.1884],\n",
      "        [-0.1951,  0.0862, -0.0282,  0.0546,  0.0069, -0.1848,  0.1213,  0.1517,\n",
      "         -0.2005, -0.1538, -0.1145,  0.1936,  0.1205, -0.2196, -0.1519, -0.1571,\n",
      "         -0.0139,  0.0418, -0.1149,  0.0102],\n",
      "        [ 0.0281, -0.0212, -0.0229, -0.1002,  0.0865,  0.2186,  0.2038,  0.2024,\n",
      "          0.0291, -0.1257, -0.2145, -0.1425, -0.1189, -0.0668, -0.1139,  0.1051,\n",
      "         -0.1742, -0.1209, -0.1209, -0.1876],\n",
      "        [-0.0252,  0.1094, -0.2223,  0.2010,  0.1541, -0.1031, -0.0235,  0.0722,\n",
      "          0.1113, -0.1164, -0.0886, -0.0565, -0.2145,  0.1586, -0.0060, -0.0243,\n",
      "         -0.1202,  0.1654,  0.1879,  0.0872],\n",
      "        [-0.0561, -0.1625, -0.0913, -0.2110, -0.1146,  0.1875, -0.1150,  0.1115,\n",
      "         -0.0098,  0.0587,  0.1066, -0.1274, -0.0155,  0.1238, -0.1311, -0.1611,\n",
      "         -0.1266, -0.2205, -0.0343, -0.1531],\n",
      "        [-0.0901, -0.0096,  0.1955,  0.1378, -0.1215, -0.1555,  0.0370,  0.1453,\n",
      "         -0.2143, -0.0098, -0.0758,  0.1898, -0.0682, -0.1473, -0.1125,  0.1572,\n",
      "         -0.0805,  0.1284,  0.0307, -0.1410],\n",
      "        [ 0.1666, -0.0945, -0.0087, -0.1158, -0.1509,  0.1680,  0.1408,  0.2128,\n",
      "         -0.1844, -0.1404,  0.1439, -0.0681, -0.0762,  0.1772,  0.2035,  0.1116,\n",
      "          0.0791,  0.2071,  0.1534,  0.1067]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0255, -0.0476, -0.0339, -0.1867, -0.1938,  0.0105, -0.1443, -0.1176,\n",
      "         0.1672,  0.0080], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0859,  0.2790, -0.2850, -0.2456, -0.2008, -0.2786, -0.3139, -0.1229,\n",
      "         -0.1095, -0.2794],\n",
      "        [ 0.0092, -0.2705,  0.1237,  0.0310,  0.2961,  0.2123, -0.2074,  0.1907,\n",
      "         -0.0881, -0.0362],\n",
      "        [ 0.0463, -0.1738,  0.1584, -0.1157, -0.1280, -0.2561,  0.2951, -0.2318,\n",
      "         -0.2052,  0.0108],\n",
      "        [ 0.1702,  0.2018,  0.0416, -0.2250,  0.1481,  0.1700,  0.0961,  0.1911,\n",
      "         -0.1796, -0.1867],\n",
      "        [-0.1963, -0.3074,  0.1416,  0.1739,  0.1572, -0.2527, -0.2784, -0.1849,\n",
      "         -0.1822, -0.2748],\n",
      "        [-0.2624,  0.0930, -0.1875, -0.1406,  0.2883,  0.0471,  0.1156,  0.2167,\n",
      "          0.1382,  0.3035],\n",
      "        [ 0.0856, -0.2795,  0.3124, -0.0551, -0.0036,  0.2439,  0.0688,  0.2440,\n",
      "         -0.1765,  0.1725],\n",
      "        [-0.1302, -0.0375, -0.0464, -0.0629, -0.0341, -0.0549,  0.2709,  0.2488,\n",
      "          0.0238,  0.0598],\n",
      "        [-0.2731, -0.2604, -0.2839,  0.1738, -0.0669, -0.0544,  0.2402, -0.0164,\n",
      "          0.0327,  0.0372],\n",
      "        [ 0.0935,  0.1213,  0.1340,  0.1516,  0.1133, -0.1592,  0.1265,  0.3078,\n",
      "          0.1611, -0.2305],\n",
      "        [-0.2559, -0.0818, -0.2728,  0.1347, -0.1654, -0.1745, -0.1099, -0.1423,\n",
      "         -0.0021,  0.1949],\n",
      "        [ 0.0194, -0.0026, -0.0108, -0.1782, -0.1586,  0.2686,  0.2808, -0.0371,\n",
      "         -0.2879, -0.1535],\n",
      "        [-0.0806, -0.0731,  0.0566,  0.1272,  0.1353, -0.0315,  0.0901,  0.2215,\n",
      "          0.1756,  0.1463],\n",
      "        [-0.2698,  0.0411, -0.0188, -0.0429, -0.2821,  0.0543, -0.2500,  0.2040,\n",
      "          0.2255,  0.1009],\n",
      "        [-0.2411,  0.2891,  0.2972,  0.1938,  0.1272, -0.3153,  0.2645, -0.1564,\n",
      "          0.3129, -0.2682],\n",
      "        [-0.2788,  0.1624,  0.0545, -0.1716, -0.0566,  0.0926, -0.2824,  0.2966,\n",
      "         -0.2001,  0.1203]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1063,  0.0013,  0.1872, -0.0773,  0.1724, -0.2838,  0.2769, -0.1909,\n",
      "        -0.0382, -0.2527, -0.1592, -0.2113, -0.0417,  0.2071, -0.2014,  0.1478],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2633, -0.1275, -0.2342,  0.1490, -0.1107, -0.0418,  0.2837, -0.0860],\n",
      "        [-0.1009, -0.0473, -0.1342, -0.2038, -0.0060, -0.2682,  0.3335, -0.1950],\n",
      "        [ 0.0132,  0.2433,  0.1661, -0.0983, -0.2150,  0.2331, -0.3130,  0.1985],\n",
      "        [ 0.2927,  0.1446, -0.2093, -0.2866,  0.1433, -0.0966,  0.3065,  0.1817],\n",
      "        [ 0.2361,  0.2501, -0.2684, -0.0074,  0.1377, -0.0252, -0.3247, -0.2981],\n",
      "        [-0.0818, -0.0483,  0.1568, -0.0841,  0.1162,  0.3455,  0.2129, -0.2980],\n",
      "        [-0.2756,  0.0019,  0.2733,  0.2069,  0.1177, -0.3483,  0.1861,  0.2825],\n",
      "        [ 0.0440, -0.3197,  0.2201,  0.2576,  0.2133, -0.0465,  0.1927, -0.0770],\n",
      "        [-0.2766,  0.0163, -0.0301, -0.0196,  0.0192,  0.1384,  0.3527, -0.2755],\n",
      "        [ 0.2458, -0.2502, -0.0635,  0.0794,  0.0656, -0.2837, -0.2082,  0.0363]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0961, -0.2699,  0.0358,  0.1530,  0.0134,  0.1235, -0.2217,  0.0838,\n",
      "        -0.3271, -0.3284], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.1485e-03,  7.4373e-02,  2.3936e-01, -2.6783e-01,  1.1297e-02,\n",
      "         -2.8367e-01,  1.2777e-01, -7.2532e-02, -2.5380e-01,  3.0860e-01],\n",
      "        [ 1.9160e-02, -9.1501e-02, -2.1993e-01,  3.1248e-01, -7.4648e-02,\n",
      "         -1.5016e-01, -9.4671e-02, -2.8179e-01, -1.4234e-02,  1.0828e-01],\n",
      "        [ 2.1279e-01, -1.6086e-01,  1.9000e-01,  2.5223e-01, -1.9815e-01,\n",
      "          1.0420e-01,  4.4367e-02, -2.0625e-02,  2.4678e-02, -1.0180e-01],\n",
      "        [-1.2249e-01, -1.8401e-01, -2.5112e-01, -7.7250e-02,  1.6210e-01,\n",
      "          2.9963e-02,  2.3302e-01, -1.0142e-02, -3.1136e-01,  2.5115e-02],\n",
      "        [-2.2712e-01, -6.1667e-02, -2.2000e-01,  2.1670e-01,  2.6645e-01,\n",
      "         -2.1802e-01,  2.5690e-01, -7.0805e-02,  3.7539e-02,  2.7883e-01],\n",
      "        [-1.2813e-01, -6.3395e-02,  2.0861e-01, -2.9947e-02, -7.5712e-02,\n",
      "         -5.8273e-02,  1.7969e-01,  3.1050e-01,  1.9216e-01, -1.7972e-01],\n",
      "        [ 2.7896e-01, -1.3455e-01,  3.0994e-01, -1.4645e-01, -1.1538e-01,\n",
      "          2.8742e-01, -3.1555e-01,  2.4315e-01,  2.0375e-01,  2.1389e-01],\n",
      "        [ 7.4755e-02,  5.8211e-02, -1.8546e-01,  2.5152e-01, -1.3708e-01,\n",
      "         -9.7257e-02, -2.0340e-01,  2.8733e-01, -5.8774e-02, -3.0740e-01],\n",
      "        [ 2.1668e-01,  2.5552e-01,  1.2478e-01,  1.1175e-01, -3.8206e-02,\n",
      "         -8.8969e-02,  2.8145e-01, -1.6313e-01, -3.0457e-02, -3.7575e-02],\n",
      "        [-2.0451e-01, -2.3175e-01, -3.3310e-02,  1.3553e-01, -1.9723e-02,\n",
      "         -2.5597e-01, -2.4002e-01,  1.7947e-04, -2.8242e-01,  1.5707e-01],\n",
      "        [ 2.8075e-01,  1.7598e-01, -1.3471e-01,  1.7001e-01, -1.9791e-01,\n",
      "         -1.4427e-02, -1.9679e-01,  2.5078e-01, -5.8911e-02,  2.2952e-01],\n",
      "        [-2.4178e-01,  2.6154e-01,  1.4852e-01, -3.0599e-01, -1.0384e-01,\n",
      "          2.0309e-01,  1.2229e-01,  1.5511e-01, -1.6831e-01,  2.4762e-01],\n",
      "        [-1.2425e-01,  1.5889e-01, -2.2643e-01,  2.2871e-02, -3.0284e-01,\n",
      "         -1.7653e-01, -5.6830e-02, -1.5310e-01, -1.4937e-01, -3.1231e-01],\n",
      "        [-2.4542e-01, -2.9823e-01,  2.6657e-01,  1.1344e-01, -2.8791e-02,\n",
      "          2.9806e-01,  1.1771e-01,  2.8186e-01,  1.5360e-02,  2.0291e-01],\n",
      "        [-2.3936e-01, -6.7287e-03,  2.4507e-01, -2.9282e-01, -1.4543e-01,\n",
      "         -2.5410e-01,  3.0625e-01,  6.5912e-03,  1.1709e-01,  7.3927e-03],\n",
      "        [ 2.0473e-01, -3.1018e-01, -7.9764e-02,  2.7430e-01,  1.8209e-01,\n",
      "          2.6287e-01,  8.2472e-02,  1.6249e-01, -5.2365e-02,  1.0349e-01],\n",
      "        [-1.4425e-01,  7.9459e-02,  2.0202e-01,  4.6543e-03, -1.7486e-02,\n",
      "          1.1660e-01,  1.3497e-01,  2.9002e-01,  2.3734e-01,  7.8135e-02],\n",
      "        [ 1.0693e-01,  2.1930e-01, -3.6290e-04,  2.9584e-02, -1.6227e-01,\n",
      "         -2.1726e-01, -1.8706e-01, -1.6089e-01, -1.4712e-01,  1.6333e-01],\n",
      "        [-2.6563e-02, -3.9201e-02, -4.8141e-02, -2.2456e-01,  2.7220e-01,\n",
      "         -1.5058e-01, -1.8983e-01, -6.8905e-02,  8.5593e-02,  1.1843e-01],\n",
      "        [-9.6708e-03,  2.4548e-01, -1.5894e-02,  1.9171e-01,  7.5218e-02,\n",
      "         -8.2706e-02,  2.8734e-01, -2.2270e-01,  2.6197e-01, -1.1132e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1020,  0.0558,  0.0851,  0.2416,  0.0019,  0.0422,  0.0100, -0.2306,\n",
      "         0.2747, -0.1164, -0.0061, -0.2365,  0.1516,  0.0397,  0.1268,  0.0539,\n",
      "        -0.1318, -0.2333,  0.2883,  0.2406], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1680, -0.0377,  0.1071,  0.1168, -0.0345,  0.0808,  0.0353, -0.0431,\n",
      "          0.0592, -0.1460,  0.0038, -0.1346,  0.0884, -0.1882,  0.1627,  0.0900,\n",
      "          0.0693,  0.1982,  0.0799,  0.1463],\n",
      "        [ 0.0569,  0.0620,  0.0443, -0.1920, -0.1607,  0.1480,  0.0182, -0.1916,\n",
      "         -0.0255, -0.1448, -0.1426, -0.1924, -0.1566, -0.0314, -0.1457,  0.0086,\n",
      "          0.1263, -0.1103,  0.1516, -0.1279],\n",
      "        [-0.1466, -0.1202,  0.1639,  0.1809,  0.0226, -0.1103, -0.0344, -0.0192,\n",
      "         -0.0949,  0.0392,  0.0585, -0.1407, -0.1496, -0.1400,  0.0091,  0.1906,\n",
      "          0.1207,  0.1578,  0.1276,  0.1301],\n",
      "        [ 0.0219,  0.1717,  0.0897, -0.1282,  0.0474,  0.1035, -0.1326, -0.0085,\n",
      "         -0.0026, -0.0140,  0.1942, -0.0804, -0.1595, -0.0357, -0.1867,  0.1542,\n",
      "          0.0285,  0.2192,  0.1119, -0.0105],\n",
      "        [-0.0328, -0.1271,  0.2096, -0.0539, -0.0432, -0.0825, -0.2146,  0.0604,\n",
      "          0.1100, -0.0112, -0.0292,  0.0363, -0.0331, -0.0222, -0.1113, -0.0297,\n",
      "         -0.0088,  0.1496,  0.0851, -0.1483],\n",
      "        [-0.1263,  0.0114, -0.0284, -0.1074, -0.1085, -0.0080, -0.1864, -0.0355,\n",
      "         -0.1718,  0.0175,  0.0898, -0.2073,  0.1972,  0.0499, -0.2175, -0.1846,\n",
      "         -0.1394,  0.0476,  0.0343, -0.0829],\n",
      "        [-0.0070,  0.2006, -0.0352,  0.0731, -0.0149,  0.1041, -0.0935,  0.1554,\n",
      "          0.0937,  0.0926,  0.0524, -0.2082,  0.0225, -0.1250, -0.1576,  0.0824,\n",
      "         -0.0727,  0.0818, -0.0304,  0.0181],\n",
      "        [ 0.0548,  0.0666, -0.1214,  0.0223,  0.0274, -0.0278, -0.1561,  0.0149,\n",
      "         -0.2057, -0.1025, -0.0163, -0.1660,  0.0013, -0.1953,  0.0357,  0.0854,\n",
      "          0.1482,  0.0811,  0.1403,  0.1889],\n",
      "        [-0.0794, -0.0385,  0.0188, -0.1124,  0.1597, -0.1036,  0.1486,  0.1967,\n",
      "          0.0696, -0.0529,  0.1591,  0.0302, -0.0860, -0.1609, -0.1523, -0.0219,\n",
      "          0.0952, -0.0533,  0.0108,  0.0140],\n",
      "        [ 0.0354,  0.1154, -0.1740,  0.0389, -0.1023, -0.0406,  0.1063,  0.0897,\n",
      "         -0.0178,  0.1445, -0.0584,  0.1105,  0.0294,  0.1337, -0.2060, -0.1560,\n",
      "         -0.0314,  0.0969,  0.2095, -0.0986],\n",
      "        [ 0.0653,  0.0035,  0.0053, -0.1190,  0.0062,  0.0021,  0.0902,  0.0453,\n",
      "          0.1413, -0.2056, -0.1204, -0.1822,  0.0900,  0.0062, -0.1702, -0.1758,\n",
      "         -0.0860, -0.1690,  0.1627, -0.1196],\n",
      "        [-0.0329, -0.1872, -0.1383,  0.1046,  0.0958,  0.0018,  0.0704, -0.1371,\n",
      "         -0.0676,  0.0193, -0.1919,  0.2129, -0.1850,  0.0843,  0.1522, -0.0023,\n",
      "         -0.2208,  0.1502,  0.0520,  0.1510],\n",
      "        [-0.1137, -0.1874, -0.1922,  0.1664, -0.1054,  0.2116,  0.0947,  0.1429,\n",
      "          0.0080, -0.0035,  0.1647,  0.0186,  0.1131,  0.0623,  0.0055,  0.1203,\n",
      "          0.0657, -0.0254,  0.0808, -0.0504],\n",
      "        [ 0.0459,  0.1309, -0.1146, -0.0751, -0.0115, -0.0108,  0.1704,  0.0536,\n",
      "          0.1966, -0.0559,  0.0195,  0.0477, -0.0805, -0.1253,  0.0749, -0.0928,\n",
      "         -0.2029, -0.2052, -0.1883, -0.0909],\n",
      "        [ 0.1464, -0.1938, -0.0897,  0.1339,  0.1111, -0.0308,  0.0553,  0.1402,\n",
      "          0.0830,  0.2166,  0.0813, -0.2152, -0.0821, -0.1332,  0.1374,  0.2193,\n",
      "          0.1437, -0.1372,  0.0071, -0.0590],\n",
      "        [ 0.0746, -0.1748,  0.2050,  0.0411, -0.2234, -0.1539,  0.1561, -0.1347,\n",
      "         -0.0237, -0.1084,  0.0325, -0.1357, -0.0679, -0.2094,  0.1100,  0.1315,\n",
      "         -0.0710, -0.0771,  0.0997, -0.1868],\n",
      "        [-0.0295,  0.1600,  0.0702,  0.0713, -0.1778, -0.1888,  0.2101,  0.1789,\n",
      "          0.0539, -0.0327, -0.0786, -0.1755,  0.0345,  0.1165,  0.1442,  0.0178,\n",
      "         -0.1889, -0.1847,  0.1360,  0.1377],\n",
      "        [-0.0297,  0.1036, -0.1016, -0.2134, -0.1439,  0.1313,  0.2023, -0.0208,\n",
      "         -0.0413, -0.1039,  0.0106,  0.0865,  0.0963, -0.0663,  0.1470, -0.0508,\n",
      "         -0.1676,  0.1790,  0.1788, -0.0509],\n",
      "        [-0.1801, -0.1024, -0.0451,  0.1073, -0.1875, -0.1411, -0.1081,  0.0427,\n",
      "         -0.2195, -0.0308,  0.1916,  0.2021,  0.1652,  0.1469,  0.1156,  0.1964,\n",
      "          0.1941,  0.0481,  0.0878, -0.1080],\n",
      "        [-0.0009, -0.1035,  0.2142, -0.0172, -0.0533, -0.0521,  0.0624,  0.0480,\n",
      "         -0.1534,  0.1102,  0.1609, -0.0802,  0.2047, -0.1930,  0.1526, -0.0539,\n",
      "         -0.0233, -0.1874,  0.0036,  0.0328]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0465, -0.0398,  0.1732,  0.0634, -0.1932, -0.0512,  0.2184, -0.0587,\n",
      "         0.0741,  0.1928, -0.0536,  0.1336,  0.1585,  0.1045, -0.0247, -0.0560,\n",
      "        -0.1646, -0.1426,  0.1791,  0.1334], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0099,  0.0428, -0.0676, -0.1263, -0.0607,  0.1166,  0.0371, -0.0483,\n",
      "         -0.0345, -0.2080, -0.0064,  0.1170, -0.2139, -0.1718, -0.1136,  0.1200,\n",
      "         -0.1581, -0.0244, -0.1870, -0.1539],\n",
      "        [ 0.0184,  0.0266, -0.0354,  0.1792,  0.0307, -0.1762, -0.0713,  0.0051,\n",
      "         -0.0913,  0.0372, -0.0449, -0.0865, -0.0201, -0.2121, -0.0690, -0.0471,\n",
      "          0.1562, -0.1341,  0.2231,  0.0759],\n",
      "        [-0.1481, -0.0280,  0.1301, -0.1776,  0.1550,  0.0098, -0.1673,  0.0776,\n",
      "         -0.0168,  0.0971, -0.1088, -0.2171, -0.1506,  0.1569,  0.0520,  0.0322,\n",
      "         -0.1743,  0.1314,  0.1845,  0.0992],\n",
      "        [ 0.1352,  0.1760,  0.1648, -0.0980,  0.1700,  0.0995,  0.0026,  0.0944,\n",
      "         -0.0262,  0.1678,  0.1520,  0.2107, -0.1591, -0.0109,  0.0159,  0.0093,\n",
      "          0.1494, -0.0942,  0.0481, -0.0087],\n",
      "        [-0.0947, -0.0821, -0.0544, -0.1933,  0.0861,  0.1208, -0.0044,  0.0110,\n",
      "          0.1805, -0.2056, -0.1477, -0.0117,  0.0961, -0.0181,  0.0268, -0.1005,\n",
      "          0.1484,  0.1262, -0.0426,  0.0551],\n",
      "        [-0.0715, -0.1645, -0.0315,  0.2001,  0.0986, -0.0939,  0.1329, -0.1015,\n",
      "          0.0008, -0.1132,  0.0503,  0.0091, -0.0675,  0.2014, -0.1851, -0.2146,\n",
      "          0.1965, -0.0965, -0.0574,  0.0380],\n",
      "        [-0.1558, -0.1657,  0.0380,  0.1883, -0.0595,  0.1867,  0.1718, -0.1630,\n",
      "          0.1911, -0.1980, -0.1938, -0.2103,  0.0407,  0.0263, -0.1960, -0.1513,\n",
      "          0.1823, -0.0212, -0.0710, -0.2018],\n",
      "        [-0.1664, -0.1217,  0.0880,  0.0949, -0.1564, -0.0982,  0.0769, -0.0793,\n",
      "         -0.0532, -0.0232, -0.0970,  0.0014,  0.2021, -0.0495,  0.0036,  0.0917,\n",
      "          0.0685,  0.1818, -0.2190,  0.2165],\n",
      "        [ 0.1441,  0.0255, -0.0450, -0.1045, -0.1393,  0.0230, -0.0694, -0.1169,\n",
      "         -0.0840, -0.0406, -0.2075, -0.1521,  0.1827,  0.0146,  0.0758,  0.0961,\n",
      "         -0.0283,  0.1224,  0.0619,  0.2173],\n",
      "        [ 0.0375,  0.1292, -0.0094, -0.1086,  0.0207,  0.0867,  0.0267, -0.2134,\n",
      "          0.1684,  0.0876,  0.0860, -0.1306, -0.0632,  0.0567,  0.0451,  0.1661,\n",
      "          0.1332, -0.0413, -0.1934, -0.1561],\n",
      "        [ 0.1747, -0.0830,  0.0942, -0.0279,  0.0128,  0.0727,  0.1403, -0.0079,\n",
      "         -0.2165,  0.1894,  0.0031,  0.0211, -0.1745, -0.0898,  0.1505,  0.1266,\n",
      "         -0.1324, -0.1892, -0.1982, -0.1826],\n",
      "        [-0.1164,  0.1989, -0.0424, -0.0197, -0.1464, -0.1921,  0.0473,  0.1500,\n",
      "         -0.0819,  0.1282, -0.1161, -0.1163, -0.0405,  0.1636, -0.0600, -0.1259,\n",
      "          0.2067, -0.0647,  0.1302,  0.0252],\n",
      "        [-0.2022, -0.0535,  0.0452, -0.1746,  0.1168,  0.0322, -0.1718,  0.1703,\n",
      "          0.0600, -0.0912,  0.1455,  0.0278,  0.1722, -0.0964, -0.0670, -0.1174,\n",
      "         -0.1938,  0.0285, -0.1893, -0.1092],\n",
      "        [ 0.1078, -0.2047, -0.1627,  0.1723, -0.1457,  0.0627,  0.1858,  0.1086,\n",
      "          0.0315, -0.0432,  0.1902, -0.0044, -0.1978,  0.0693, -0.1266,  0.0871,\n",
      "          0.1865, -0.0585, -0.1305, -0.0464],\n",
      "        [-0.1232, -0.0462,  0.1894, -0.1892,  0.2113,  0.1012,  0.0419, -0.0274,\n",
      "          0.1757,  0.0405, -0.0045,  0.0496, -0.1697, -0.0944,  0.0635,  0.1421,\n",
      "         -0.0440,  0.1399, -0.0412,  0.1006],\n",
      "        [ 0.1200,  0.0507, -0.0613,  0.2140, -0.1801, -0.0631,  0.0644, -0.2162,\n",
      "         -0.1398, -0.0826,  0.1594,  0.1511,  0.1661, -0.2161, -0.0869, -0.2045,\n",
      "         -0.0150, -0.1812, -0.0674,  0.1175],\n",
      "        [-0.0928,  0.0089, -0.1557, -0.1528,  0.0822, -0.0973, -0.0884, -0.1502,\n",
      "         -0.1835, -0.0962,  0.0731,  0.1075,  0.0309,  0.1365, -0.1095,  0.0275,\n",
      "         -0.0639,  0.0184,  0.1093, -0.2169],\n",
      "        [ 0.1828, -0.1743, -0.0870,  0.0743,  0.0101,  0.0626, -0.2001, -0.0684,\n",
      "         -0.0470,  0.0925, -0.1221, -0.0190, -0.1654, -0.1298, -0.0016,  0.2126,\n",
      "          0.1144, -0.2226,  0.0503, -0.0148],\n",
      "        [-0.0173, -0.0154, -0.0231, -0.1076, -0.1323,  0.1953,  0.0970,  0.1379,\n",
      "          0.1126, -0.0877,  0.0277,  0.1218,  0.1053,  0.0312, -0.1984,  0.1266,\n",
      "          0.0961, -0.2142, -0.1548, -0.0126],\n",
      "        [-0.2097, -0.1876, -0.1059, -0.1640,  0.1444,  0.1646,  0.1673,  0.0803,\n",
      "          0.1363,  0.0817,  0.0154, -0.1671, -0.1522, -0.0813,  0.1716,  0.0914,\n",
      "          0.2190, -0.2098, -0.0472,  0.0166]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0651,  0.1059,  0.1762, -0.1025, -0.1568,  0.1841, -0.1659, -0.0353,\n",
      "         0.1037, -0.1618,  0.1708,  0.0479,  0.0537,  0.0284,  0.0251,  0.1646,\n",
      "        -0.2128,  0.2131, -0.0477,  0.1623], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1641, -0.0921,  0.2180, -0.0590, -0.1319,  0.0056, -0.2208, -0.1102,\n",
      "         -0.1760,  0.0695, -0.1005, -0.2088, -0.0734,  0.1869,  0.1547, -0.1620,\n",
      "         -0.0713, -0.1046,  0.2007, -0.1589],\n",
      "        [-0.1079, -0.1616,  0.0466, -0.0557,  0.1523,  0.1967, -0.1171,  0.1615,\n",
      "         -0.1620,  0.1088,  0.1870, -0.0066, -0.0791, -0.1434, -0.0281, -0.1263,\n",
      "         -0.2229,  0.1047, -0.1203, -0.2235],\n",
      "        [-0.0481,  0.2022, -0.1023,  0.2089,  0.1341, -0.0007, -0.2020, -0.0027,\n",
      "         -0.1153,  0.2003,  0.1578, -0.1403,  0.1191,  0.0227, -0.0983, -0.1531,\n",
      "         -0.2212, -0.1069,  0.2222, -0.0661],\n",
      "        [ 0.2117, -0.1374,  0.1897,  0.1060,  0.0988, -0.2123,  0.2037,  0.0111,\n",
      "          0.0817,  0.1808,  0.0191, -0.1953, -0.0755, -0.1745,  0.0875, -0.1003,\n",
      "          0.0994, -0.0472,  0.2101, -0.0451],\n",
      "        [-0.0989, -0.1025, -0.1975, -0.1991, -0.1082, -0.1919, -0.1449,  0.0167,\n",
      "          0.1214,  0.1270,  0.0909,  0.1705,  0.0417, -0.1033, -0.0764,  0.1915,\n",
      "          0.0498,  0.1011, -0.1042, -0.1350],\n",
      "        [-0.1023,  0.0718,  0.0661, -0.0098, -0.2154,  0.0589,  0.1578,  0.2187,\n",
      "         -0.0121, -0.0127,  0.1026, -0.1786, -0.0243, -0.0273, -0.1032, -0.1056,\n",
      "          0.1467, -0.1166, -0.2203, -0.2070],\n",
      "        [-0.1948,  0.0671,  0.2033,  0.1862,  0.1430,  0.0105, -0.0887, -0.1189,\n",
      "         -0.2009, -0.1632, -0.1342, -0.1299,  0.1912, -0.0947, -0.0856, -0.0426,\n",
      "          0.0532,  0.1916, -0.2010,  0.0342],\n",
      "        [-0.2080,  0.0229,  0.0026,  0.2007, -0.0712, -0.1733, -0.1948,  0.1522,\n",
      "         -0.0910, -0.1147, -0.1495, -0.0971, -0.0331,  0.2126, -0.2136,  0.1886,\n",
      "         -0.0781,  0.0083,  0.2180, -0.1784],\n",
      "        [ 0.1778, -0.0619,  0.0393,  0.1518, -0.0825,  0.1504, -0.1976,  0.1829,\n",
      "          0.1793, -0.1037,  0.2029, -0.1116,  0.2161, -0.1412,  0.0713,  0.1016,\n",
      "         -0.0802,  0.1257, -0.2035,  0.0053],\n",
      "        [-0.0231,  0.0668,  0.0502,  0.1316,  0.0971,  0.0386,  0.1442, -0.2075,\n",
      "          0.0214, -0.0294, -0.0287,  0.0782,  0.1723, -0.1739,  0.0948, -0.0352,\n",
      "          0.0043, -0.1255,  0.1511,  0.1138],\n",
      "        [ 0.2013, -0.1244, -0.0672, -0.2051, -0.1623, -0.1605, -0.1984,  0.0774,\n",
      "          0.1884, -0.0033, -0.0098, -0.0768, -0.1642,  0.1993,  0.0332,  0.0942,\n",
      "          0.1693, -0.2073,  0.2065, -0.1400],\n",
      "        [-0.1549, -0.0562, -0.0181, -0.0136,  0.0819, -0.1363, -0.1436, -0.1225,\n",
      "         -0.1627, -0.1437, -0.1861, -0.1877, -0.0467,  0.1300,  0.1991,  0.1842,\n",
      "          0.0299, -0.0494, -0.1846,  0.0329],\n",
      "        [ 0.1964, -0.0933, -0.1544, -0.1676,  0.1544, -0.2225, -0.1648, -0.0237,\n",
      "         -0.1647,  0.0003,  0.1488, -0.0592,  0.1472,  0.1169, -0.0478, -0.2191,\n",
      "         -0.1256, -0.0569, -0.0288,  0.1810],\n",
      "        [ 0.1475,  0.0931, -0.2000,  0.0277,  0.1009,  0.2001, -0.1471,  0.0434,\n",
      "          0.1558,  0.0319,  0.1774,  0.0816,  0.1178, -0.1481, -0.0469, -0.1973,\n",
      "         -0.2111,  0.1297, -0.2181,  0.0596],\n",
      "        [ 0.0068, -0.1822,  0.1027,  0.1709,  0.1371,  0.0711,  0.1500,  0.1090,\n",
      "          0.0818,  0.1754, -0.1916,  0.0755,  0.1888, -0.0750, -0.2197, -0.1310,\n",
      "          0.0935, -0.0963,  0.0203, -0.0546],\n",
      "        [ 0.1582,  0.0814, -0.0418,  0.1245, -0.0904,  0.1016, -0.1724,  0.0330,\n",
      "         -0.0291, -0.1367, -0.1595, -0.1225, -0.0449,  0.1187, -0.1451,  0.1951,\n",
      "         -0.0578, -0.0726,  0.0589, -0.2051],\n",
      "        [-0.0290, -0.2074,  0.1823,  0.0533,  0.1773, -0.1596,  0.0574,  0.0875,\n",
      "          0.0357,  0.0339,  0.0177,  0.1407, -0.0735,  0.0172, -0.1076, -0.0974,\n",
      "          0.1186,  0.1836, -0.1089, -0.0767],\n",
      "        [ 0.0489,  0.0325,  0.0994,  0.1450,  0.0156, -0.1909, -0.0704, -0.1181,\n",
      "         -0.0061,  0.0079,  0.2122, -0.1307, -0.0210, -0.1692,  0.1509, -0.0162,\n",
      "          0.1615, -0.0150,  0.2033,  0.0655],\n",
      "        [ 0.0370, -0.0759, -0.0264,  0.1047, -0.0168,  0.0124, -0.0259,  0.0721,\n",
      "          0.0276,  0.0683,  0.0407, -0.0169, -0.0162, -0.0379, -0.1902,  0.1729,\n",
      "          0.0669,  0.1799,  0.1803,  0.1274],\n",
      "        [ 0.1842, -0.0732, -0.1559, -0.2108, -0.0395,  0.1011, -0.0005, -0.0624,\n",
      "          0.0851, -0.1175, -0.1733,  0.1370, -0.0309,  0.0005, -0.2081,  0.1716,\n",
      "          0.0342, -0.2061, -0.0455,  0.0079]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1573,  0.1472, -0.0349, -0.0351, -0.2052, -0.1421,  0.1493, -0.1556,\n",
      "        -0.0562,  0.1879, -0.1397, -0.1535,  0.2011, -0.0770,  0.1629, -0.0762,\n",
      "        -0.0290,  0.0676, -0.1986, -0.1488], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2133, -0.1333, -0.1835, -0.0203,  0.0902, -0.1158,  0.1631,  0.1882,\n",
      "          0.0744, -0.1843,  0.0694,  0.0733, -0.0764,  0.1623, -0.0573,  0.0425,\n",
      "         -0.0819,  0.0728,  0.2187, -0.1508],\n",
      "        [ 0.1119,  0.1228,  0.1837, -0.2026,  0.1015, -0.1706, -0.2192, -0.1824,\n",
      "         -0.0283,  0.1440, -0.1170,  0.0653, -0.0648,  0.0314, -0.0830, -0.0179,\n",
      "          0.0511,  0.1178, -0.2108,  0.2175],\n",
      "        [ 0.0501,  0.0682,  0.0242, -0.0592,  0.0974,  0.1185, -0.0072, -0.2087,\n",
      "          0.2164, -0.2113, -0.1109, -0.1280,  0.1316, -0.1728, -0.0877,  0.0347,\n",
      "         -0.0622,  0.1479, -0.1524, -0.1877],\n",
      "        [-0.2102, -0.1332, -0.0578, -0.0325, -0.1399, -0.1274,  0.0589, -0.1271,\n",
      "          0.1991,  0.0070, -0.1352, -0.2226,  0.0752, -0.2107,  0.2083,  0.1852,\n",
      "         -0.0160, -0.0997, -0.0707,  0.2229],\n",
      "        [ 0.2175, -0.1139, -0.0297,  0.0643,  0.1653,  0.1138, -0.2061, -0.1348,\n",
      "         -0.1385,  0.0585, -0.0041, -0.1493,  0.0633,  0.0769, -0.1875, -0.1105,\n",
      "         -0.1360, -0.1654, -0.0035, -0.0962],\n",
      "        [-0.2148, -0.0908, -0.2162,  0.0049, -0.1836,  0.1897,  0.0945,  0.0920,\n",
      "          0.0516,  0.0435, -0.1117,  0.0313, -0.0169, -0.2159, -0.2072,  0.1396,\n",
      "          0.2205,  0.1867,  0.1892, -0.1652],\n",
      "        [ 0.1523, -0.1725, -0.0784,  0.1073,  0.1445,  0.1534,  0.2021, -0.1447,\n",
      "          0.0822, -0.0163, -0.1277, -0.1443, -0.0373, -0.0752,  0.2150,  0.0061,\n",
      "          0.0986, -0.0691,  0.2218, -0.0567],\n",
      "        [ 0.1716, -0.0969,  0.1377,  0.0245,  0.1936,  0.0818,  0.1629,  0.0227,\n",
      "          0.0051, -0.1567, -0.2169, -0.0767,  0.0082,  0.0045, -0.0800, -0.1488,\n",
      "         -0.0835, -0.0692, -0.1102, -0.1099],\n",
      "        [-0.1548,  0.1119, -0.1069, -0.0344,  0.1285, -0.1802,  0.1303, -0.0106,\n",
      "          0.1254,  0.0230, -0.2087, -0.1715, -0.1211,  0.0635, -0.0183,  0.0696,\n",
      "         -0.1489,  0.2202, -0.1697, -0.1094],\n",
      "        [-0.1174,  0.0940, -0.1274,  0.2104,  0.0703, -0.0306, -0.0542, -0.0319,\n",
      "         -0.1827, -0.0911,  0.2100,  0.0413,  0.2089,  0.0143, -0.1210, -0.0777,\n",
      "         -0.0052, -0.1738, -0.0636,  0.1039],\n",
      "        [ 0.0628,  0.2109,  0.0410, -0.2192,  0.1037,  0.2099,  0.1247, -0.0426,\n",
      "          0.1913, -0.1075,  0.0296, -0.0428,  0.1033, -0.0042,  0.0167, -0.0296,\n",
      "          0.1025,  0.1454, -0.2179, -0.1796],\n",
      "        [ 0.0644, -0.2148,  0.0319,  0.1975, -0.1194, -0.1373,  0.0089,  0.1117,\n",
      "         -0.1139, -0.0027, -0.1900, -0.0099,  0.1587,  0.2194,  0.1735,  0.0598,\n",
      "         -0.1863, -0.2224,  0.2041,  0.0381],\n",
      "        [ 0.1786, -0.1833, -0.0526, -0.0472,  0.1192,  0.0826,  0.1712, -0.0595,\n",
      "         -0.0880, -0.2165, -0.1381, -0.1959,  0.1893,  0.0127,  0.0202, -0.0372,\n",
      "          0.1397, -0.0741,  0.1081,  0.2071],\n",
      "        [ 0.1021, -0.1566, -0.1523,  0.1081,  0.1891,  0.2209,  0.0509, -0.0681,\n",
      "          0.2026,  0.0121,  0.0078,  0.1905, -0.1917, -0.1981, -0.1410,  0.2058,\n",
      "         -0.1901, -0.2206,  0.2228,  0.2028],\n",
      "        [-0.1284,  0.0549,  0.1075,  0.1129,  0.0072, -0.1866, -0.1455,  0.0267,\n",
      "          0.1358,  0.0448, -0.1889, -0.1794, -0.0788, -0.0921,  0.1714, -0.0275,\n",
      "         -0.0768, -0.1041, -0.1721, -0.0280],\n",
      "        [-0.0398,  0.1516,  0.1143, -0.0855, -0.0902, -0.2197,  0.0724, -0.1740,\n",
      "          0.0777,  0.0802,  0.1676,  0.0007,  0.0063,  0.2147, -0.2222,  0.1637,\n",
      "         -0.0899, -0.1974,  0.1575, -0.1037],\n",
      "        [-0.0179, -0.0345, -0.0848, -0.0903,  0.0437, -0.0541, -0.0308,  0.0418,\n",
      "          0.1804, -0.1767, -0.1372,  0.0284,  0.0143, -0.0469, -0.0638, -0.2175,\n",
      "         -0.0942,  0.1759,  0.0584,  0.1992],\n",
      "        [-0.1903, -0.0644, -0.0210, -0.0071,  0.0698, -0.1105,  0.0287,  0.1870,\n",
      "         -0.1907,  0.1191, -0.0240,  0.1477, -0.0734, -0.1793,  0.1808, -0.0088,\n",
      "          0.0774,  0.1243,  0.0476, -0.0946],\n",
      "        [-0.0345,  0.1770,  0.1212,  0.1482,  0.1943, -0.0688, -0.0358, -0.1969,\n",
      "          0.2186,  0.1817, -0.2036, -0.0474,  0.1164, -0.1155,  0.0525,  0.1045,\n",
      "         -0.1858, -0.1694, -0.0264, -0.0109],\n",
      "        [ 0.1137,  0.0494, -0.0024, -0.1311, -0.0018,  0.0023, -0.1580,  0.1642,\n",
      "          0.2171, -0.1372, -0.0603,  0.0218, -0.2166,  0.2199, -0.0245,  0.2208,\n",
      "          0.1526, -0.1256, -0.0970,  0.1195]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1744, -0.0362,  0.1013,  0.0239, -0.0396,  0.0997, -0.0805, -0.1356,\n",
      "         0.0202,  0.1934, -0.1981, -0.0858, -0.2164, -0.2114,  0.1404,  0.1472,\n",
      "         0.0789, -0.0175, -0.0547, -0.1536], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1313,  0.0636,  0.1361,  0.2148,  0.1838, -0.1811,  0.0794, -0.0229,\n",
      "         -0.0278,  0.0899, -0.0706, -0.1223,  0.1987, -0.1199, -0.0697,  0.1622,\n",
      "          0.1555, -0.1272, -0.0141,  0.0799],\n",
      "        [-0.0563, -0.1085, -0.1381, -0.1367,  0.1241, -0.0690,  0.0186, -0.1647,\n",
      "          0.1494, -0.0712,  0.2054,  0.0668,  0.1172, -0.2062, -0.1547, -0.0328,\n",
      "          0.0092, -0.0694, -0.0663, -0.1205],\n",
      "        [ 0.1237, -0.2175, -0.1662, -0.0811,  0.1657,  0.0676, -0.0776, -0.0479,\n",
      "          0.1818, -0.1670, -0.2005, -0.0513,  0.0631,  0.1628,  0.0751, -0.0709,\n",
      "          0.1526,  0.1696, -0.0442, -0.2025],\n",
      "        [-0.0466,  0.0220,  0.0443,  0.1569, -0.1807, -0.2211,  0.0541,  0.1510,\n",
      "          0.2151, -0.0391, -0.1266, -0.0965,  0.0503,  0.0777, -0.1997,  0.0134,\n",
      "          0.1757,  0.1456, -0.0765,  0.0267],\n",
      "        [ 0.0582, -0.2142,  0.1502, -0.1552,  0.0356, -0.0724,  0.1853, -0.1915,\n",
      "         -0.1353,  0.0569, -0.1676, -0.2079, -0.0116, -0.2123, -0.1901, -0.1326,\n",
      "         -0.0166,  0.0026, -0.1005, -0.0801],\n",
      "        [-0.2008, -0.0763,  0.2032,  0.0880, -0.0839, -0.0589, -0.0765, -0.1142,\n",
      "         -0.0276, -0.1563,  0.0526,  0.2039, -0.0321, -0.0124, -0.0552,  0.0975,\n",
      "          0.0421,  0.0057, -0.1652, -0.0088],\n",
      "        [-0.1559, -0.0293, -0.1896, -0.0320, -0.1121, -0.1903,  0.1382,  0.0616,\n",
      "          0.0317,  0.1390, -0.2125,  0.1902,  0.1025,  0.0284, -0.0182, -0.0787,\n",
      "          0.0464, -0.0753,  0.0892, -0.1105],\n",
      "        [-0.0208, -0.0322, -0.0416, -0.2180, -0.2067,  0.1381, -0.1207, -0.1157,\n",
      "         -0.1422,  0.1494,  0.0817, -0.0835, -0.1494, -0.0757, -0.1571, -0.1437,\n",
      "          0.1248, -0.1064,  0.1626, -0.1622],\n",
      "        [ 0.1857,  0.0020, -0.0623, -0.0176, -0.1304,  0.0086,  0.1014, -0.0306,\n",
      "         -0.0834,  0.1728, -0.1061,  0.2011, -0.1338,  0.1234,  0.1210, -0.0651,\n",
      "          0.0351,  0.0387, -0.1264, -0.1818],\n",
      "        [-0.0306, -0.1326, -0.0236,  0.0753,  0.0957, -0.0359,  0.0434, -0.1300,\n",
      "          0.1940,  0.0444, -0.1036,  0.1293,  0.0859,  0.1618, -0.0408,  0.0578,\n",
      "         -0.2104, -0.1619,  0.0708,  0.0445],\n",
      "        [-0.1176, -0.0374,  0.0611,  0.1840,  0.0842, -0.0471, -0.1960, -0.1891,\n",
      "          0.0192, -0.1228, -0.0511, -0.0766, -0.0846, -0.1254,  0.2091, -0.0768,\n",
      "          0.2094,  0.0839,  0.0221,  0.1520],\n",
      "        [-0.0568, -0.0500, -0.0231,  0.0858, -0.0882, -0.1930, -0.0625, -0.0938,\n",
      "         -0.1582, -0.0669, -0.0764,  0.1957,  0.1843,  0.0887, -0.1080, -0.1389,\n",
      "          0.1258, -0.1426,  0.0588, -0.0587],\n",
      "        [-0.0965,  0.1999, -0.1541, -0.1803, -0.0319,  0.1503, -0.1087, -0.0473,\n",
      "          0.0962, -0.1151, -0.1294, -0.0720, -0.1291, -0.0685,  0.1785,  0.1468,\n",
      "         -0.0576,  0.0069, -0.1208,  0.1906],\n",
      "        [-0.1822,  0.0978,  0.1475,  0.0611, -0.1139,  0.0986, -0.0968, -0.0041,\n",
      "         -0.1713,  0.0984,  0.0885,  0.1100, -0.1909, -0.1229,  0.1915, -0.1930,\n",
      "         -0.0635,  0.1488,  0.2136, -0.0398],\n",
      "        [ 0.1954,  0.0252,  0.1578,  0.1901, -0.2219, -0.0350,  0.0029, -0.2077,\n",
      "         -0.0092,  0.1231, -0.0541,  0.0369,  0.0808,  0.0179,  0.1962,  0.1591,\n",
      "          0.0299, -0.0600, -0.1843,  0.0299],\n",
      "        [ 0.0155, -0.1473, -0.0288, -0.1861,  0.1401, -0.1973, -0.2094, -0.0659,\n",
      "         -0.2086,  0.1306, -0.2200, -0.0629, -0.1474,  0.0451,  0.0092,  0.1446,\n",
      "          0.0125,  0.1724, -0.0230, -0.1669],\n",
      "        [ 0.0695,  0.1695, -0.1224, -0.1614, -0.0750,  0.0033,  0.0455,  0.1787,\n",
      "          0.0508,  0.0788,  0.0342,  0.0060,  0.1493,  0.1084, -0.0775,  0.1128,\n",
      "         -0.0695, -0.0272,  0.0440,  0.0494],\n",
      "        [-0.0347, -0.0568, -0.1053, -0.1345,  0.0677,  0.0351, -0.0347, -0.0353,\n",
      "         -0.0907, -0.1468,  0.0419,  0.0117, -0.0075, -0.1880, -0.0450, -0.0478,\n",
      "          0.1988, -0.2213, -0.0051, -0.1960],\n",
      "        [ 0.1100,  0.0290, -0.2221, -0.0603,  0.0843, -0.2103,  0.1675, -0.0292,\n",
      "          0.1809,  0.0485, -0.1189, -0.1432, -0.0525, -0.1412, -0.0067, -0.0924,\n",
      "         -0.0851,  0.1432, -0.2157,  0.1411],\n",
      "        [ 0.0171, -0.1862, -0.0759,  0.0063,  0.1914,  0.1886,  0.0236,  0.1652,\n",
      "          0.1459,  0.1310, -0.0618, -0.0490,  0.1103,  0.2126, -0.0662, -0.0382,\n",
      "         -0.0242,  0.0697,  0.0767,  0.2149]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0916,  0.0472, -0.1588, -0.2146, -0.0668,  0.0292, -0.1747,  0.1031,\n",
      "         0.2174,  0.1256,  0.1782,  0.1528,  0.0252, -0.1035,  0.1531,  0.2027,\n",
      "         0.0513, -0.1790,  0.1614,  0.1509], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0535,  0.0130,  0.0397, -0.1249,  0.1712,  0.0636,  0.1012,  0.1188,\n",
      "          0.2103,  0.0114,  0.1823, -0.1049, -0.1658, -0.1983, -0.1115, -0.1672,\n",
      "         -0.0780, -0.0427, -0.1003, -0.0877],\n",
      "        [-0.0673, -0.0457, -0.1097, -0.2016, -0.1395, -0.1937, -0.1600, -0.1668,\n",
      "         -0.2226, -0.2108,  0.1225,  0.0022, -0.0455, -0.1937,  0.0136,  0.0789,\n",
      "         -0.1432,  0.0898, -0.1374,  0.0422],\n",
      "        [ 0.1747,  0.1026,  0.1894, -0.0959, -0.0799,  0.0928, -0.0857, -0.1358,\n",
      "          0.1635,  0.0679,  0.0392,  0.0509,  0.0578,  0.1068, -0.0279,  0.0338,\n",
      "         -0.2138,  0.0360, -0.0372,  0.1669],\n",
      "        [ 0.1174, -0.0695,  0.0764,  0.2107,  0.1154,  0.1455,  0.1243,  0.1088,\n",
      "         -0.0254, -0.1327,  0.2002,  0.1191,  0.0378,  0.0375,  0.1998, -0.1629,\n",
      "          0.0872,  0.0883, -0.1655,  0.1160],\n",
      "        [-0.1380,  0.0876, -0.0674,  0.1772, -0.1872,  0.1712, -0.0147,  0.1404,\n",
      "          0.2065, -0.0391,  0.1611, -0.0549, -0.1421,  0.0471,  0.1801,  0.1747,\n",
      "         -0.0903,  0.0245,  0.1265,  0.1878],\n",
      "        [-0.0218,  0.1666, -0.0913,  0.1512,  0.0961, -0.1733,  0.0234,  0.0966,\n",
      "         -0.0211, -0.2052,  0.0720, -0.1387,  0.1932,  0.1401,  0.1769,  0.0812,\n",
      "          0.2067,  0.1781,  0.1962, -0.1307],\n",
      "        [-0.1973,  0.1202,  0.0687, -0.1147, -0.1492,  0.1916, -0.1538, -0.1779,\n",
      "          0.1865, -0.0007, -0.0832,  0.0720,  0.1858,  0.1990, -0.1931, -0.0818,\n",
      "          0.0446, -0.1452,  0.1283,  0.0007],\n",
      "        [-0.0142, -0.0466, -0.1470,  0.1376, -0.1178, -0.0769, -0.0777, -0.0016,\n",
      "         -0.1010, -0.0147, -0.1717, -0.1314,  0.1093,  0.1261, -0.1248,  0.2142,\n",
      "          0.0703,  0.0135, -0.0605,  0.0012],\n",
      "        [ 0.0842, -0.1273, -0.0911,  0.1956,  0.2064,  0.1239, -0.1495, -0.1617,\n",
      "          0.1079, -0.1686, -0.1279, -0.1626, -0.0436, -0.0408,  0.2233,  0.1051,\n",
      "          0.0122,  0.0370, -0.0776,  0.1790],\n",
      "        [-0.2188, -0.1432, -0.0397, -0.1361, -0.1476,  0.0207, -0.1688, -0.1010,\n",
      "          0.0102,  0.0180, -0.1651,  0.1884,  0.1655, -0.1306,  0.1621,  0.1915,\n",
      "         -0.2233,  0.0292,  0.1430,  0.1295],\n",
      "        [-0.1058,  0.0021, -0.1903, -0.1209,  0.0467,  0.0292,  0.2076,  0.0381,\n",
      "          0.0742, -0.0754, -0.0875, -0.0885,  0.1944,  0.0094,  0.0159,  0.2069,\n",
      "         -0.1077,  0.0051, -0.0770, -0.1234],\n",
      "        [ 0.0336, -0.1159,  0.1890, -0.1324,  0.2185,  0.1030, -0.1874, -0.1982,\n",
      "         -0.0800,  0.1897, -0.1988, -0.2115,  0.1932, -0.1687, -0.1172, -0.1120,\n",
      "          0.1255,  0.0720, -0.1153,  0.1071],\n",
      "        [ 0.1218,  0.1160, -0.0444, -0.0229,  0.0609,  0.1207,  0.0172, -0.1689,\n",
      "         -0.2081,  0.0502, -0.1539, -0.0378,  0.0079, -0.0177,  0.0358, -0.0205,\n",
      "         -0.0113, -0.0021, -0.1485, -0.1792],\n",
      "        [ 0.1351,  0.0195, -0.0115, -0.0510, -0.1619, -0.0525, -0.0804, -0.0038,\n",
      "         -0.1905, -0.0152,  0.0397,  0.0058,  0.0693,  0.1344,  0.0605,  0.0094,\n",
      "          0.0150,  0.1476, -0.0232,  0.0397],\n",
      "        [ 0.1263,  0.0441,  0.1780, -0.0538,  0.0555,  0.1967, -0.0151,  0.2025,\n",
      "          0.2021,  0.1250, -0.1549, -0.2115,  0.0034, -0.1657,  0.0327,  0.1588,\n",
      "         -0.1950,  0.0092,  0.0994,  0.0341],\n",
      "        [ 0.1857, -0.1769,  0.0357,  0.1167, -0.1461, -0.2119, -0.0623,  0.1474,\n",
      "         -0.2112, -0.0255,  0.0476,  0.1667, -0.1685,  0.0260,  0.2129, -0.1313,\n",
      "          0.2179, -0.0243, -0.1644, -0.1294],\n",
      "        [-0.1950,  0.0088, -0.0403,  0.1227,  0.0222, -0.1612,  0.1152,  0.0888,\n",
      "          0.2045, -0.0629, -0.1753, -0.0448, -0.1619, -0.1964,  0.0678,  0.2015,\n",
      "          0.2146,  0.0006,  0.0378,  0.1640],\n",
      "        [-0.0471,  0.1905,  0.1037,  0.0829, -0.1390,  0.1248, -0.1677,  0.0874,\n",
      "         -0.1804,  0.1611,  0.0489,  0.1955,  0.1937,  0.1976,  0.1925, -0.0625,\n",
      "          0.0053,  0.1945,  0.1894, -0.1482],\n",
      "        [ 0.2226,  0.0579, -0.1769, -0.0132,  0.1320, -0.1005,  0.1442, -0.2061,\n",
      "         -0.1806,  0.0421, -0.0189, -0.2005,  0.0212, -0.0566, -0.0404,  0.1482,\n",
      "          0.2143,  0.0307,  0.0984, -0.1158],\n",
      "        [ 0.1940, -0.0228, -0.0671, -0.2108,  0.0211, -0.0844,  0.0015,  0.1378,\n",
      "         -0.0440, -0.1061, -0.1772,  0.0115,  0.1574,  0.1148, -0.2092, -0.2076,\n",
      "         -0.2119,  0.1963,  0.1837,  0.2118]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1963, -0.2188,  0.2018,  0.1891, -0.2009,  0.0083,  0.0043,  0.1691,\n",
      "         0.1808,  0.0287, -0.0956,  0.1268,  0.0603,  0.1747,  0.0741,  0.1997,\n",
      "        -0.0709, -0.1957, -0.1515,  0.0336], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1217, -0.1696,  0.0853, -0.1404,  0.0089, -0.2134,  0.0427, -0.1475,\n",
      "          0.1423, -0.0222,  0.1384, -0.0023, -0.1981, -0.0622,  0.1932,  0.0405,\n",
      "          0.1170,  0.0835, -0.1957, -0.1831],\n",
      "        [-0.1333, -0.1979,  0.1222,  0.1398,  0.0596,  0.1687, -0.0042,  0.0121,\n",
      "          0.1433,  0.1463,  0.2073, -0.1925, -0.1639,  0.1479, -0.0777,  0.1994,\n",
      "         -0.0436, -0.2101, -0.0297, -0.0505],\n",
      "        [-0.1022, -0.0905,  0.1929, -0.1316,  0.1524,  0.1300, -0.1663,  0.1194,\n",
      "          0.1159,  0.1317, -0.0006,  0.0031, -0.1464, -0.0438,  0.1072,  0.0483,\n",
      "         -0.1520, -0.1199, -0.0016,  0.0099],\n",
      "        [ 0.1856,  0.1716, -0.0440, -0.2178, -0.2061,  0.1061,  0.2088,  0.0326,\n",
      "         -0.1080,  0.2200,  0.1161, -0.1258, -0.1400, -0.0691, -0.2142,  0.2130,\n",
      "          0.1396, -0.1381,  0.1873,  0.0270],\n",
      "        [-0.0242, -0.1099,  0.1054,  0.0378,  0.1553, -0.0407,  0.0651,  0.1293,\n",
      "          0.1026, -0.0546, -0.0178, -0.1499, -0.0367, -0.1915, -0.0768,  0.0398,\n",
      "         -0.1759,  0.0976, -0.0496,  0.1469],\n",
      "        [-0.1515, -0.1779, -0.0990, -0.1481, -0.1885,  0.0497,  0.1492, -0.1712,\n",
      "          0.0488,  0.1052, -0.0913, -0.1363,  0.1803,  0.0020, -0.1509,  0.1556,\n",
      "          0.1777,  0.1621,  0.2008, -0.1807],\n",
      "        [-0.2204, -0.0075,  0.0767, -0.2152, -0.0620,  0.0103, -0.0130,  0.0964,\n",
      "          0.0236,  0.1768, -0.0869,  0.0304,  0.1361,  0.0446,  0.0402, -0.0736,\n",
      "          0.1708, -0.0573,  0.0924,  0.1735],\n",
      "        [ 0.0460, -0.1128,  0.1317, -0.0884,  0.0320, -0.0787,  0.1005,  0.1696,\n",
      "         -0.1713, -0.2035, -0.0428, -0.1966,  0.1533,  0.0302,  0.0750,  0.0923,\n",
      "          0.0936, -0.0490, -0.1042, -0.0357],\n",
      "        [-0.1312, -0.0128,  0.1764, -0.1321,  0.1124, -0.1244, -0.1220, -0.2234,\n",
      "         -0.0974, -0.2088, -0.0746,  0.1337,  0.1709,  0.0860, -0.1701, -0.0745,\n",
      "         -0.1942,  0.0675, -0.1386, -0.0916],\n",
      "        [ 0.0572, -0.2211, -0.0632,  0.0702,  0.0927,  0.0919,  0.0291, -0.1534,\n",
      "         -0.1331,  0.0134, -0.1011, -0.0784, -0.1686,  0.1843,  0.1932, -0.0168,\n",
      "          0.0431, -0.2162,  0.0884, -0.2200],\n",
      "        [ 0.0358, -0.1354,  0.0509,  0.0170, -0.0400, -0.1105, -0.0949, -0.1586,\n",
      "          0.2076, -0.1529,  0.0823,  0.1167,  0.0696,  0.0089,  0.0314, -0.0024,\n",
      "         -0.0678, -0.0568, -0.0258,  0.0424],\n",
      "        [-0.0738,  0.2093,  0.1394,  0.1220,  0.1610,  0.1319, -0.1991, -0.0762,\n",
      "         -0.0686, -0.0430, -0.0470, -0.1449,  0.0833, -0.0607, -0.1815, -0.1273,\n",
      "         -0.0410,  0.1893, -0.0827,  0.0379],\n",
      "        [-0.0514, -0.1075, -0.1778,  0.1777, -0.2100,  0.1181, -0.1921,  0.1951,\n",
      "          0.1982, -0.1097,  0.0802, -0.1004, -0.1708, -0.0709,  0.0743, -0.1198,\n",
      "         -0.0288,  0.0084,  0.0216, -0.0627],\n",
      "        [ 0.0416, -0.0876,  0.1072,  0.1361, -0.0160,  0.0013,  0.1370,  0.0617,\n",
      "          0.0084,  0.0725, -0.1878, -0.0755, -0.0142,  0.0154, -0.2052,  0.0034,\n",
      "         -0.2184, -0.0514,  0.0565,  0.1690],\n",
      "        [ 0.2035,  0.1137, -0.1667,  0.1477,  0.1314, -0.1884,  0.1275,  0.1175,\n",
      "         -0.0291,  0.0461,  0.1928,  0.0066,  0.0286,  0.1946,  0.1926, -0.1891,\n",
      "         -0.1579,  0.0245, -0.1898,  0.1870],\n",
      "        [-0.0225,  0.1894,  0.1818, -0.1883, -0.1294, -0.1320, -0.0191,  0.2052,\n",
      "         -0.0797, -0.1582,  0.1392, -0.2090, -0.0143,  0.1502,  0.1389, -0.1174,\n",
      "          0.1235, -0.0848, -0.1711, -0.0192],\n",
      "        [-0.2113,  0.0605, -0.0412,  0.1684, -0.0784, -0.0353, -0.1707, -0.0655,\n",
      "         -0.1318,  0.1296,  0.1311,  0.0377, -0.1347,  0.1030, -0.0688,  0.1008,\n",
      "         -0.0206, -0.0682, -0.1129,  0.0786],\n",
      "        [ 0.1515, -0.1294, -0.0282, -0.2158,  0.2211,  0.0791, -0.1733,  0.0799,\n",
      "         -0.1181, -0.1144, -0.1394, -0.1486, -0.1249,  0.1824,  0.0370, -0.0277,\n",
      "         -0.2106, -0.2148, -0.0697, -0.1644],\n",
      "        [ 0.0321,  0.2198, -0.1586, -0.2231,  0.1923, -0.0695,  0.1910,  0.0373,\n",
      "          0.1092,  0.1512, -0.0636,  0.0705, -0.0753, -0.1522,  0.1331, -0.0917,\n",
      "          0.2134, -0.1838,  0.0812, -0.0524],\n",
      "        [ 0.1227,  0.1593,  0.1163,  0.1233,  0.0134, -0.0042,  0.0241, -0.1529,\n",
      "         -0.1464,  0.1739,  0.0171,  0.0956,  0.0157,  0.1132, -0.0555, -0.1832,\n",
      "          0.0468, -0.1153, -0.1481, -0.2015]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1194, -0.0305,  0.0089, -0.0407,  0.1539, -0.1010, -0.0981, -0.2061,\n",
      "        -0.0401, -0.1553,  0.0754,  0.1804, -0.0750, -0.1357, -0.1448, -0.1117,\n",
      "         0.1362, -0.0372,  0.2081,  0.1057], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0746,  0.1652,  0.0803,  0.0085,  0.1461, -0.0263,  0.2091, -0.1957,\n",
      "         -0.0233, -0.0009,  0.1524,  0.2191, -0.1231,  0.0408,  0.1165, -0.0729,\n",
      "         -0.1989, -0.2216,  0.1263, -0.1560],\n",
      "        [-0.0725, -0.2211, -0.0630, -0.0401,  0.2019, -0.1829,  0.1986, -0.0038,\n",
      "          0.1350,  0.1821, -0.1716,  0.0075, -0.0293, -0.0924,  0.0892, -0.1016,\n",
      "         -0.0587, -0.0190,  0.1166,  0.0752],\n",
      "        [-0.0475, -0.0473,  0.0077, -0.0628, -0.0949,  0.0361,  0.0367, -0.1699,\n",
      "         -0.0393,  0.0031, -0.2116, -0.1823, -0.0814,  0.0613,  0.0061, -0.1787,\n",
      "          0.0471,  0.1276,  0.1359, -0.0465],\n",
      "        [ 0.1651, -0.0541,  0.1310, -0.1229, -0.0331, -0.1101, -0.1489,  0.1711,\n",
      "         -0.0133, -0.0180,  0.0839,  0.0990,  0.1012, -0.1021, -0.2204,  0.0423,\n",
      "          0.0820,  0.2093, -0.0515,  0.1020],\n",
      "        [ 0.0680, -0.1222,  0.0606, -0.0848,  0.0133, -0.0056,  0.0417, -0.0859,\n",
      "          0.1000, -0.1172,  0.0154,  0.1960, -0.0071, -0.0964, -0.1661, -0.0835,\n",
      "         -0.1486,  0.0244, -0.0332, -0.2161],\n",
      "        [-0.1438, -0.0682,  0.0174,  0.1912, -0.1250,  0.1756,  0.1024, -0.1378,\n",
      "          0.1415,  0.1714, -0.0255,  0.0764,  0.1712,  0.0046,  0.0444, -0.0332,\n",
      "          0.1574,  0.0111, -0.2033,  0.0650],\n",
      "        [ 0.0437,  0.0872,  0.0185, -0.0805, -0.0632,  0.1244,  0.0590, -0.1996,\n",
      "         -0.1687, -0.0192,  0.1225, -0.0380,  0.2042, -0.0336, -0.0058, -0.1688,\n",
      "          0.0605,  0.2013,  0.1579, -0.1612],\n",
      "        [-0.1901,  0.0897, -0.1274, -0.1074,  0.2197,  0.0916,  0.2163,  0.1943,\n",
      "         -0.1548, -0.0229,  0.0819, -0.0389,  0.1968,  0.0832, -0.1740, -0.0081,\n",
      "          0.2205, -0.1449, -0.0724,  0.1205],\n",
      "        [ 0.2165,  0.1387,  0.0584, -0.1059, -0.0202, -0.1681,  0.0988,  0.0357,\n",
      "          0.0858, -0.0212,  0.1199, -0.0940, -0.0423, -0.1831,  0.1636, -0.2031,\n",
      "          0.0422, -0.0194, -0.0076, -0.0500],\n",
      "        [-0.1779,  0.1406,  0.1631,  0.1863,  0.1158,  0.1305, -0.1140,  0.0351,\n",
      "         -0.0857,  0.2187,  0.1977,  0.0759, -0.0621,  0.0294,  0.1916,  0.1204,\n",
      "         -0.2184, -0.0583,  0.1450, -0.1648],\n",
      "        [ 0.1510, -0.2197, -0.0960,  0.1222, -0.1858,  0.0740,  0.1304,  0.0251,\n",
      "          0.0858,  0.0726, -0.1137,  0.1479,  0.0156,  0.2192, -0.1946, -0.1727,\n",
      "          0.1522,  0.2198,  0.0805, -0.1391],\n",
      "        [-0.1845,  0.1789,  0.2220, -0.0072, -0.0447, -0.1898,  0.1228, -0.0151,\n",
      "         -0.0634,  0.0739, -0.0006, -0.1588,  0.1559,  0.0831,  0.0879, -0.0959,\n",
      "          0.0211, -0.1541, -0.0274, -0.1274],\n",
      "        [ 0.0789,  0.0254, -0.1938,  0.0493,  0.1062,  0.1141, -0.1927,  0.0601,\n",
      "         -0.0328,  0.2165,  0.0167,  0.1874,  0.0901,  0.0836,  0.1389, -0.0811,\n",
      "          0.0006,  0.1660,  0.0425,  0.1156],\n",
      "        [-0.1611,  0.2064, -0.2126, -0.0338,  0.1358, -0.2008,  0.1686,  0.1581,\n",
      "         -0.2132,  0.0279,  0.1693,  0.2195,  0.0984,  0.2034,  0.1313, -0.1426,\n",
      "         -0.0233, -0.0521,  0.1896, -0.1622],\n",
      "        [ 0.1173,  0.1583,  0.1669, -0.1641,  0.1021, -0.0933,  0.1165, -0.0040,\n",
      "         -0.1445, -0.0275,  0.1316, -0.1436,  0.0800, -0.0332, -0.0104,  0.1510,\n",
      "          0.0858, -0.1508, -0.1834,  0.1266],\n",
      "        [ 0.0481,  0.0336,  0.0601,  0.1220,  0.2161, -0.1897,  0.0962, -0.0292,\n",
      "         -0.1964,  0.1893,  0.0018,  0.1032, -0.0373,  0.0812,  0.0977,  0.1295,\n",
      "          0.0641,  0.0827, -0.0636, -0.1034],\n",
      "        [-0.0266, -0.0732,  0.0642,  0.0093,  0.1258,  0.1063,  0.1895,  0.0225,\n",
      "          0.1098, -0.2109,  0.1671,  0.2216, -0.0166,  0.0781, -0.0652, -0.1720,\n",
      "          0.2064, -0.0091, -0.1856, -0.0428],\n",
      "        [ 0.2046,  0.1500,  0.2073,  0.1050,  0.1734,  0.0181, -0.0423, -0.0932,\n",
      "         -0.0572, -0.0035,  0.0663,  0.2130,  0.0277,  0.2100, -0.0013,  0.1352,\n",
      "         -0.0748,  0.1514,  0.1164,  0.1359],\n",
      "        [ 0.2046, -0.1565, -0.1598, -0.1061, -0.0301, -0.1467, -0.0404, -0.1859,\n",
      "          0.1794,  0.1516, -0.1582,  0.1899,  0.0948, -0.1186, -0.2135, -0.1435,\n",
      "          0.1223, -0.0815,  0.1131, -0.0210],\n",
      "        [ 0.0254, -0.0034,  0.0703, -0.2081, -0.1947, -0.1382, -0.0663,  0.0969,\n",
      "         -0.2011,  0.0387, -0.0615,  0.1564,  0.0472,  0.0390,  0.0756,  0.1083,\n",
      "         -0.0609,  0.0676, -0.1063, -0.2171],\n",
      "        [ 0.0080, -0.1123,  0.1081,  0.1350, -0.2227, -0.2199,  0.0008, -0.2057,\n",
      "          0.1506,  0.1655, -0.0868, -0.0820,  0.0565,  0.0631, -0.1555, -0.0976,\n",
      "          0.0486,  0.0324, -0.1943,  0.0497],\n",
      "        [-0.0338,  0.2111,  0.2085, -0.1218,  0.1995, -0.0371, -0.0318, -0.0032,\n",
      "          0.0642,  0.0288,  0.0134,  0.1927, -0.0086, -0.0846, -0.1029,  0.1060,\n",
      "         -0.0138, -0.1165, -0.0461,  0.0221],\n",
      "        [ 0.0411,  0.1292, -0.1458,  0.1326, -0.0169,  0.1199,  0.0238,  0.1067,\n",
      "          0.2001, -0.0511,  0.1145,  0.1129,  0.0636, -0.0855,  0.0814,  0.0007,\n",
      "          0.0689, -0.0399, -0.1347,  0.1216],\n",
      "        [-0.1055, -0.0885,  0.0650,  0.0014,  0.0140, -0.1438,  0.0052, -0.0495,\n",
      "         -0.1762,  0.1698,  0.0817, -0.1369, -0.1792,  0.0651, -0.1093,  0.1369,\n",
      "          0.1353, -0.0361,  0.0194,  0.0025],\n",
      "        [ 0.1648,  0.0067,  0.1101,  0.0675,  0.1665, -0.0039, -0.1063, -0.1003,\n",
      "         -0.2224,  0.1220, -0.1442, -0.1462,  0.1362, -0.2056,  0.1859,  0.1166,\n",
      "         -0.0718, -0.2012,  0.0986,  0.0748],\n",
      "        [ 0.0836,  0.2101, -0.1055,  0.0184, -0.1575,  0.0978, -0.1417, -0.0202,\n",
      "          0.0744, -0.1619, -0.0487, -0.2092, -0.2088, -0.0147,  0.1585, -0.0214,\n",
      "          0.1130,  0.0892,  0.1187, -0.1276],\n",
      "        [ 0.0905, -0.1971,  0.0068, -0.0314,  0.0004, -0.0609,  0.0178,  0.1918,\n",
      "          0.0621,  0.0781,  0.2043,  0.1231,  0.1340,  0.0565, -0.1526, -0.1417,\n",
      "         -0.1271,  0.1584, -0.0976,  0.1221],\n",
      "        [ 0.0223, -0.0460, -0.0671,  0.0193, -0.0474, -0.0325,  0.0239,  0.0419,\n",
      "          0.0945, -0.1266, -0.0480,  0.0736, -0.0248, -0.1037,  0.1582,  0.0300,\n",
      "         -0.0348,  0.1176, -0.1553,  0.0718],\n",
      "        [ 0.1512,  0.1394, -0.1166, -0.0234, -0.0538,  0.1357, -0.1646, -0.1610,\n",
      "         -0.1597,  0.1124,  0.2127,  0.0666, -0.0071,  0.1685, -0.0225, -0.1973,\n",
      "         -0.0915,  0.2185,  0.1500, -0.1285],\n",
      "        [-0.0235, -0.1456,  0.1005,  0.2124, -0.0068, -0.1648, -0.1022,  0.1178,\n",
      "         -0.1350,  0.0649, -0.0379,  0.0078,  0.0248,  0.0710,  0.0526, -0.1893,\n",
      "          0.2236,  0.1538,  0.2003,  0.1362]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1209,  0.1628,  0.0071,  0.1758, -0.1054,  0.2051, -0.0207, -0.1346,\n",
      "        -0.0125,  0.0653,  0.0732,  0.0113,  0.1943,  0.1867,  0.1027,  0.1102,\n",
      "        -0.0958,  0.1689,  0.1129, -0.0076,  0.0963,  0.0757,  0.0114,  0.2218,\n",
      "        -0.0799, -0.0337,  0.1452, -0.2209, -0.2144, -0.1002],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0121,  0.0933,  0.1672, -0.0240,  0.1501, -0.2744,  0.1721, -0.1200],\n",
      "        [ 0.0484, -0.1256, -0.0912,  0.2646, -0.0934,  0.0815, -0.2589,  0.0096],\n",
      "        [-0.2152,  0.1611, -0.2176,  0.0762, -0.1182, -0.1354, -0.3080, -0.1668]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1432,  0.0201,  0.0288], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in obj1.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59iLYeT-NRsG"
   },
   "source": [
    "# RUN for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ekz8oACnb3tA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "====> Epoch: 1 total_train_loss: 0.765314 Total_test_loss: 0.731020 Total_BCE_test_loss: 0.618669 Total_KLD_test_loss: 0.004048 Total_CEP_test_loss: 0.108302\n",
      "====> Epoch: 2 total_train_loss: 0.749772 Total_test_loss: 0.724199 Total_BCE_test_loss: 0.611667 Total_KLD_test_loss: 0.004484 Total_CEP_test_loss: 0.108048\n",
      "====> Epoch: 3 total_train_loss: 0.751808 Total_test_loss: 0.733837 Total_BCE_test_loss: 0.621553 Total_KLD_test_loss: 0.004535 Total_CEP_test_loss: 0.107749\n",
      "====> Epoch: 4 total_train_loss: 0.744504 Total_test_loss: 0.724691 Total_BCE_test_loss: 0.612404 Total_KLD_test_loss: 0.004356 Total_CEP_test_loss: 0.107931\n",
      "====> Epoch: 5 total_train_loss: 0.743410 Total_test_loss: 0.726429 Total_BCE_test_loss: 0.614339 Total_KLD_test_loss: 0.003984 Total_CEP_test_loss: 0.108106\n",
      "====> Epoch: 6 total_train_loss: 0.744630 Total_test_loss: 0.723725 Total_BCE_test_loss: 0.611662 Total_KLD_test_loss: 0.004142 Total_CEP_test_loss: 0.107920\n",
      "====> Epoch: 7 total_train_loss: 0.746384 Total_test_loss: 0.722558 Total_BCE_test_loss: 0.610399 Total_KLD_test_loss: 0.004225 Total_CEP_test_loss: 0.107934\n",
      "====> Epoch: 8 total_train_loss: 0.746196 Total_test_loss: 0.722772 Total_BCE_test_loss: 0.610840 Total_KLD_test_loss: 0.004186 Total_CEP_test_loss: 0.107746\n",
      "====> Epoch: 9 total_train_loss: 0.743339 Total_test_loss: 0.718911 Total_BCE_test_loss: 0.607036 Total_KLD_test_loss: 0.004041 Total_CEP_test_loss: 0.107834\n",
      "====> Epoch: 10 total_train_loss: 0.742069 Total_test_loss: 0.719078 Total_BCE_test_loss: 0.607434 Total_KLD_test_loss: 0.003887 Total_CEP_test_loss: 0.107758\n",
      "====> Epoch: 11 total_train_loss: 0.742492 Total_test_loss: 0.718235 Total_BCE_test_loss: 0.606774 Total_KLD_test_loss: 0.003850 Total_CEP_test_loss: 0.107612\n",
      "====> Epoch: 12 total_train_loss: 0.741016 Total_test_loss: 0.717602 Total_BCE_test_loss: 0.606214 Total_KLD_test_loss: 0.003853 Total_CEP_test_loss: 0.107535\n",
      "====> Epoch: 13 total_train_loss: 0.740762 Total_test_loss: 0.717218 Total_BCE_test_loss: 0.605898 Total_KLD_test_loss: 0.003784 Total_CEP_test_loss: 0.107536\n",
      "====> Epoch: 14 total_train_loss: 0.741266 Total_test_loss: 0.718102 Total_BCE_test_loss: 0.606586 Total_KLD_test_loss: 0.003903 Total_CEP_test_loss: 0.107614\n",
      "====> Epoch: 15 total_train_loss: 0.739452 Total_test_loss: 0.715935 Total_BCE_test_loss: 0.604407 Total_KLD_test_loss: 0.003830 Total_CEP_test_loss: 0.107697\n",
      "====> Epoch: 16 total_train_loss: 0.736991 Total_test_loss: 0.717030 Total_BCE_test_loss: 0.605673 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107689\n",
      "====> Epoch: 17 total_train_loss: 0.741243 Total_test_loss: 0.720019 Total_BCE_test_loss: 0.609164 Total_KLD_test_loss: 0.003356 Total_CEP_test_loss: 0.107499\n",
      "====> Epoch: 18 total_train_loss: 0.739986 Total_test_loss: 0.720952 Total_BCE_test_loss: 0.609823 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107438\n",
      "====> Epoch: 19 total_train_loss: 0.739364 Total_test_loss: 0.717499 Total_BCE_test_loss: 0.606379 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107536\n",
      "====> Epoch: 20 total_train_loss: 0.745300 Total_test_loss: 0.720820 Total_BCE_test_loss: 0.609314 Total_KLD_test_loss: 0.003807 Total_CEP_test_loss: 0.107699\n",
      "====> Epoch: 21 total_train_loss: 0.741881 Total_test_loss: 0.720801 Total_BCE_test_loss: 0.609353 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107688\n",
      "====> Epoch: 22 total_train_loss: 0.745770 Total_test_loss: 0.720867 Total_BCE_test_loss: 0.609506 Total_KLD_test_loss: 0.003863 Total_CEP_test_loss: 0.107498\n",
      "====> Epoch: 23 total_train_loss: 0.746004 Total_test_loss: 0.718163 Total_BCE_test_loss: 0.606762 Total_KLD_test_loss: 0.004105 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 24 total_train_loss: 0.748149 Total_test_loss: 0.725137 Total_BCE_test_loss: 0.613746 Total_KLD_test_loss: 0.004040 Total_CEP_test_loss: 0.107351\n",
      "====> Epoch: 25 total_train_loss: 0.749736 Total_test_loss: 0.730413 Total_BCE_test_loss: 0.618928 Total_KLD_test_loss: 0.003993 Total_CEP_test_loss: 0.107493\n",
      "====> Epoch: 26 total_train_loss: 0.745809 Total_test_loss: 0.719383 Total_BCE_test_loss: 0.607743 Total_KLD_test_loss: 0.003982 Total_CEP_test_loss: 0.107658\n",
      "====> Epoch: 27 total_train_loss: 0.745764 Total_test_loss: 0.719137 Total_BCE_test_loss: 0.607320 Total_KLD_test_loss: 0.004009 Total_CEP_test_loss: 0.107808\n",
      "====> Epoch: 28 total_train_loss: 0.743817 Total_test_loss: 0.724739 Total_BCE_test_loss: 0.613279 Total_KLD_test_loss: 0.003619 Total_CEP_test_loss: 0.107842\n",
      "====> Epoch: 29 total_train_loss: 0.746650 Total_test_loss: 0.717117 Total_BCE_test_loss: 0.605321 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.108086\n",
      "====> Epoch: 30 total_train_loss: 0.742195 Total_test_loss: 0.717981 Total_BCE_test_loss: 0.606390 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107920\n",
      "====> Epoch: 31 total_train_loss: 0.741468 Total_test_loss: 0.715475 Total_BCE_test_loss: 0.604110 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107756\n",
      "====> Epoch: 32 total_train_loss: 0.739794 Total_test_loss: 0.718787 Total_BCE_test_loss: 0.607674 Total_KLD_test_loss: 0.003561 Total_CEP_test_loss: 0.107552\n",
      "====> Epoch: 33 total_train_loss: 0.740753 Total_test_loss: 0.713462 Total_BCE_test_loss: 0.602330 Total_KLD_test_loss: 0.003531 Total_CEP_test_loss: 0.107601\n",
      "====> Epoch: 34 total_train_loss: 0.737288 Total_test_loss: 0.717422 Total_BCE_test_loss: 0.606366 Total_KLD_test_loss: 0.003484 Total_CEP_test_loss: 0.107572\n",
      "====> Epoch: 35 total_train_loss: 0.740935 Total_test_loss: 0.718225 Total_BCE_test_loss: 0.607125 Total_KLD_test_loss: 0.003561 Total_CEP_test_loss: 0.107539\n",
      "====> Epoch: 36 total_train_loss: 0.740174 Total_test_loss: 0.715689 Total_BCE_test_loss: 0.604409 Total_KLD_test_loss: 0.003653 Total_CEP_test_loss: 0.107627\n",
      "====> Epoch: 37 total_train_loss: 0.738307 Total_test_loss: 0.715073 Total_BCE_test_loss: 0.603867 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107460\n",
      "====> Epoch: 38 total_train_loss: 0.739130 Total_test_loss: 0.711749 Total_BCE_test_loss: 0.600604 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107395\n",
      "====> Epoch: 39 total_train_loss: 0.738158 Total_test_loss: 0.711525 Total_BCE_test_loss: 0.600363 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107397\n",
      "====> Epoch: 40 total_train_loss: 0.740326 Total_test_loss: 0.713829 Total_BCE_test_loss: 0.602795 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107366\n",
      "====> Epoch: 41 total_train_loss: 0.736347 Total_test_loss: 0.714885 Total_BCE_test_loss: 0.603801 Total_KLD_test_loss: 0.003643 Total_CEP_test_loss: 0.107441\n",
      "====> Epoch: 42 total_train_loss: 0.735310 Total_test_loss: 0.715498 Total_BCE_test_loss: 0.604378 Total_KLD_test_loss: 0.003589 Total_CEP_test_loss: 0.107531\n",
      "====> Epoch: 43 total_train_loss: 0.736684 Total_test_loss: 0.714682 Total_BCE_test_loss: 0.603679 Total_KLD_test_loss: 0.003508 Total_CEP_test_loss: 0.107496\n",
      "====> Epoch: 44 total_train_loss: 0.736108 Total_test_loss: 0.714574 Total_BCE_test_loss: 0.603526 Total_KLD_test_loss: 0.003450 Total_CEP_test_loss: 0.107598\n",
      "====> Epoch: 45 total_train_loss: 0.735619 Total_test_loss: 0.712415 Total_BCE_test_loss: 0.601392 Total_KLD_test_loss: 0.003400 Total_CEP_test_loss: 0.107622\n",
      "====> Epoch: 46 total_train_loss: 0.734396 Total_test_loss: 0.710868 Total_BCE_test_loss: 0.599785 Total_KLD_test_loss: 0.003384 Total_CEP_test_loss: 0.107699\n",
      "====> Epoch: 47 total_train_loss: 0.734383 Total_test_loss: 0.710956 Total_BCE_test_loss: 0.599879 Total_KLD_test_loss: 0.003433 Total_CEP_test_loss: 0.107644\n",
      "====> Epoch: 48 total_train_loss: 0.732465 Total_test_loss: 0.711086 Total_BCE_test_loss: 0.599953 Total_KLD_test_loss: 0.003499 Total_CEP_test_loss: 0.107634\n",
      "====> Epoch: 49 total_train_loss: 0.734482 Total_test_loss: 0.710509 Total_BCE_test_loss: 0.599369 Total_KLD_test_loss: 0.003553 Total_CEP_test_loss: 0.107586\n",
      "====> Epoch: 50 total_train_loss: 0.733091 Total_test_loss: 0.713475 Total_BCE_test_loss: 0.602378 Total_KLD_test_loss: 0.003548 Total_CEP_test_loss: 0.107549\n",
      "====> Epoch: 51 total_train_loss: 0.733435 Total_test_loss: 0.710206 Total_BCE_test_loss: 0.599234 Total_KLD_test_loss: 0.003504 Total_CEP_test_loss: 0.107467\n",
      "====> Epoch: 52 total_train_loss: 0.732410 Total_test_loss: 0.708453 Total_BCE_test_loss: 0.597429 Total_KLD_test_loss: 0.003554 Total_CEP_test_loss: 0.107470\n",
      "====> Epoch: 53 total_train_loss: 0.730619 Total_test_loss: 0.707817 Total_BCE_test_loss: 0.596713 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 54 total_train_loss: 0.732446 Total_test_loss: 0.709693 Total_BCE_test_loss: 0.598733 Total_KLD_test_loss: 0.003652 Total_CEP_test_loss: 0.107308\n",
      "====> Epoch: 55 total_train_loss: 0.734066 Total_test_loss: 0.713094 Total_BCE_test_loss: 0.602385 Total_KLD_test_loss: 0.003649 Total_CEP_test_loss: 0.107061\n",
      "====> Epoch: 56 total_train_loss: 0.734749 Total_test_loss: 0.713284 Total_BCE_test_loss: 0.602632 Total_KLD_test_loss: 0.003571 Total_CEP_test_loss: 0.107081\n",
      "====> Epoch: 57 total_train_loss: 0.733707 Total_test_loss: 0.713187 Total_BCE_test_loss: 0.602543 Total_KLD_test_loss: 0.003579 Total_CEP_test_loss: 0.107065\n",
      "====> Epoch: 58 total_train_loss: 0.735053 Total_test_loss: 0.715216 Total_BCE_test_loss: 0.604428 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107103\n",
      "====> Epoch: 59 total_train_loss: 0.734384 Total_test_loss: 0.711374 Total_BCE_test_loss: 0.600350 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107325\n",
      "====> Epoch: 60 total_train_loss: 0.735219 Total_test_loss: 0.709698 Total_BCE_test_loss: 0.598515 Total_KLD_test_loss: 0.003776 Total_CEP_test_loss: 0.107406\n",
      "====> Epoch: 61 total_train_loss: 0.733714 Total_test_loss: 0.709846 Total_BCE_test_loss: 0.598512 Total_KLD_test_loss: 0.003832 Total_CEP_test_loss: 0.107502\n",
      "====> Epoch: 62 total_train_loss: 0.730041 Total_test_loss: 0.713227 Total_BCE_test_loss: 0.602074 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107461\n",
      "====> Epoch: 63 total_train_loss: 0.733865 Total_test_loss: 0.714243 Total_BCE_test_loss: 0.603183 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107338\n",
      "====> Epoch: 64 total_train_loss: 0.737838 Total_test_loss: 0.712505 Total_BCE_test_loss: 0.601125 Total_KLD_test_loss: 0.004105 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 65 total_train_loss: 0.733628 Total_test_loss: 0.709241 Total_BCE_test_loss: 0.598049 Total_KLD_test_loss: 0.004004 Total_CEP_test_loss: 0.107188\n",
      "====> Epoch: 66 total_train_loss: 0.732200 Total_test_loss: 0.710022 Total_BCE_test_loss: 0.598944 Total_KLD_test_loss: 0.003860 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 67 total_train_loss: 0.731839 Total_test_loss: 0.708139 Total_BCE_test_loss: 0.597147 Total_KLD_test_loss: 0.003789 Total_CEP_test_loss: 0.107202\n",
      "====> Epoch: 68 total_train_loss: 0.731661 Total_test_loss: 0.708942 Total_BCE_test_loss: 0.597807 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107371\n",
      "====> Epoch: 69 total_train_loss: 0.730499 Total_test_loss: 0.708433 Total_BCE_test_loss: 0.597273 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107430\n",
      "====> Epoch: 70 total_train_loss: 0.730381 Total_test_loss: 0.708803 Total_BCE_test_loss: 0.597626 Total_KLD_test_loss: 0.003772 Total_CEP_test_loss: 0.107405\n",
      "====> Epoch: 71 total_train_loss: 0.729646 Total_test_loss: 0.706198 Total_BCE_test_loss: 0.595280 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 72 total_train_loss: 0.729466 Total_test_loss: 0.704866 Total_BCE_test_loss: 0.593952 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 73 total_train_loss: 0.730893 Total_test_loss: 0.707471 Total_BCE_test_loss: 0.596665 Total_KLD_test_loss: 0.003582 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 74 total_train_loss: 0.730245 Total_test_loss: 0.706612 Total_BCE_test_loss: 0.595799 Total_KLD_test_loss: 0.003585 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 75 total_train_loss: 0.731202 Total_test_loss: 0.706796 Total_BCE_test_loss: 0.595824 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107293\n",
      "====> Epoch: 76 total_train_loss: 0.731440 Total_test_loss: 0.703917 Total_BCE_test_loss: 0.592844 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107337\n",
      "====> Epoch: 77 total_train_loss: 0.729132 Total_test_loss: 0.703253 Total_BCE_test_loss: 0.592125 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107422\n",
      "====> Epoch: 78 total_train_loss: 0.727871 Total_test_loss: 0.704117 Total_BCE_test_loss: 0.592973 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107478\n",
      "====> Epoch: 79 total_train_loss: 0.728825 Total_test_loss: 0.704480 Total_BCE_test_loss: 0.593398 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107378\n",
      "====> Epoch: 80 total_train_loss: 0.726202 Total_test_loss: 0.703696 Total_BCE_test_loss: 0.592991 Total_KLD_test_loss: 0.003653 Total_CEP_test_loss: 0.107051\n",
      "====> Epoch: 81 total_train_loss: 0.727188 Total_test_loss: 0.702566 Total_BCE_test_loss: 0.591774 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107168\n",
      "====> Epoch: 82 total_train_loss: 0.728768 Total_test_loss: 0.707082 Total_BCE_test_loss: 0.596333 Total_KLD_test_loss: 0.003585 Total_CEP_test_loss: 0.107163\n",
      "====> Epoch: 83 total_train_loss: 0.726964 Total_test_loss: 0.704609 Total_BCE_test_loss: 0.593851 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107165\n",
      "====> Epoch: 84 total_train_loss: 0.726885 Total_test_loss: 0.702054 Total_BCE_test_loss: 0.591234 Total_KLD_test_loss: 0.003636 Total_CEP_test_loss: 0.107184\n",
      "====> Epoch: 85 total_train_loss: 0.726134 Total_test_loss: 0.701435 Total_BCE_test_loss: 0.590535 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107213\n",
      "====> Epoch: 86 total_train_loss: 0.725071 Total_test_loss: 0.701116 Total_BCE_test_loss: 0.590265 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107191\n",
      "====> Epoch: 87 total_train_loss: 0.728733 Total_test_loss: 0.700750 Total_BCE_test_loss: 0.589891 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 88 total_train_loss: 0.728199 Total_test_loss: 0.700939 Total_BCE_test_loss: 0.590133 Total_KLD_test_loss: 0.003558 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 89 total_train_loss: 0.730494 Total_test_loss: 0.701430 Total_BCE_test_loss: 0.590518 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107303\n",
      "====> Epoch: 90 total_train_loss: 0.725988 Total_test_loss: 0.702631 Total_BCE_test_loss: 0.591773 Total_KLD_test_loss: 0.003603 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 91 total_train_loss: 0.725594 Total_test_loss: 0.699856 Total_BCE_test_loss: 0.589026 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 92 total_train_loss: 0.724671 Total_test_loss: 0.701004 Total_BCE_test_loss: 0.589991 Total_KLD_test_loss: 0.003583 Total_CEP_test_loss: 0.107429\n",
      "====> Epoch: 93 total_train_loss: 0.726691 Total_test_loss: 0.699989 Total_BCE_test_loss: 0.589089 Total_KLD_test_loss: 0.003587 Total_CEP_test_loss: 0.107313\n",
      "====> Epoch: 94 total_train_loss: 0.725271 Total_test_loss: 0.699651 Total_BCE_test_loss: 0.588627 Total_KLD_test_loss: 0.003640 Total_CEP_test_loss: 0.107384\n",
      "====> Epoch: 95 total_train_loss: 0.723782 Total_test_loss: 0.700168 Total_BCE_test_loss: 0.589109 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107396\n",
      "====> Epoch: 96 total_train_loss: 0.725984 Total_test_loss: 0.701904 Total_BCE_test_loss: 0.590843 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107351\n",
      "====> Epoch: 97 total_train_loss: 0.726741 Total_test_loss: 0.698270 Total_BCE_test_loss: 0.587304 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 98 total_train_loss: 0.724418 Total_test_loss: 0.698934 Total_BCE_test_loss: 0.587973 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 99 total_train_loss: 0.726176 Total_test_loss: 0.699355 Total_BCE_test_loss: 0.588471 Total_KLD_test_loss: 0.003617 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 100 total_train_loss: 0.724411 Total_test_loss: 0.699214 Total_BCE_test_loss: 0.588520 Total_KLD_test_loss: 0.003529 Total_CEP_test_loss: 0.107166\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "====> Epoch: 1 total_train_loss: 0.723531 Total_test_loss: 0.696638 Total_BCE_test_loss: 0.585901 Total_KLD_test_loss: 0.003573 Total_CEP_test_loss: 0.107164\n",
      "====> Epoch: 2 total_train_loss: 0.721371 Total_test_loss: 0.695297 Total_BCE_test_loss: 0.584520 Total_KLD_test_loss: 0.003570 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 3 total_train_loss: 0.721214 Total_test_loss: 0.694391 Total_BCE_test_loss: 0.583605 Total_KLD_test_loss: 0.003554 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 4 total_train_loss: 0.721059 Total_test_loss: 0.694688 Total_BCE_test_loss: 0.583796 Total_KLD_test_loss: 0.003601 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 5 total_train_loss: 0.719165 Total_test_loss: 0.694661 Total_BCE_test_loss: 0.583894 Total_KLD_test_loss: 0.003579 Total_CEP_test_loss: 0.107188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 6 total_train_loss: 0.721481 Total_test_loss: 0.694351 Total_BCE_test_loss: 0.583563 Total_KLD_test_loss: 0.003572 Total_CEP_test_loss: 0.107217\n",
      "====> Epoch: 7 total_train_loss: 0.721941 Total_test_loss: 0.693197 Total_BCE_test_loss: 0.582342 Total_KLD_test_loss: 0.003608 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 8 total_train_loss: 0.720630 Total_test_loss: 0.693358 Total_BCE_test_loss: 0.582453 Total_KLD_test_loss: 0.003629 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 9 total_train_loss: 0.718360 Total_test_loss: 0.693182 Total_BCE_test_loss: 0.582325 Total_KLD_test_loss: 0.003615 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 10 total_train_loss: 0.719279 Total_test_loss: 0.692944 Total_BCE_test_loss: 0.582019 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 11 total_train_loss: 0.720835 Total_test_loss: 0.692950 Total_BCE_test_loss: 0.582057 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107210\n",
      "====> Epoch: 12 total_train_loss: 0.720899 Total_test_loss: 0.693043 Total_BCE_test_loss: 0.582005 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107354\n",
      "====> Epoch: 13 total_train_loss: 0.719195 Total_test_loss: 0.693974 Total_BCE_test_loss: 0.583053 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 14 total_train_loss: 0.719396 Total_test_loss: 0.693487 Total_BCE_test_loss: 0.582589 Total_KLD_test_loss: 0.003634 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 15 total_train_loss: 0.719338 Total_test_loss: 0.693266 Total_BCE_test_loss: 0.582378 Total_KLD_test_loss: 0.003628 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 16 total_train_loss: 0.718375 Total_test_loss: 0.692666 Total_BCE_test_loss: 0.581727 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107309\n",
      "====> Epoch: 17 total_train_loss: 0.721422 Total_test_loss: 0.691793 Total_BCE_test_loss: 0.580833 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 18 total_train_loss: 0.721481 Total_test_loss: 0.692165 Total_BCE_test_loss: 0.581230 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 19 total_train_loss: 0.720102 Total_test_loss: 0.693100 Total_BCE_test_loss: 0.582200 Total_KLD_test_loss: 0.003611 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 20 total_train_loss: 0.718640 Total_test_loss: 0.693427 Total_BCE_test_loss: 0.582544 Total_KLD_test_loss: 0.003581 Total_CEP_test_loss: 0.107303\n",
      "====> Epoch: 21 total_train_loss: 0.717835 Total_test_loss: 0.692961 Total_BCE_test_loss: 0.582068 Total_KLD_test_loss: 0.003591 Total_CEP_test_loss: 0.107301\n",
      "====> Epoch: 22 total_train_loss: 0.718554 Total_test_loss: 0.692503 Total_BCE_test_loss: 0.581631 Total_KLD_test_loss: 0.003586 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 23 total_train_loss: 0.719772 Total_test_loss: 0.692731 Total_BCE_test_loss: 0.581864 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 24 total_train_loss: 0.722097 Total_test_loss: 0.693127 Total_BCE_test_loss: 0.582207 Total_KLD_test_loss: 0.003604 Total_CEP_test_loss: 0.107315\n",
      "====> Epoch: 25 total_train_loss: 0.718402 Total_test_loss: 0.693308 Total_BCE_test_loss: 0.582433 Total_KLD_test_loss: 0.003559 Total_CEP_test_loss: 0.107316\n",
      "====> Epoch: 26 total_train_loss: 0.720561 Total_test_loss: 0.692932 Total_BCE_test_loss: 0.582038 Total_KLD_test_loss: 0.003605 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 27 total_train_loss: 0.719277 Total_test_loss: 0.692684 Total_BCE_test_loss: 0.581805 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107299\n",
      "====> Epoch: 28 total_train_loss: 0.720634 Total_test_loss: 0.691129 Total_BCE_test_loss: 0.580207 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 29 total_train_loss: 0.718135 Total_test_loss: 0.691197 Total_BCE_test_loss: 0.580288 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 30 total_train_loss: 0.717591 Total_test_loss: 0.691693 Total_BCE_test_loss: 0.580858 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 31 total_train_loss: 0.718845 Total_test_loss: 0.691557 Total_BCE_test_loss: 0.580735 Total_KLD_test_loss: 0.003578 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 32 total_train_loss: 0.718291 Total_test_loss: 0.690985 Total_BCE_test_loss: 0.580087 Total_KLD_test_loss: 0.003636 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 33 total_train_loss: 0.719235 Total_test_loss: 0.691662 Total_BCE_test_loss: 0.580758 Total_KLD_test_loss: 0.003622 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 34 total_train_loss: 0.718469 Total_test_loss: 0.691453 Total_BCE_test_loss: 0.580468 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107329\n",
      "====> Epoch: 35 total_train_loss: 0.718908 Total_test_loss: 0.691598 Total_BCE_test_loss: 0.580650 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 36 total_train_loss: 0.720743 Total_test_loss: 0.690818 Total_BCE_test_loss: 0.579818 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107315\n",
      "====> Epoch: 37 total_train_loss: 0.717863 Total_test_loss: 0.691016 Total_BCE_test_loss: 0.580117 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107305\n",
      "====> Epoch: 38 total_train_loss: 0.718751 Total_test_loss: 0.690756 Total_BCE_test_loss: 0.579822 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 39 total_train_loss: 0.717307 Total_test_loss: 0.691061 Total_BCE_test_loss: 0.580191 Total_KLD_test_loss: 0.003548 Total_CEP_test_loss: 0.107321\n",
      "====> Epoch: 40 total_train_loss: 0.717486 Total_test_loss: 0.690541 Total_BCE_test_loss: 0.579707 Total_KLD_test_loss: 0.003546 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 41 total_train_loss: 0.719672 Total_test_loss: 0.689908 Total_BCE_test_loss: 0.579036 Total_KLD_test_loss: 0.003587 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 42 total_train_loss: 0.719159 Total_test_loss: 0.689807 Total_BCE_test_loss: 0.578928 Total_KLD_test_loss: 0.003590 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 43 total_train_loss: 0.717980 Total_test_loss: 0.689960 Total_BCE_test_loss: 0.579127 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 44 total_train_loss: 0.717795 Total_test_loss: 0.690543 Total_BCE_test_loss: 0.579727 Total_KLD_test_loss: 0.003597 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 45 total_train_loss: 0.717952 Total_test_loss: 0.690393 Total_BCE_test_loss: 0.579528 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 46 total_train_loss: 0.716834 Total_test_loss: 0.690455 Total_BCE_test_loss: 0.579550 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 47 total_train_loss: 0.717424 Total_test_loss: 0.690513 Total_BCE_test_loss: 0.579665 Total_KLD_test_loss: 0.003579 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 48 total_train_loss: 0.716064 Total_test_loss: 0.689759 Total_BCE_test_loss: 0.578859 Total_KLD_test_loss: 0.003622 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 49 total_train_loss: 0.718061 Total_test_loss: 0.690043 Total_BCE_test_loss: 0.579211 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 50 total_train_loss: 0.717594 Total_test_loss: 0.689060 Total_BCE_test_loss: 0.578216 Total_KLD_test_loss: 0.003612 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 51 total_train_loss: 0.716993 Total_test_loss: 0.689892 Total_BCE_test_loss: 0.579078 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 52 total_train_loss: 0.717408 Total_test_loss: 0.690556 Total_BCE_test_loss: 0.579737 Total_KLD_test_loss: 0.003583 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 53 total_train_loss: 0.718531 Total_test_loss: 0.690389 Total_BCE_test_loss: 0.579584 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107196\n",
      "====> Epoch: 54 total_train_loss: 0.714737 Total_test_loss: 0.689803 Total_BCE_test_loss: 0.579006 Total_KLD_test_loss: 0.003599 Total_CEP_test_loss: 0.107199\n",
      "====> Epoch: 55 total_train_loss: 0.718413 Total_test_loss: 0.689265 Total_BCE_test_loss: 0.578422 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 56 total_train_loss: 0.716785 Total_test_loss: 0.689059 Total_BCE_test_loss: 0.578206 Total_KLD_test_loss: 0.003570 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 57 total_train_loss: 0.717028 Total_test_loss: 0.688402 Total_BCE_test_loss: 0.577546 Total_KLD_test_loss: 0.003606 Total_CEP_test_loss: 0.107250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 58 total_train_loss: 0.719380 Total_test_loss: 0.688376 Total_BCE_test_loss: 0.577483 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 59 total_train_loss: 0.717312 Total_test_loss: 0.689696 Total_BCE_test_loss: 0.578781 Total_KLD_test_loss: 0.003601 Total_CEP_test_loss: 0.107314\n",
      "====> Epoch: 60 total_train_loss: 0.717936 Total_test_loss: 0.689272 Total_BCE_test_loss: 0.578426 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 61 total_train_loss: 0.718915 Total_test_loss: 0.688605 Total_BCE_test_loss: 0.577724 Total_KLD_test_loss: 0.003604 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 62 total_train_loss: 0.717582 Total_test_loss: 0.688460 Total_BCE_test_loss: 0.577601 Total_KLD_test_loss: 0.003620 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 63 total_train_loss: 0.717086 Total_test_loss: 0.689467 Total_BCE_test_loss: 0.578576 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 64 total_train_loss: 0.718902 Total_test_loss: 0.690020 Total_BCE_test_loss: 0.579158 Total_KLD_test_loss: 0.003614 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 65 total_train_loss: 0.716487 Total_test_loss: 0.689409 Total_BCE_test_loss: 0.578570 Total_KLD_test_loss: 0.003606 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 66 total_train_loss: 0.715835 Total_test_loss: 0.688539 Total_BCE_test_loss: 0.577709 Total_KLD_test_loss: 0.003645 Total_CEP_test_loss: 0.107186\n",
      "====> Epoch: 67 total_train_loss: 0.718073 Total_test_loss: 0.689132 Total_BCE_test_loss: 0.578329 Total_KLD_test_loss: 0.003618 Total_CEP_test_loss: 0.107185\n",
      "====> Epoch: 68 total_train_loss: 0.715827 Total_test_loss: 0.688885 Total_BCE_test_loss: 0.578060 Total_KLD_test_loss: 0.003613 Total_CEP_test_loss: 0.107212\n",
      "====> Epoch: 69 total_train_loss: 0.716033 Total_test_loss: 0.688254 Total_BCE_test_loss: 0.577449 Total_KLD_test_loss: 0.003605 Total_CEP_test_loss: 0.107200\n",
      "====> Epoch: 70 total_train_loss: 0.717052 Total_test_loss: 0.688996 Total_BCE_test_loss: 0.578203 Total_KLD_test_loss: 0.003565 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 71 total_train_loss: 0.717192 Total_test_loss: 0.687302 Total_BCE_test_loss: 0.576434 Total_KLD_test_loss: 0.003655 Total_CEP_test_loss: 0.107213\n",
      "====> Epoch: 72 total_train_loss: 0.716809 Total_test_loss: 0.687533 Total_BCE_test_loss: 0.576701 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107223\n",
      "====> Epoch: 73 total_train_loss: 0.714226 Total_test_loss: 0.688562 Total_BCE_test_loss: 0.577772 Total_KLD_test_loss: 0.003590 Total_CEP_test_loss: 0.107201\n",
      "====> Epoch: 74 total_train_loss: 0.714998 Total_test_loss: 0.687608 Total_BCE_test_loss: 0.576831 Total_KLD_test_loss: 0.003612 Total_CEP_test_loss: 0.107165\n",
      "====> Epoch: 75 total_train_loss: 0.717373 Total_test_loss: 0.687780 Total_BCE_test_loss: 0.577032 Total_KLD_test_loss: 0.003566 Total_CEP_test_loss: 0.107182\n",
      "====> Epoch: 76 total_train_loss: 0.714414 Total_test_loss: 0.687278 Total_BCE_test_loss: 0.576497 Total_KLD_test_loss: 0.003601 Total_CEP_test_loss: 0.107180\n",
      "====> Epoch: 77 total_train_loss: 0.717130 Total_test_loss: 0.688019 Total_BCE_test_loss: 0.577341 Total_KLD_test_loss: 0.003547 Total_CEP_test_loss: 0.107131\n",
      "====> Epoch: 78 total_train_loss: 0.716571 Total_test_loss: 0.687250 Total_BCE_test_loss: 0.576506 Total_KLD_test_loss: 0.003533 Total_CEP_test_loss: 0.107211\n",
      "====> Epoch: 79 total_train_loss: 0.714698 Total_test_loss: 0.686853 Total_BCE_test_loss: 0.576092 Total_KLD_test_loss: 0.003564 Total_CEP_test_loss: 0.107197\n",
      "====> Epoch: 80 total_train_loss: 0.717764 Total_test_loss: 0.687248 Total_BCE_test_loss: 0.576512 Total_KLD_test_loss: 0.003548 Total_CEP_test_loss: 0.107188\n",
      "====> Epoch: 81 total_train_loss: 0.716701 Total_test_loss: 0.687389 Total_BCE_test_loss: 0.576642 Total_KLD_test_loss: 0.003558 Total_CEP_test_loss: 0.107188\n",
      "====> Epoch: 82 total_train_loss: 0.717951 Total_test_loss: 0.687234 Total_BCE_test_loss: 0.576407 Total_KLD_test_loss: 0.003585 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 83 total_train_loss: 0.717446 Total_test_loss: 0.686773 Total_BCE_test_loss: 0.575981 Total_KLD_test_loss: 0.003605 Total_CEP_test_loss: 0.107187\n",
      "====> Epoch: 84 total_train_loss: 0.716080 Total_test_loss: 0.686502 Total_BCE_test_loss: 0.575758 Total_KLD_test_loss: 0.003576 Total_CEP_test_loss: 0.107167\n",
      "====> Epoch: 85 total_train_loss: 0.715553 Total_test_loss: 0.687139 Total_BCE_test_loss: 0.576415 Total_KLD_test_loss: 0.003551 Total_CEP_test_loss: 0.107173\n",
      "====> Epoch: 86 total_train_loss: 0.713654 Total_test_loss: 0.687356 Total_BCE_test_loss: 0.576621 Total_KLD_test_loss: 0.003551 Total_CEP_test_loss: 0.107185\n",
      "====> Epoch: 87 total_train_loss: 0.715810 Total_test_loss: 0.687339 Total_BCE_test_loss: 0.576607 Total_KLD_test_loss: 0.003579 Total_CEP_test_loss: 0.107154\n",
      "====> Epoch: 88 total_train_loss: 0.715428 Total_test_loss: 0.686836 Total_BCE_test_loss: 0.576078 Total_KLD_test_loss: 0.003553 Total_CEP_test_loss: 0.107205\n",
      "====> Epoch: 89 total_train_loss: 0.715237 Total_test_loss: 0.686959 Total_BCE_test_loss: 0.576148 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 90 total_train_loss: 0.716914 Total_test_loss: 0.687157 Total_BCE_test_loss: 0.576289 Total_KLD_test_loss: 0.003618 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 91 total_train_loss: 0.716135 Total_test_loss: 0.687092 Total_BCE_test_loss: 0.576256 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 92 total_train_loss: 0.717692 Total_test_loss: 0.686994 Total_BCE_test_loss: 0.576166 Total_KLD_test_loss: 0.003565 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 93 total_train_loss: 0.715895 Total_test_loss: 0.687002 Total_BCE_test_loss: 0.576168 Total_KLD_test_loss: 0.003588 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 94 total_train_loss: 0.715945 Total_test_loss: 0.686513 Total_BCE_test_loss: 0.575642 Total_KLD_test_loss: 0.003632 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 95 total_train_loss: 0.716560 Total_test_loss: 0.686809 Total_BCE_test_loss: 0.575944 Total_KLD_test_loss: 0.003585 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 96 total_train_loss: 0.716840 Total_test_loss: 0.686590 Total_BCE_test_loss: 0.575743 Total_KLD_test_loss: 0.003576 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 97 total_train_loss: 0.714822 Total_test_loss: 0.687235 Total_BCE_test_loss: 0.576390 Total_KLD_test_loss: 0.003569 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 98 total_train_loss: 0.715632 Total_test_loss: 0.687075 Total_BCE_test_loss: 0.576200 Total_KLD_test_loss: 0.003622 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 99 total_train_loss: 0.714004 Total_test_loss: 0.686294 Total_BCE_test_loss: 0.575359 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 100 total_train_loss: 0.715013 Total_test_loss: 0.685596 Total_BCE_test_loss: 0.574693 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107223\n",
      "0.0005\n",
      "====> Epoch: 1 total_train_loss: 0.717174 Total_test_loss: 0.685627 Total_BCE_test_loss: 0.574708 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 2 total_train_loss: 0.713773 Total_test_loss: 0.686230 Total_BCE_test_loss: 0.575370 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 3 total_train_loss: 0.713589 Total_test_loss: 0.685732 Total_BCE_test_loss: 0.574914 Total_KLD_test_loss: 0.003581 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 4 total_train_loss: 0.714646 Total_test_loss: 0.685664 Total_BCE_test_loss: 0.574872 Total_KLD_test_loss: 0.003569 Total_CEP_test_loss: 0.107222\n",
      "====> Epoch: 5 total_train_loss: 0.714577 Total_test_loss: 0.685717 Total_BCE_test_loss: 0.574888 Total_KLD_test_loss: 0.003615 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 6 total_train_loss: 0.715310 Total_test_loss: 0.685712 Total_BCE_test_loss: 0.574929 Total_KLD_test_loss: 0.003567 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 7 total_train_loss: 0.713964 Total_test_loss: 0.685990 Total_BCE_test_loss: 0.575204 Total_KLD_test_loss: 0.003561 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 8 total_train_loss: 0.716334 Total_test_loss: 0.686199 Total_BCE_test_loss: 0.575397 Total_KLD_test_loss: 0.003555 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 9 total_train_loss: 0.715602 Total_test_loss: 0.686081 Total_BCE_test_loss: 0.575333 Total_KLD_test_loss: 0.003521 Total_CEP_test_loss: 0.107227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 total_train_loss: 0.714534 Total_test_loss: 0.685290 Total_BCE_test_loss: 0.574487 Total_KLD_test_loss: 0.003566 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 11 total_train_loss: 0.714549 Total_test_loss: 0.685575 Total_BCE_test_loss: 0.574754 Total_KLD_test_loss: 0.003553 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 12 total_train_loss: 0.714636 Total_test_loss: 0.685459 Total_BCE_test_loss: 0.574670 Total_KLD_test_loss: 0.003543 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 13 total_train_loss: 0.714816 Total_test_loss: 0.685689 Total_BCE_test_loss: 0.574879 Total_KLD_test_loss: 0.003552 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 14 total_train_loss: 0.715073 Total_test_loss: 0.685968 Total_BCE_test_loss: 0.575075 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107309\n",
      "====> Epoch: 15 total_train_loss: 0.714259 Total_test_loss: 0.685197 Total_BCE_test_loss: 0.574307 Total_KLD_test_loss: 0.003602 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 16 total_train_loss: 0.713152 Total_test_loss: 0.684399 Total_BCE_test_loss: 0.573461 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 17 total_train_loss: 0.714436 Total_test_loss: 0.685665 Total_BCE_test_loss: 0.574766 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 18 total_train_loss: 0.714126 Total_test_loss: 0.686329 Total_BCE_test_loss: 0.575436 Total_KLD_test_loss: 0.003598 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 19 total_train_loss: 0.715875 Total_test_loss: 0.686365 Total_BCE_test_loss: 0.575449 Total_KLD_test_loss: 0.003569 Total_CEP_test_loss: 0.107348\n",
      "====> Epoch: 20 total_train_loss: 0.715207 Total_test_loss: 0.686029 Total_BCE_test_loss: 0.575208 Total_KLD_test_loss: 0.003544 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 21 total_train_loss: 0.715741 Total_test_loss: 0.685510 Total_BCE_test_loss: 0.574618 Total_KLD_test_loss: 0.003579 Total_CEP_test_loss: 0.107312\n",
      "====> Epoch: 22 total_train_loss: 0.715801 Total_test_loss: 0.684959 Total_BCE_test_loss: 0.574018 Total_KLD_test_loss: 0.003598 Total_CEP_test_loss: 0.107342\n",
      "====> Epoch: 23 total_train_loss: 0.715547 Total_test_loss: 0.685108 Total_BCE_test_loss: 0.574154 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107329\n",
      "====> Epoch: 24 total_train_loss: 0.714686 Total_test_loss: 0.685232 Total_BCE_test_loss: 0.574288 Total_KLD_test_loss: 0.003619 Total_CEP_test_loss: 0.107325\n",
      "====> Epoch: 25 total_train_loss: 0.713347 Total_test_loss: 0.685382 Total_BCE_test_loss: 0.574438 Total_KLD_test_loss: 0.003623 Total_CEP_test_loss: 0.107322\n",
      "====> Epoch: 26 total_train_loss: 0.715611 Total_test_loss: 0.685577 Total_BCE_test_loss: 0.574653 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107328\n",
      "====> Epoch: 27 total_train_loss: 0.715016 Total_test_loss: 0.685423 Total_BCE_test_loss: 0.574470 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107343\n",
      "====> Epoch: 28 total_train_loss: 0.714398 Total_test_loss: 0.685465 Total_BCE_test_loss: 0.574527 Total_KLD_test_loss: 0.003616 Total_CEP_test_loss: 0.107322\n",
      "====> Epoch: 29 total_train_loss: 0.712952 Total_test_loss: 0.685471 Total_BCE_test_loss: 0.574609 Total_KLD_test_loss: 0.003578 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 30 total_train_loss: 0.713564 Total_test_loss: 0.685619 Total_BCE_test_loss: 0.574771 Total_KLD_test_loss: 0.003564 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 31 total_train_loss: 0.716913 Total_test_loss: 0.684637 Total_BCE_test_loss: 0.573799 Total_KLD_test_loss: 0.003577 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 32 total_train_loss: 0.714473 Total_test_loss: 0.685374 Total_BCE_test_loss: 0.574421 Total_KLD_test_loss: 0.003591 Total_CEP_test_loss: 0.107362\n",
      "====> Epoch: 33 total_train_loss: 0.714761 Total_test_loss: 0.685397 Total_BCE_test_loss: 0.574445 Total_KLD_test_loss: 0.003604 Total_CEP_test_loss: 0.107347\n",
      "====> Epoch: 34 total_train_loss: 0.715040 Total_test_loss: 0.685244 Total_BCE_test_loss: 0.574332 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107328\n",
      "====> Epoch: 35 total_train_loss: 0.714321 Total_test_loss: 0.685360 Total_BCE_test_loss: 0.574502 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 36 total_train_loss: 0.714452 Total_test_loss: 0.685246 Total_BCE_test_loss: 0.574391 Total_KLD_test_loss: 0.003565 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 37 total_train_loss: 0.715211 Total_test_loss: 0.685194 Total_BCE_test_loss: 0.574337 Total_KLD_test_loss: 0.003565 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 38 total_train_loss: 0.713001 Total_test_loss: 0.685532 Total_BCE_test_loss: 0.574678 Total_KLD_test_loss: 0.003553 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 39 total_train_loss: 0.716397 Total_test_loss: 0.685360 Total_BCE_test_loss: 0.574515 Total_KLD_test_loss: 0.003571 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 40 total_train_loss: 0.714303 Total_test_loss: 0.685049 Total_BCE_test_loss: 0.574183 Total_KLD_test_loss: 0.003586 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 41 total_train_loss: 0.713614 Total_test_loss: 0.684773 Total_BCE_test_loss: 0.573905 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 42 total_train_loss: 0.713289 Total_test_loss: 0.685288 Total_BCE_test_loss: 0.574414 Total_KLD_test_loss: 0.003585 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 43 total_train_loss: 0.714748 Total_test_loss: 0.685100 Total_BCE_test_loss: 0.574244 Total_KLD_test_loss: 0.003533 Total_CEP_test_loss: 0.107323\n",
      "====> Epoch: 44 total_train_loss: 0.714784 Total_test_loss: 0.684928 Total_BCE_test_loss: 0.574067 Total_KLD_test_loss: 0.003552 Total_CEP_test_loss: 0.107309\n",
      "====> Epoch: 45 total_train_loss: 0.714533 Total_test_loss: 0.684927 Total_BCE_test_loss: 0.574047 Total_KLD_test_loss: 0.003577 Total_CEP_test_loss: 0.107303\n",
      "====> Epoch: 46 total_train_loss: 0.716236 Total_test_loss: 0.685482 Total_BCE_test_loss: 0.574648 Total_KLD_test_loss: 0.003530 Total_CEP_test_loss: 0.107304\n",
      "====> Epoch: 47 total_train_loss: 0.712864 Total_test_loss: 0.685939 Total_BCE_test_loss: 0.575096 Total_KLD_test_loss: 0.003527 Total_CEP_test_loss: 0.107315\n",
      "====> Epoch: 48 total_train_loss: 0.712970 Total_test_loss: 0.685768 Total_BCE_test_loss: 0.574966 Total_KLD_test_loss: 0.003524 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 49 total_train_loss: 0.711977 Total_test_loss: 0.685548 Total_BCE_test_loss: 0.574747 Total_KLD_test_loss: 0.003526 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 50 total_train_loss: 0.713542 Total_test_loss: 0.685490 Total_BCE_test_loss: 0.574720 Total_KLD_test_loss: 0.003496 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 51 total_train_loss: 0.714693 Total_test_loss: 0.684748 Total_BCE_test_loss: 0.573933 Total_KLD_test_loss: 0.003547 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 52 total_train_loss: 0.713592 Total_test_loss: 0.684537 Total_BCE_test_loss: 0.573732 Total_KLD_test_loss: 0.003567 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 53 total_train_loss: 0.713319 Total_test_loss: 0.684373 Total_BCE_test_loss: 0.573559 Total_KLD_test_loss: 0.003559 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 54 total_train_loss: 0.713481 Total_test_loss: 0.684635 Total_BCE_test_loss: 0.573809 Total_KLD_test_loss: 0.003554 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 55 total_train_loss: 0.714422 Total_test_loss: 0.684833 Total_BCE_test_loss: 0.573993 Total_KLD_test_loss: 0.003576 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 56 total_train_loss: 0.712135 Total_test_loss: 0.684834 Total_BCE_test_loss: 0.573969 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 57 total_train_loss: 0.713235 Total_test_loss: 0.683980 Total_BCE_test_loss: 0.573108 Total_KLD_test_loss: 0.003632 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 58 total_train_loss: 0.713706 Total_test_loss: 0.684896 Total_BCE_test_loss: 0.574100 Total_KLD_test_loss: 0.003544 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 59 total_train_loss: 0.712713 Total_test_loss: 0.684035 Total_BCE_test_loss: 0.573191 Total_KLD_test_loss: 0.003570 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 60 total_train_loss: 0.713591 Total_test_loss: 0.684354 Total_BCE_test_loss: 0.573562 Total_KLD_test_loss: 0.003568 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 61 total_train_loss: 0.715786 Total_test_loss: 0.684775 Total_BCE_test_loss: 0.573929 Total_KLD_test_loss: 0.003591 Total_CEP_test_loss: 0.107255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 62 total_train_loss: 0.716848 Total_test_loss: 0.684603 Total_BCE_test_loss: 0.573766 Total_KLD_test_loss: 0.003567 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 63 total_train_loss: 0.710493 Total_test_loss: 0.684927 Total_BCE_test_loss: 0.574078 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 64 total_train_loss: 0.714299 Total_test_loss: 0.684194 Total_BCE_test_loss: 0.573330 Total_KLD_test_loss: 0.003601 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 65 total_train_loss: 0.713813 Total_test_loss: 0.683875 Total_BCE_test_loss: 0.573053 Total_KLD_test_loss: 0.003560 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 66 total_train_loss: 0.714426 Total_test_loss: 0.684073 Total_BCE_test_loss: 0.573256 Total_KLD_test_loss: 0.003549 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 67 total_train_loss: 0.712246 Total_test_loss: 0.684044 Total_BCE_test_loss: 0.573235 Total_KLD_test_loss: 0.003534 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 68 total_train_loss: 0.715336 Total_test_loss: 0.683628 Total_BCE_test_loss: 0.572755 Total_KLD_test_loss: 0.003591 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 69 total_train_loss: 0.713097 Total_test_loss: 0.683927 Total_BCE_test_loss: 0.573072 Total_KLD_test_loss: 0.003575 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 70 total_train_loss: 0.714427 Total_test_loss: 0.683869 Total_BCE_test_loss: 0.572983 Total_KLD_test_loss: 0.003621 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 71 total_train_loss: 0.715616 Total_test_loss: 0.684849 Total_BCE_test_loss: 0.573949 Total_KLD_test_loss: 0.003612 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 72 total_train_loss: 0.713212 Total_test_loss: 0.684480 Total_BCE_test_loss: 0.573601 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 73 total_train_loss: 0.714218 Total_test_loss: 0.684865 Total_BCE_test_loss: 0.574016 Total_KLD_test_loss: 0.003576 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 74 total_train_loss: 0.712757 Total_test_loss: 0.685315 Total_BCE_test_loss: 0.574473 Total_KLD_test_loss: 0.003590 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 75 total_train_loss: 0.712253 Total_test_loss: 0.684743 Total_BCE_test_loss: 0.573820 Total_KLD_test_loss: 0.003642 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 76 total_train_loss: 0.712997 Total_test_loss: 0.685745 Total_BCE_test_loss: 0.574858 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107307\n",
      "====> Epoch: 77 total_train_loss: 0.715550 Total_test_loss: 0.685114 Total_BCE_test_loss: 0.574276 Total_KLD_test_loss: 0.003562 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 78 total_train_loss: 0.710967 Total_test_loss: 0.685316 Total_BCE_test_loss: 0.574454 Total_KLD_test_loss: 0.003577 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 79 total_train_loss: 0.715137 Total_test_loss: 0.684741 Total_BCE_test_loss: 0.573829 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 80 total_train_loss: 0.714582 Total_test_loss: 0.683568 Total_BCE_test_loss: 0.572647 Total_KLD_test_loss: 0.003629 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 81 total_train_loss: 0.712813 Total_test_loss: 0.684620 Total_BCE_test_loss: 0.573670 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107313\n",
      "====> Epoch: 82 total_train_loss: 0.713522 Total_test_loss: 0.684251 Total_BCE_test_loss: 0.573348 Total_KLD_test_loss: 0.003597 Total_CEP_test_loss: 0.107305\n",
      "====> Epoch: 83 total_train_loss: 0.713974 Total_test_loss: 0.684220 Total_BCE_test_loss: 0.573342 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 84 total_train_loss: 0.713681 Total_test_loss: 0.684997 Total_BCE_test_loss: 0.574150 Total_KLD_test_loss: 0.003536 Total_CEP_test_loss: 0.107311\n",
      "====> Epoch: 85 total_train_loss: 0.712422 Total_test_loss: 0.684910 Total_BCE_test_loss: 0.574036 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107295\n",
      "====> Epoch: 86 total_train_loss: 0.712316 Total_test_loss: 0.684306 Total_BCE_test_loss: 0.573412 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 87 total_train_loss: 0.714613 Total_test_loss: 0.684686 Total_BCE_test_loss: 0.573828 Total_KLD_test_loss: 0.003601 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 88 total_train_loss: 0.711228 Total_test_loss: 0.684922 Total_BCE_test_loss: 0.574056 Total_KLD_test_loss: 0.003582 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 89 total_train_loss: 0.713930 Total_test_loss: 0.684418 Total_BCE_test_loss: 0.573542 Total_KLD_test_loss: 0.003569 Total_CEP_test_loss: 0.107307\n",
      "====> Epoch: 90 total_train_loss: 0.712314 Total_test_loss: 0.684412 Total_BCE_test_loss: 0.573625 Total_KLD_test_loss: 0.003521 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 91 total_train_loss: 0.713654 Total_test_loss: 0.683636 Total_BCE_test_loss: 0.572789 Total_KLD_test_loss: 0.003559 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 92 total_train_loss: 0.712291 Total_test_loss: 0.683299 Total_BCE_test_loss: 0.572426 Total_KLD_test_loss: 0.003609 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 93 total_train_loss: 0.714443 Total_test_loss: 0.683093 Total_BCE_test_loss: 0.572132 Total_KLD_test_loss: 0.003623 Total_CEP_test_loss: 0.107338\n",
      "====> Epoch: 94 total_train_loss: 0.714312 Total_test_loss: 0.684465 Total_BCE_test_loss: 0.573601 Total_KLD_test_loss: 0.003575 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 95 total_train_loss: 0.712594 Total_test_loss: 0.684512 Total_BCE_test_loss: 0.573625 Total_KLD_test_loss: 0.003613 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 96 total_train_loss: 0.713233 Total_test_loss: 0.684746 Total_BCE_test_loss: 0.573888 Total_KLD_test_loss: 0.003580 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 97 total_train_loss: 0.713367 Total_test_loss: 0.684246 Total_BCE_test_loss: 0.573363 Total_KLD_test_loss: 0.003597 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 98 total_train_loss: 0.714076 Total_test_loss: 0.684708 Total_BCE_test_loss: 0.573840 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 99 total_train_loss: 0.711475 Total_test_loss: 0.683958 Total_BCE_test_loss: 0.573058 Total_KLD_test_loss: 0.003622 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 100 total_train_loss: 0.716642 Total_test_loss: 0.683095 Total_BCE_test_loss: 0.572095 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 101 total_train_loss: 0.714646 Total_test_loss: 0.683326 Total_BCE_test_loss: 0.572389 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 102 total_train_loss: 0.713622 Total_test_loss: 0.684021 Total_BCE_test_loss: 0.573116 Total_KLD_test_loss: 0.003619 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 103 total_train_loss: 0.713338 Total_test_loss: 0.683360 Total_BCE_test_loss: 0.572471 Total_KLD_test_loss: 0.003613 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 104 total_train_loss: 0.712622 Total_test_loss: 0.683653 Total_BCE_test_loss: 0.572785 Total_KLD_test_loss: 0.003605 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 105 total_train_loss: 0.713267 Total_test_loss: 0.683994 Total_BCE_test_loss: 0.573108 Total_KLD_test_loss: 0.003578 Total_CEP_test_loss: 0.107308\n",
      "====> Epoch: 106 total_train_loss: 0.712652 Total_test_loss: 0.683513 Total_BCE_test_loss: 0.572670 Total_KLD_test_loss: 0.003566 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 107 total_train_loss: 0.713579 Total_test_loss: 0.683416 Total_BCE_test_loss: 0.572489 Total_KLD_test_loss: 0.003616 Total_CEP_test_loss: 0.107312\n",
      "====> Epoch: 108 total_train_loss: 0.714844 Total_test_loss: 0.682922 Total_BCE_test_loss: 0.571959 Total_KLD_test_loss: 0.003644 Total_CEP_test_loss: 0.107319\n",
      "====> Epoch: 109 total_train_loss: 0.712088 Total_test_loss: 0.683063 Total_BCE_test_loss: 0.572139 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 110 total_train_loss: 0.712176 Total_test_loss: 0.683084 Total_BCE_test_loss: 0.572115 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107322\n",
      "====> Epoch: 111 total_train_loss: 0.712574 Total_test_loss: 0.683895 Total_BCE_test_loss: 0.572967 Total_KLD_test_loss: 0.003623 Total_CEP_test_loss: 0.107306\n",
      "====> Epoch: 112 total_train_loss: 0.713897 Total_test_loss: 0.683422 Total_BCE_test_loss: 0.572530 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 113 total_train_loss: 0.712367 Total_test_loss: 0.683715 Total_BCE_test_loss: 0.572851 Total_KLD_test_loss: 0.003583 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 114 total_train_loss: 0.713314 Total_test_loss: 0.684278 Total_BCE_test_loss: 0.573432 Total_KLD_test_loss: 0.003573 Total_CEP_test_loss: 0.107274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 115 total_train_loss: 0.714559 Total_test_loss: 0.683889 Total_BCE_test_loss: 0.573047 Total_KLD_test_loss: 0.003574 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 116 total_train_loss: 0.712706 Total_test_loss: 0.683744 Total_BCE_test_loss: 0.572891 Total_KLD_test_loss: 0.003570 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 117 total_train_loss: 0.711728 Total_test_loss: 0.683541 Total_BCE_test_loss: 0.572679 Total_KLD_test_loss: 0.003564 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 118 total_train_loss: 0.714963 Total_test_loss: 0.683254 Total_BCE_test_loss: 0.572431 Total_KLD_test_loss: 0.003572 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 119 total_train_loss: 0.713771 Total_test_loss: 0.682933 Total_BCE_test_loss: 0.572059 Total_KLD_test_loss: 0.003618 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 120 total_train_loss: 0.711322 Total_test_loss: 0.684095 Total_BCE_test_loss: 0.573255 Total_KLD_test_loss: 0.003560 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 121 total_train_loss: 0.712768 Total_test_loss: 0.684104 Total_BCE_test_loss: 0.573308 Total_KLD_test_loss: 0.003549 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 122 total_train_loss: 0.710586 Total_test_loss: 0.684458 Total_BCE_test_loss: 0.573671 Total_KLD_test_loss: 0.003535 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 123 total_train_loss: 0.712006 Total_test_loss: 0.684207 Total_BCE_test_loss: 0.573408 Total_KLD_test_loss: 0.003543 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 124 total_train_loss: 0.712674 Total_test_loss: 0.683345 Total_BCE_test_loss: 0.572539 Total_KLD_test_loss: 0.003538 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 125 total_train_loss: 0.713154 Total_test_loss: 0.683622 Total_BCE_test_loss: 0.572824 Total_KLD_test_loss: 0.003510 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 126 total_train_loss: 0.713536 Total_test_loss: 0.683524 Total_BCE_test_loss: 0.572759 Total_KLD_test_loss: 0.003507 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 127 total_train_loss: 0.710356 Total_test_loss: 0.684470 Total_BCE_test_loss: 0.573732 Total_KLD_test_loss: 0.003502 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 128 total_train_loss: 0.712120 Total_test_loss: 0.683237 Total_BCE_test_loss: 0.572376 Total_KLD_test_loss: 0.003568 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 129 total_train_loss: 0.715156 Total_test_loss: 0.682956 Total_BCE_test_loss: 0.572039 Total_KLD_test_loss: 0.003611 Total_CEP_test_loss: 0.107306\n",
      "====> Epoch: 130 total_train_loss: 0.713385 Total_test_loss: 0.682893 Total_BCE_test_loss: 0.571993 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 131 total_train_loss: 0.713301 Total_test_loss: 0.683075 Total_BCE_test_loss: 0.572178 Total_KLD_test_loss: 0.003600 Total_CEP_test_loss: 0.107297\n",
      "====> Epoch: 132 total_train_loss: 0.713468 Total_test_loss: 0.682895 Total_BCE_test_loss: 0.572001 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 133 total_train_loss: 0.712279 Total_test_loss: 0.683674 Total_BCE_test_loss: 0.572781 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107300\n",
      "====> Epoch: 134 total_train_loss: 0.712461 Total_test_loss: 0.683996 Total_BCE_test_loss: 0.573109 Total_KLD_test_loss: 0.003598 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 135 total_train_loss: 0.712004 Total_test_loss: 0.683445 Total_BCE_test_loss: 0.572506 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107308\n",
      "====> Epoch: 136 total_train_loss: 0.714718 Total_test_loss: 0.683026 Total_BCE_test_loss: 0.572122 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107310\n",
      "====> Epoch: 137 total_train_loss: 0.709864 Total_test_loss: 0.682885 Total_BCE_test_loss: 0.571962 Total_KLD_test_loss: 0.003634 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 138 total_train_loss: 0.709575 Total_test_loss: 0.683244 Total_BCE_test_loss: 0.572285 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107320\n",
      "====> Epoch: 139 total_train_loss: 0.711119 Total_test_loss: 0.682280 Total_BCE_test_loss: 0.571302 Total_KLD_test_loss: 0.003655 Total_CEP_test_loss: 0.107324\n",
      "====> Epoch: 140 total_train_loss: 0.711334 Total_test_loss: 0.682918 Total_BCE_test_loss: 0.572034 Total_KLD_test_loss: 0.003599 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 141 total_train_loss: 0.713738 Total_test_loss: 0.682649 Total_BCE_test_loss: 0.571740 Total_KLD_test_loss: 0.003602 Total_CEP_test_loss: 0.107308\n",
      "====> Epoch: 142 total_train_loss: 0.711831 Total_test_loss: 0.682800 Total_BCE_test_loss: 0.571904 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 143 total_train_loss: 0.712307 Total_test_loss: 0.682718 Total_BCE_test_loss: 0.571868 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 144 total_train_loss: 0.713495 Total_test_loss: 0.682546 Total_BCE_test_loss: 0.571693 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 145 total_train_loss: 0.712208 Total_test_loss: 0.683034 Total_BCE_test_loss: 0.572182 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 146 total_train_loss: 0.712605 Total_test_loss: 0.682777 Total_BCE_test_loss: 0.571900 Total_KLD_test_loss: 0.003621 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 147 total_train_loss: 0.712607 Total_test_loss: 0.682736 Total_BCE_test_loss: 0.571882 Total_KLD_test_loss: 0.003616 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 148 total_train_loss: 0.711358 Total_test_loss: 0.682702 Total_BCE_test_loss: 0.571836 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 149 total_train_loss: 0.712539 Total_test_loss: 0.682937 Total_BCE_test_loss: 0.572140 Total_KLD_test_loss: 0.003584 Total_CEP_test_loss: 0.107213\n",
      "====> Epoch: 150 total_train_loss: 0.713160 Total_test_loss: 0.681763 Total_BCE_test_loss: 0.570928 Total_KLD_test_loss: 0.003607 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 151 total_train_loss: 0.711588 Total_test_loss: 0.682181 Total_BCE_test_loss: 0.571354 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107192\n",
      "====> Epoch: 152 total_train_loss: 0.713496 Total_test_loss: 0.682833 Total_BCE_test_loss: 0.571982 Total_KLD_test_loss: 0.003589 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 153 total_train_loss: 0.710865 Total_test_loss: 0.682591 Total_BCE_test_loss: 0.571737 Total_KLD_test_loss: 0.003607 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 154 total_train_loss: 0.711699 Total_test_loss: 0.682340 Total_BCE_test_loss: 0.571526 Total_KLD_test_loss: 0.003594 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 155 total_train_loss: 0.714197 Total_test_loss: 0.682545 Total_BCE_test_loss: 0.571696 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 156 total_train_loss: 0.711974 Total_test_loss: 0.682179 Total_BCE_test_loss: 0.571341 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107214\n",
      "====> Epoch: 157 total_train_loss: 0.712158 Total_test_loss: 0.683128 Total_BCE_test_loss: 0.572339 Total_KLD_test_loss: 0.003571 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 158 total_train_loss: 0.711741 Total_test_loss: 0.682626 Total_BCE_test_loss: 0.571808 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 159 total_train_loss: 0.713131 Total_test_loss: 0.682757 Total_BCE_test_loss: 0.571993 Total_KLD_test_loss: 0.003571 Total_CEP_test_loss: 0.107193\n",
      "====> Epoch: 160 total_train_loss: 0.712065 Total_test_loss: 0.683215 Total_BCE_test_loss: 0.572471 Total_KLD_test_loss: 0.003562 Total_CEP_test_loss: 0.107182\n",
      "====> Epoch: 161 total_train_loss: 0.708896 Total_test_loss: 0.682977 Total_BCE_test_loss: 0.572231 Total_KLD_test_loss: 0.003552 Total_CEP_test_loss: 0.107194\n",
      "====> Epoch: 162 total_train_loss: 0.711805 Total_test_loss: 0.682726 Total_BCE_test_loss: 0.571964 Total_KLD_test_loss: 0.003527 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 163 total_train_loss: 0.709250 Total_test_loss: 0.682503 Total_BCE_test_loss: 0.571753 Total_KLD_test_loss: 0.003532 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 164 total_train_loss: 0.712969 Total_test_loss: 0.681996 Total_BCE_test_loss: 0.571235 Total_KLD_test_loss: 0.003577 Total_CEP_test_loss: 0.107184\n",
      "====> Epoch: 165 total_train_loss: 0.710446 Total_test_loss: 0.682561 Total_BCE_test_loss: 0.571834 Total_KLD_test_loss: 0.003536 Total_CEP_test_loss: 0.107191\n",
      "====> Epoch: 166 total_train_loss: 0.711379 Total_test_loss: 0.682159 Total_BCE_test_loss: 0.571378 Total_KLD_test_loss: 0.003575 Total_CEP_test_loss: 0.107206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 167 total_train_loss: 0.709803 Total_test_loss: 0.682378 Total_BCE_test_loss: 0.571590 Total_KLD_test_loss: 0.003559 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 168 total_train_loss: 0.714297 Total_test_loss: 0.682321 Total_BCE_test_loss: 0.571537 Total_KLD_test_loss: 0.003574 Total_CEP_test_loss: 0.107210\n",
      "====> Epoch: 169 total_train_loss: 0.713212 Total_test_loss: 0.682203 Total_BCE_test_loss: 0.571309 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 170 total_train_loss: 0.711452 Total_test_loss: 0.682285 Total_BCE_test_loss: 0.571445 Total_KLD_test_loss: 0.003604 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 171 total_train_loss: 0.710259 Total_test_loss: 0.681905 Total_BCE_test_loss: 0.571090 Total_KLD_test_loss: 0.003586 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 172 total_train_loss: 0.710652 Total_test_loss: 0.682260 Total_BCE_test_loss: 0.571468 Total_KLD_test_loss: 0.003577 Total_CEP_test_loss: 0.107214\n",
      "====> Epoch: 173 total_train_loss: 0.711760 Total_test_loss: 0.681618 Total_BCE_test_loss: 0.570754 Total_KLD_test_loss: 0.003633 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 174 total_train_loss: 0.710511 Total_test_loss: 0.682058 Total_BCE_test_loss: 0.571241 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107194\n",
      "====> Epoch: 175 total_train_loss: 0.711890 Total_test_loss: 0.682377 Total_BCE_test_loss: 0.571571 Total_KLD_test_loss: 0.003587 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 176 total_train_loss: 0.715445 Total_test_loss: 0.681413 Total_BCE_test_loss: 0.570587 Total_KLD_test_loss: 0.003621 Total_CEP_test_loss: 0.107204\n",
      "====> Epoch: 177 total_train_loss: 0.710815 Total_test_loss: 0.681767 Total_BCE_test_loss: 0.570967 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 178 total_train_loss: 0.712956 Total_test_loss: 0.682229 Total_BCE_test_loss: 0.571414 Total_KLD_test_loss: 0.003600 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 179 total_train_loss: 0.711192 Total_test_loss: 0.682398 Total_BCE_test_loss: 0.571642 Total_KLD_test_loss: 0.003588 Total_CEP_test_loss: 0.107168\n",
      "====> Epoch: 180 total_train_loss: 0.712336 Total_test_loss: 0.681494 Total_BCE_test_loss: 0.570669 Total_KLD_test_loss: 0.003631 Total_CEP_test_loss: 0.107193\n",
      "====> Epoch: 181 total_train_loss: 0.711863 Total_test_loss: 0.681710 Total_BCE_test_loss: 0.570916 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107203\n",
      "====> Epoch: 182 total_train_loss: 0.710442 Total_test_loss: 0.682132 Total_BCE_test_loss: 0.571348 Total_KLD_test_loss: 0.003588 Total_CEP_test_loss: 0.107197\n",
      "====> Epoch: 183 total_train_loss: 0.711242 Total_test_loss: 0.682101 Total_BCE_test_loss: 0.571319 Total_KLD_test_loss: 0.003574 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 184 total_train_loss: 0.712949 Total_test_loss: 0.681730 Total_BCE_test_loss: 0.570924 Total_KLD_test_loss: 0.003598 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 185 total_train_loss: 0.710920 Total_test_loss: 0.681373 Total_BCE_test_loss: 0.570543 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107200\n",
      "====> Epoch: 186 total_train_loss: 0.712764 Total_test_loss: 0.681572 Total_BCE_test_loss: 0.570674 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107212\n",
      "====> Epoch: 187 total_train_loss: 0.711084 Total_test_loss: 0.681258 Total_BCE_test_loss: 0.570329 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107199\n",
      "====> Epoch: 188 total_train_loss: 0.712074 Total_test_loss: 0.682006 Total_BCE_test_loss: 0.571125 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107181\n",
      "====> Epoch: 189 total_train_loss: 0.710154 Total_test_loss: 0.683065 Total_BCE_test_loss: 0.572241 Total_KLD_test_loss: 0.003629 Total_CEP_test_loss: 0.107195\n",
      "====> Epoch: 190 total_train_loss: 0.711338 Total_test_loss: 0.682663 Total_BCE_test_loss: 0.571837 Total_KLD_test_loss: 0.003611 Total_CEP_test_loss: 0.107216\n",
      "====> Epoch: 191 total_train_loss: 0.711399 Total_test_loss: 0.682314 Total_BCE_test_loss: 0.571467 Total_KLD_test_loss: 0.003626 Total_CEP_test_loss: 0.107221\n",
      "====> Epoch: 192 total_train_loss: 0.710583 Total_test_loss: 0.682041 Total_BCE_test_loss: 0.571145 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 193 total_train_loss: 0.711125 Total_test_loss: 0.681883 Total_BCE_test_loss: 0.571000 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 194 total_train_loss: 0.712572 Total_test_loss: 0.681770 Total_BCE_test_loss: 0.570870 Total_KLD_test_loss: 0.003647 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 195 total_train_loss: 0.712003 Total_test_loss: 0.682034 Total_BCE_test_loss: 0.571215 Total_KLD_test_loss: 0.003599 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 196 total_train_loss: 0.710878 Total_test_loss: 0.681281 Total_BCE_test_loss: 0.570415 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 197 total_train_loss: 0.712566 Total_test_loss: 0.680927 Total_BCE_test_loss: 0.570013 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 198 total_train_loss: 0.711145 Total_test_loss: 0.681741 Total_BCE_test_loss: 0.570919 Total_KLD_test_loss: 0.003602 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 199 total_train_loss: 0.711176 Total_test_loss: 0.681915 Total_BCE_test_loss: 0.571098 Total_KLD_test_loss: 0.003602 Total_CEP_test_loss: 0.107214\n",
      "====> Epoch: 200 total_train_loss: 0.711315 Total_test_loss: 0.681724 Total_BCE_test_loss: 0.570832 Total_KLD_test_loss: 0.003620 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 201 total_train_loss: 0.710828 Total_test_loss: 0.680939 Total_BCE_test_loss: 0.569998 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 202 total_train_loss: 0.711119 Total_test_loss: 0.681605 Total_BCE_test_loss: 0.570666 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 203 total_train_loss: 0.710916 Total_test_loss: 0.681273 Total_BCE_test_loss: 0.570340 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 204 total_train_loss: 0.711273 Total_test_loss: 0.681214 Total_BCE_test_loss: 0.570249 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 205 total_train_loss: 0.711150 Total_test_loss: 0.682204 Total_BCE_test_loss: 0.571310 Total_KLD_test_loss: 0.003642 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 206 total_train_loss: 0.712048 Total_test_loss: 0.682051 Total_BCE_test_loss: 0.571159 Total_KLD_test_loss: 0.003644 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 207 total_train_loss: 0.711993 Total_test_loss: 0.681588 Total_BCE_test_loss: 0.570689 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107209\n",
      "====> Epoch: 208 total_train_loss: 0.709414 Total_test_loss: 0.681713 Total_BCE_test_loss: 0.570816 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 209 total_train_loss: 0.711708 Total_test_loss: 0.681757 Total_BCE_test_loss: 0.570842 Total_KLD_test_loss: 0.003644 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 210 total_train_loss: 0.712018 Total_test_loss: 0.682220 Total_BCE_test_loss: 0.571331 Total_KLD_test_loss: 0.003641 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 211 total_train_loss: 0.710453 Total_test_loss: 0.682254 Total_BCE_test_loss: 0.571387 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 212 total_train_loss: 0.709257 Total_test_loss: 0.682041 Total_BCE_test_loss: 0.571192 Total_KLD_test_loss: 0.003608 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 213 total_train_loss: 0.710163 Total_test_loss: 0.681781 Total_BCE_test_loss: 0.570901 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 214 total_train_loss: 0.710815 Total_test_loss: 0.681747 Total_BCE_test_loss: 0.570884 Total_KLD_test_loss: 0.003623 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 215 total_train_loss: 0.711256 Total_test_loss: 0.681733 Total_BCE_test_loss: 0.570844 Total_KLD_test_loss: 0.003649 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 216 total_train_loss: 0.710892 Total_test_loss: 0.681796 Total_BCE_test_loss: 0.570887 Total_KLD_test_loss: 0.003653 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 217 total_train_loss: 0.711727 Total_test_loss: 0.681778 Total_BCE_test_loss: 0.570887 Total_KLD_test_loss: 0.003652 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 218 total_train_loss: 0.711885 Total_test_loss: 0.682186 Total_BCE_test_loss: 0.571356 Total_KLD_test_loss: 0.003603 Total_CEP_test_loss: 0.107228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 219 total_train_loss: 0.710796 Total_test_loss: 0.681651 Total_BCE_test_loss: 0.570823 Total_KLD_test_loss: 0.003610 Total_CEP_test_loss: 0.107217\n",
      "====> Epoch: 220 total_train_loss: 0.708721 Total_test_loss: 0.681882 Total_BCE_test_loss: 0.571108 Total_KLD_test_loss: 0.003553 Total_CEP_test_loss: 0.107221\n",
      "====> Epoch: 221 total_train_loss: 0.710248 Total_test_loss: 0.680549 Total_BCE_test_loss: 0.569733 Total_KLD_test_loss: 0.003626 Total_CEP_test_loss: 0.107190\n",
      "====> Epoch: 222 total_train_loss: 0.710828 Total_test_loss: 0.681044 Total_BCE_test_loss: 0.570258 Total_KLD_test_loss: 0.003603 Total_CEP_test_loss: 0.107183\n",
      "====> Epoch: 223 total_train_loss: 0.711393 Total_test_loss: 0.680362 Total_BCE_test_loss: 0.569506 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107195\n",
      "====> Epoch: 224 total_train_loss: 0.711296 Total_test_loss: 0.681366 Total_BCE_test_loss: 0.570515 Total_KLD_test_loss: 0.003607 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 225 total_train_loss: 0.710568 Total_test_loss: 0.681478 Total_BCE_test_loss: 0.570677 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107206\n",
      "====> Epoch: 226 total_train_loss: 0.709815 Total_test_loss: 0.681043 Total_BCE_test_loss: 0.570238 Total_KLD_test_loss: 0.003608 Total_CEP_test_loss: 0.107197\n",
      "====> Epoch: 227 total_train_loss: 0.711694 Total_test_loss: 0.680820 Total_BCE_test_loss: 0.569997 Total_KLD_test_loss: 0.003616 Total_CEP_test_loss: 0.107207\n",
      "====> Epoch: 228 total_train_loss: 0.710292 Total_test_loss: 0.680521 Total_BCE_test_loss: 0.569645 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 229 total_train_loss: 0.711283 Total_test_loss: 0.680512 Total_BCE_test_loss: 0.569687 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107164\n",
      "====> Epoch: 230 total_train_loss: 0.710024 Total_test_loss: 0.680509 Total_BCE_test_loss: 0.569738 Total_KLD_test_loss: 0.003605 Total_CEP_test_loss: 0.107166\n",
      "====> Epoch: 231 total_train_loss: 0.710240 Total_test_loss: 0.681406 Total_BCE_test_loss: 0.570573 Total_KLD_test_loss: 0.003607 Total_CEP_test_loss: 0.107227\n",
      "====> Epoch: 232 total_train_loss: 0.708880 Total_test_loss: 0.681265 Total_BCE_test_loss: 0.570450 Total_KLD_test_loss: 0.003597 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 233 total_train_loss: 0.710487 Total_test_loss: 0.681534 Total_BCE_test_loss: 0.570723 Total_KLD_test_loss: 0.003592 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 234 total_train_loss: 0.709616 Total_test_loss: 0.681093 Total_BCE_test_loss: 0.570287 Total_KLD_test_loss: 0.003589 Total_CEP_test_loss: 0.107216\n",
      "====> Epoch: 235 total_train_loss: 0.710101 Total_test_loss: 0.680528 Total_BCE_test_loss: 0.569689 Total_KLD_test_loss: 0.003619 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 236 total_train_loss: 0.710739 Total_test_loss: 0.680801 Total_BCE_test_loss: 0.569954 Total_KLD_test_loss: 0.003615 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 237 total_train_loss: 0.709247 Total_test_loss: 0.681494 Total_BCE_test_loss: 0.570606 Total_KLD_test_loss: 0.003625 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 238 total_train_loss: 0.710671 Total_test_loss: 0.681317 Total_BCE_test_loss: 0.570421 Total_KLD_test_loss: 0.003612 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 239 total_train_loss: 0.711678 Total_test_loss: 0.681182 Total_BCE_test_loss: 0.570282 Total_KLD_test_loss: 0.003652 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 240 total_train_loss: 0.709816 Total_test_loss: 0.680837 Total_BCE_test_loss: 0.570006 Total_KLD_test_loss: 0.003618 Total_CEP_test_loss: 0.107213\n",
      "====> Epoch: 241 total_train_loss: 0.712912 Total_test_loss: 0.680720 Total_BCE_test_loss: 0.569860 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 242 total_train_loss: 0.709475 Total_test_loss: 0.680773 Total_BCE_test_loss: 0.569879 Total_KLD_test_loss: 0.003632 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 243 total_train_loss: 0.712117 Total_test_loss: 0.681183 Total_BCE_test_loss: 0.570325 Total_KLD_test_loss: 0.003625 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 244 total_train_loss: 0.713004 Total_test_loss: 0.680743 Total_BCE_test_loss: 0.569872 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107212\n",
      "====> Epoch: 245 total_train_loss: 0.709944 Total_test_loss: 0.680584 Total_BCE_test_loss: 0.569661 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 246 total_train_loss: 0.710367 Total_test_loss: 0.680685 Total_BCE_test_loss: 0.569761 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 247 total_train_loss: 0.707675 Total_test_loss: 0.680971 Total_BCE_test_loss: 0.570073 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 248 total_train_loss: 0.711459 Total_test_loss: 0.680700 Total_BCE_test_loss: 0.569794 Total_KLD_test_loss: 0.003639 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 249 total_train_loss: 0.711165 Total_test_loss: 0.681101 Total_BCE_test_loss: 0.570162 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 250 total_train_loss: 0.711214 Total_test_loss: 0.680687 Total_BCE_test_loss: 0.569770 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 251 total_train_loss: 0.710945 Total_test_loss: 0.680619 Total_BCE_test_loss: 0.569741 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 252 total_train_loss: 0.710207 Total_test_loss: 0.681080 Total_BCE_test_loss: 0.570191 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 253 total_train_loss: 0.711242 Total_test_loss: 0.680399 Total_BCE_test_loss: 0.569455 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 254 total_train_loss: 0.710466 Total_test_loss: 0.680867 Total_BCE_test_loss: 0.569912 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 255 total_train_loss: 0.711607 Total_test_loss: 0.680454 Total_BCE_test_loss: 0.569560 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107223\n",
      "====> Epoch: 256 total_train_loss: 0.710332 Total_test_loss: 0.680623 Total_BCE_test_loss: 0.569735 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 257 total_train_loss: 0.711706 Total_test_loss: 0.680956 Total_BCE_test_loss: 0.570104 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107196\n",
      "====> Epoch: 258 total_train_loss: 0.709147 Total_test_loss: 0.681615 Total_BCE_test_loss: 0.570809 Total_KLD_test_loss: 0.003625 Total_CEP_test_loss: 0.107181\n",
      "====> Epoch: 259 total_train_loss: 0.709428 Total_test_loss: 0.681364 Total_BCE_test_loss: 0.570496 Total_KLD_test_loss: 0.003644 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 260 total_train_loss: 0.709094 Total_test_loss: 0.681434 Total_BCE_test_loss: 0.570583 Total_KLD_test_loss: 0.003649 Total_CEP_test_loss: 0.107203\n",
      "====> Epoch: 261 total_train_loss: 0.709326 Total_test_loss: 0.681411 Total_BCE_test_loss: 0.570557 Total_KLD_test_loss: 0.003643 Total_CEP_test_loss: 0.107210\n",
      "====> Epoch: 262 total_train_loss: 0.709744 Total_test_loss: 0.680980 Total_BCE_test_loss: 0.570134 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107209\n",
      "====> Epoch: 263 total_train_loss: 0.708721 Total_test_loss: 0.681282 Total_BCE_test_loss: 0.570485 Total_KLD_test_loss: 0.003599 Total_CEP_test_loss: 0.107198\n",
      "====> Epoch: 264 total_train_loss: 0.710113 Total_test_loss: 0.680751 Total_BCE_test_loss: 0.569855 Total_KLD_test_loss: 0.003628 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 265 total_train_loss: 0.710283 Total_test_loss: 0.680360 Total_BCE_test_loss: 0.569433 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107221\n",
      "====> Epoch: 266 total_train_loss: 0.709336 Total_test_loss: 0.680865 Total_BCE_test_loss: 0.569992 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 267 total_train_loss: 0.710094 Total_test_loss: 0.681166 Total_BCE_test_loss: 0.570284 Total_KLD_test_loss: 0.003631 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 268 total_train_loss: 0.709780 Total_test_loss: 0.680985 Total_BCE_test_loss: 0.570087 Total_KLD_test_loss: 0.003664 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 269 total_train_loss: 0.710982 Total_test_loss: 0.680542 Total_BCE_test_loss: 0.569593 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 270 total_train_loss: 0.710388 Total_test_loss: 0.680322 Total_BCE_test_loss: 0.569335 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 271 total_train_loss: 0.709975 Total_test_loss: 0.680825 Total_BCE_test_loss: 0.569929 Total_KLD_test_loss: 0.003640 Total_CEP_test_loss: 0.107256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 272 total_train_loss: 0.709324 Total_test_loss: 0.681510 Total_BCE_test_loss: 0.570707 Total_KLD_test_loss: 0.003587 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 273 total_train_loss: 0.710016 Total_test_loss: 0.680661 Total_BCE_test_loss: 0.569863 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107202\n",
      "====> Epoch: 274 total_train_loss: 0.708673 Total_test_loss: 0.680101 Total_BCE_test_loss: 0.569248 Total_KLD_test_loss: 0.003616 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 275 total_train_loss: 0.710014 Total_test_loss: 0.680223 Total_BCE_test_loss: 0.569425 Total_KLD_test_loss: 0.003593 Total_CEP_test_loss: 0.107205\n",
      "====> Epoch: 276 total_train_loss: 0.709746 Total_test_loss: 0.680268 Total_BCE_test_loss: 0.569421 Total_KLD_test_loss: 0.003615 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 277 total_train_loss: 0.713572 Total_test_loss: 0.679747 Total_BCE_test_loss: 0.568876 Total_KLD_test_loss: 0.003641 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 278 total_train_loss: 0.709434 Total_test_loss: 0.680621 Total_BCE_test_loss: 0.569741 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 279 total_train_loss: 0.709519 Total_test_loss: 0.680729 Total_BCE_test_loss: 0.569876 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 280 total_train_loss: 0.712044 Total_test_loss: 0.679918 Total_BCE_test_loss: 0.569021 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107210\n",
      "====> Epoch: 281 total_train_loss: 0.708895 Total_test_loss: 0.680684 Total_BCE_test_loss: 0.569788 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107201\n",
      "====> Epoch: 282 total_train_loss: 0.708868 Total_test_loss: 0.680726 Total_BCE_test_loss: 0.569857 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107203\n",
      "====> Epoch: 283 total_train_loss: 0.711144 Total_test_loss: 0.681720 Total_BCE_test_loss: 0.570835 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107217\n",
      "====> Epoch: 284 total_train_loss: 0.712064 Total_test_loss: 0.681223 Total_BCE_test_loss: 0.570405 Total_KLD_test_loss: 0.003614 Total_CEP_test_loss: 0.107204\n",
      "====> Epoch: 285 total_train_loss: 0.710624 Total_test_loss: 0.680650 Total_BCE_test_loss: 0.569758 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107213\n",
      "====> Epoch: 286 total_train_loss: 0.710269 Total_test_loss: 0.680667 Total_BCE_test_loss: 0.569818 Total_KLD_test_loss: 0.003641 Total_CEP_test_loss: 0.107208\n",
      "====> Epoch: 287 total_train_loss: 0.708552 Total_test_loss: 0.680548 Total_BCE_test_loss: 0.569670 Total_KLD_test_loss: 0.003641 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 288 total_train_loss: 0.706952 Total_test_loss: 0.680260 Total_BCE_test_loss: 0.569382 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 289 total_train_loss: 0.708633 Total_test_loss: 0.680217 Total_BCE_test_loss: 0.569319 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107199\n",
      "====> Epoch: 290 total_train_loss: 0.711323 Total_test_loss: 0.679449 Total_BCE_test_loss: 0.568458 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 291 total_train_loss: 0.710868 Total_test_loss: 0.679389 Total_BCE_test_loss: 0.568421 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 292 total_train_loss: 0.709667 Total_test_loss: 0.680181 Total_BCE_test_loss: 0.569258 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 293 total_train_loss: 0.710905 Total_test_loss: 0.679443 Total_BCE_test_loss: 0.568501 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 294 total_train_loss: 0.709244 Total_test_loss: 0.679548 Total_BCE_test_loss: 0.568624 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 295 total_train_loss: 0.709723 Total_test_loss: 0.680934 Total_BCE_test_loss: 0.570046 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 296 total_train_loss: 0.709922 Total_test_loss: 0.680698 Total_BCE_test_loss: 0.569797 Total_KLD_test_loss: 0.003639 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 297 total_train_loss: 0.711622 Total_test_loss: 0.680505 Total_BCE_test_loss: 0.569554 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 298 total_train_loss: 0.710094 Total_test_loss: 0.680950 Total_BCE_test_loss: 0.570022 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 299 total_train_loss: 0.710536 Total_test_loss: 0.681351 Total_BCE_test_loss: 0.570506 Total_KLD_test_loss: 0.003603 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 300 total_train_loss: 0.708470 Total_test_loss: 0.680572 Total_BCE_test_loss: 0.569718 Total_KLD_test_loss: 0.003633 Total_CEP_test_loss: 0.107222\n",
      "====> Epoch: 301 total_train_loss: 0.710404 Total_test_loss: 0.680365 Total_BCE_test_loss: 0.569465 Total_KLD_test_loss: 0.003631 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 302 total_train_loss: 0.709943 Total_test_loss: 0.680265 Total_BCE_test_loss: 0.569373 Total_KLD_test_loss: 0.003645 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 303 total_train_loss: 0.708799 Total_test_loss: 0.680520 Total_BCE_test_loss: 0.569701 Total_KLD_test_loss: 0.003600 Total_CEP_test_loss: 0.107218\n",
      "====> Epoch: 304 total_train_loss: 0.710715 Total_test_loss: 0.679974 Total_BCE_test_loss: 0.569130 Total_KLD_test_loss: 0.003625 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 305 total_train_loss: 0.709162 Total_test_loss: 0.680032 Total_BCE_test_loss: 0.569146 Total_KLD_test_loss: 0.003632 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 306 total_train_loss: 0.708986 Total_test_loss: 0.679820 Total_BCE_test_loss: 0.568977 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107216\n",
      "====> Epoch: 307 total_train_loss: 0.709860 Total_test_loss: 0.679650 Total_BCE_test_loss: 0.568744 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 308 total_train_loss: 0.707308 Total_test_loss: 0.679987 Total_BCE_test_loss: 0.569105 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 309 total_train_loss: 0.709430 Total_test_loss: 0.679768 Total_BCE_test_loss: 0.568930 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 310 total_train_loss: 0.710902 Total_test_loss: 0.679286 Total_BCE_test_loss: 0.568371 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 311 total_train_loss: 0.708416 Total_test_loss: 0.679299 Total_BCE_test_loss: 0.568344 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 312 total_train_loss: 0.708550 Total_test_loss: 0.679999 Total_BCE_test_loss: 0.569118 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107223\n",
      "====> Epoch: 313 total_train_loss: 0.708739 Total_test_loss: 0.680424 Total_BCE_test_loss: 0.569531 Total_KLD_test_loss: 0.003623 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 314 total_train_loss: 0.706884 Total_test_loss: 0.680374 Total_BCE_test_loss: 0.569555 Total_KLD_test_loss: 0.003622 Total_CEP_test_loss: 0.107197\n",
      "====> Epoch: 315 total_train_loss: 0.708984 Total_test_loss: 0.679796 Total_BCE_test_loss: 0.568896 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 316 total_train_loss: 0.707420 Total_test_loss: 0.680105 Total_BCE_test_loss: 0.569184 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 317 total_train_loss: 0.708622 Total_test_loss: 0.680467 Total_BCE_test_loss: 0.569556 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 318 total_train_loss: 0.710754 Total_test_loss: 0.680265 Total_BCE_test_loss: 0.569407 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107196\n",
      "====> Epoch: 319 total_train_loss: 0.710258 Total_test_loss: 0.681138 Total_BCE_test_loss: 0.570328 Total_KLD_test_loss: 0.003595 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 320 total_train_loss: 0.709833 Total_test_loss: 0.680492 Total_BCE_test_loss: 0.569682 Total_KLD_test_loss: 0.003614 Total_CEP_test_loss: 0.107196\n",
      "====> Epoch: 321 total_train_loss: 0.708487 Total_test_loss: 0.679880 Total_BCE_test_loss: 0.568982 Total_KLD_test_loss: 0.003655 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 322 total_train_loss: 0.711703 Total_test_loss: 0.679086 Total_BCE_test_loss: 0.568186 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107220\n",
      "====> Epoch: 323 total_train_loss: 0.708827 Total_test_loss: 0.679685 Total_BCE_test_loss: 0.568889 Total_KLD_test_loss: 0.003581 Total_CEP_test_loss: 0.107215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 324 total_train_loss: 0.710783 Total_test_loss: 0.679364 Total_BCE_test_loss: 0.568520 Total_KLD_test_loss: 0.003631 Total_CEP_test_loss: 0.107212\n",
      "====> Epoch: 325 total_train_loss: 0.707403 Total_test_loss: 0.679501 Total_BCE_test_loss: 0.568638 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 326 total_train_loss: 0.707031 Total_test_loss: 0.680063 Total_BCE_test_loss: 0.569179 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 327 total_train_loss: 0.708464 Total_test_loss: 0.680034 Total_BCE_test_loss: 0.569170 Total_KLD_test_loss: 0.003634 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 328 total_train_loss: 0.708221 Total_test_loss: 0.679438 Total_BCE_test_loss: 0.568567 Total_KLD_test_loss: 0.003644 Total_CEP_test_loss: 0.107227\n",
      "====> Epoch: 329 total_train_loss: 0.708733 Total_test_loss: 0.679826 Total_BCE_test_loss: 0.569002 Total_KLD_test_loss: 0.003596 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 330 total_train_loss: 0.709988 Total_test_loss: 0.679281 Total_BCE_test_loss: 0.568358 Total_KLD_test_loss: 0.003652 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 331 total_train_loss: 0.708837 Total_test_loss: 0.678930 Total_BCE_test_loss: 0.568013 Total_KLD_test_loss: 0.003659 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 332 total_train_loss: 0.708924 Total_test_loss: 0.679870 Total_BCE_test_loss: 0.569002 Total_KLD_test_loss: 0.003633 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 333 total_train_loss: 0.709768 Total_test_loss: 0.679170 Total_BCE_test_loss: 0.568245 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 334 total_train_loss: 0.708242 Total_test_loss: 0.679316 Total_BCE_test_loss: 0.568441 Total_KLD_test_loss: 0.003635 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 335 total_train_loss: 0.709624 Total_test_loss: 0.679276 Total_BCE_test_loss: 0.568268 Total_KLD_test_loss: 0.003753 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 336 total_train_loss: 0.708769 Total_test_loss: 0.680501 Total_BCE_test_loss: 0.569645 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107202\n",
      "====> Epoch: 337 total_train_loss: 0.711381 Total_test_loss: 0.680706 Total_BCE_test_loss: 0.569844 Total_KLD_test_loss: 0.003611 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 338 total_train_loss: 0.709651 Total_test_loss: 0.679494 Total_BCE_test_loss: 0.568670 Total_KLD_test_loss: 0.003629 Total_CEP_test_loss: 0.107195\n",
      "====> Epoch: 339 total_train_loss: 0.709076 Total_test_loss: 0.679649 Total_BCE_test_loss: 0.568826 Total_KLD_test_loss: 0.003636 Total_CEP_test_loss: 0.107187\n",
      "====> Epoch: 340 total_train_loss: 0.709654 Total_test_loss: 0.679503 Total_BCE_test_loss: 0.568588 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 341 total_train_loss: 0.708328 Total_test_loss: 0.678845 Total_BCE_test_loss: 0.567951 Total_KLD_test_loss: 0.003645 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 342 total_train_loss: 0.709458 Total_test_loss: 0.678821 Total_BCE_test_loss: 0.567915 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 343 total_train_loss: 0.707242 Total_test_loss: 0.679810 Total_BCE_test_loss: 0.568933 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 344 total_train_loss: 0.711695 Total_test_loss: 0.679371 Total_BCE_test_loss: 0.568504 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107205\n",
      "====> Epoch: 345 total_train_loss: 0.706866 Total_test_loss: 0.679364 Total_BCE_test_loss: 0.568441 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 346 total_train_loss: 0.710863 Total_test_loss: 0.679599 Total_BCE_test_loss: 0.568712 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 347 total_train_loss: 0.708818 Total_test_loss: 0.679348 Total_BCE_test_loss: 0.568436 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 348 total_train_loss: 0.711269 Total_test_loss: 0.679522 Total_BCE_test_loss: 0.568611 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 349 total_train_loss: 0.708269 Total_test_loss: 0.679634 Total_BCE_test_loss: 0.568756 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 350 total_train_loss: 0.708522 Total_test_loss: 0.679743 Total_BCE_test_loss: 0.568799 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 351 total_train_loss: 0.711037 Total_test_loss: 0.679941 Total_BCE_test_loss: 0.569049 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107222\n",
      "====> Epoch: 352 total_train_loss: 0.707960 Total_test_loss: 0.680231 Total_BCE_test_loss: 0.569330 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 353 total_train_loss: 0.708580 Total_test_loss: 0.679856 Total_BCE_test_loss: 0.568962 Total_KLD_test_loss: 0.003651 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 354 total_train_loss: 0.708630 Total_test_loss: 0.680075 Total_BCE_test_loss: 0.569149 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 355 total_train_loss: 0.707720 Total_test_loss: 0.680502 Total_BCE_test_loss: 0.569612 Total_KLD_test_loss: 0.003655 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 356 total_train_loss: 0.709714 Total_test_loss: 0.679850 Total_BCE_test_loss: 0.568951 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 357 total_train_loss: 0.708740 Total_test_loss: 0.679849 Total_BCE_test_loss: 0.568921 Total_KLD_test_loss: 0.003643 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 358 total_train_loss: 0.710879 Total_test_loss: 0.679763 Total_BCE_test_loss: 0.568850 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 359 total_train_loss: 0.706880 Total_test_loss: 0.680285 Total_BCE_test_loss: 0.569325 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 360 total_train_loss: 0.708239 Total_test_loss: 0.680177 Total_BCE_test_loss: 0.569256 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 361 total_train_loss: 0.706569 Total_test_loss: 0.679279 Total_BCE_test_loss: 0.568373 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 362 total_train_loss: 0.710196 Total_test_loss: 0.679674 Total_BCE_test_loss: 0.568756 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 363 total_train_loss: 0.708868 Total_test_loss: 0.679426 Total_BCE_test_loss: 0.568529 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 364 total_train_loss: 0.707200 Total_test_loss: 0.679851 Total_BCE_test_loss: 0.569031 Total_KLD_test_loss: 0.003620 Total_CEP_test_loss: 0.107199\n",
      "====> Epoch: 365 total_train_loss: 0.709380 Total_test_loss: 0.679796 Total_BCE_test_loss: 0.568980 Total_KLD_test_loss: 0.003619 Total_CEP_test_loss: 0.107197\n",
      "====> Epoch: 366 total_train_loss: 0.708125 Total_test_loss: 0.679972 Total_BCE_test_loss: 0.569049 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 367 total_train_loss: 0.709378 Total_test_loss: 0.678776 Total_BCE_test_loss: 0.567766 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 368 total_train_loss: 0.708386 Total_test_loss: 0.679411 Total_BCE_test_loss: 0.568425 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 369 total_train_loss: 0.707409 Total_test_loss: 0.679532 Total_BCE_test_loss: 0.568611 Total_KLD_test_loss: 0.003641 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 370 total_train_loss: 0.709824 Total_test_loss: 0.678038 Total_BCE_test_loss: 0.567136 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107217\n",
      "====> Epoch: 371 total_train_loss: 0.706700 Total_test_loss: 0.678420 Total_BCE_test_loss: 0.567527 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 372 total_train_loss: 0.709291 Total_test_loss: 0.679252 Total_BCE_test_loss: 0.568387 Total_KLD_test_loss: 0.003591 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 373 total_train_loss: 0.706198 Total_test_loss: 0.679736 Total_BCE_test_loss: 0.568841 Total_KLD_test_loss: 0.003627 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 374 total_train_loss: 0.705835 Total_test_loss: 0.679540 Total_BCE_test_loss: 0.568632 Total_KLD_test_loss: 0.003621 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 375 total_train_loss: 0.706148 Total_test_loss: 0.679643 Total_BCE_test_loss: 0.568727 Total_KLD_test_loss: 0.003642 Total_CEP_test_loss: 0.107273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 376 total_train_loss: 0.706805 Total_test_loss: 0.679218 Total_BCE_test_loss: 0.568348 Total_KLD_test_loss: 0.003618 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 377 total_train_loss: 0.707584 Total_test_loss: 0.678778 Total_BCE_test_loss: 0.567888 Total_KLD_test_loss: 0.003624 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 378 total_train_loss: 0.708390 Total_test_loss: 0.679384 Total_BCE_test_loss: 0.568477 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 379 total_train_loss: 0.708266 Total_test_loss: 0.679422 Total_BCE_test_loss: 0.568529 Total_KLD_test_loss: 0.003621 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 380 total_train_loss: 0.709953 Total_test_loss: 0.679292 Total_BCE_test_loss: 0.568400 Total_KLD_test_loss: 0.003608 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 381 total_train_loss: 0.707150 Total_test_loss: 0.678849 Total_BCE_test_loss: 0.567942 Total_KLD_test_loss: 0.003639 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 382 total_train_loss: 0.707806 Total_test_loss: 0.678108 Total_BCE_test_loss: 0.567116 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 383 total_train_loss: 0.707653 Total_test_loss: 0.678558 Total_BCE_test_loss: 0.567674 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107200\n",
      "====> Epoch: 384 total_train_loss: 0.706146 Total_test_loss: 0.678836 Total_BCE_test_loss: 0.567915 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107227\n",
      "====> Epoch: 385 total_train_loss: 0.710041 Total_test_loss: 0.679028 Total_BCE_test_loss: 0.568092 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 386 total_train_loss: 0.708086 Total_test_loss: 0.680020 Total_BCE_test_loss: 0.569117 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 387 total_train_loss: 0.709709 Total_test_loss: 0.679457 Total_BCE_test_loss: 0.568559 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107199\n",
      "====> Epoch: 388 total_train_loss: 0.711847 Total_test_loss: 0.679029 Total_BCE_test_loss: 0.568169 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107201\n",
      "====> Epoch: 389 total_train_loss: 0.706246 Total_test_loss: 0.679074 Total_BCE_test_loss: 0.568257 Total_KLD_test_loss: 0.003617 Total_CEP_test_loss: 0.107200\n",
      "====> Epoch: 390 total_train_loss: 0.709551 Total_test_loss: 0.678949 Total_BCE_test_loss: 0.568046 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 391 total_train_loss: 0.708676 Total_test_loss: 0.678360 Total_BCE_test_loss: 0.567448 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 392 total_train_loss: 0.709445 Total_test_loss: 0.678585 Total_BCE_test_loss: 0.567611 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 393 total_train_loss: 0.711876 Total_test_loss: 0.679085 Total_BCE_test_loss: 0.568154 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 394 total_train_loss: 0.706880 Total_test_loss: 0.679264 Total_BCE_test_loss: 0.568297 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 395 total_train_loss: 0.709270 Total_test_loss: 0.679323 Total_BCE_test_loss: 0.568354 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 396 total_train_loss: 0.707151 Total_test_loss: 0.679411 Total_BCE_test_loss: 0.568437 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 397 total_train_loss: 0.708744 Total_test_loss: 0.679407 Total_BCE_test_loss: 0.568437 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 398 total_train_loss: 0.709191 Total_test_loss: 0.678990 Total_BCE_test_loss: 0.568003 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 399 total_train_loss: 0.710555 Total_test_loss: 0.678755 Total_BCE_test_loss: 0.567775 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 400 total_train_loss: 0.708046 Total_test_loss: 0.677902 Total_BCE_test_loss: 0.566895 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107300\n",
      "====> Epoch: 401 total_train_loss: 0.708679 Total_test_loss: 0.678115 Total_BCE_test_loss: 0.567115 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 402 total_train_loss: 0.709358 Total_test_loss: 0.678211 Total_BCE_test_loss: 0.567203 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 403 total_train_loss: 0.708141 Total_test_loss: 0.678600 Total_BCE_test_loss: 0.567588 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 404 total_train_loss: 0.708365 Total_test_loss: 0.678642 Total_BCE_test_loss: 0.567672 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 405 total_train_loss: 0.710545 Total_test_loss: 0.678373 Total_BCE_test_loss: 0.567368 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 406 total_train_loss: 0.706672 Total_test_loss: 0.677838 Total_BCE_test_loss: 0.566795 Total_KLD_test_loss: 0.003759 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 407 total_train_loss: 0.707162 Total_test_loss: 0.678520 Total_BCE_test_loss: 0.567500 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 408 total_train_loss: 0.707824 Total_test_loss: 0.679439 Total_BCE_test_loss: 0.568487 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 409 total_train_loss: 0.709792 Total_test_loss: 0.678481 Total_BCE_test_loss: 0.567527 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 410 total_train_loss: 0.708831 Total_test_loss: 0.678840 Total_BCE_test_loss: 0.567893 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 411 total_train_loss: 0.709878 Total_test_loss: 0.678536 Total_BCE_test_loss: 0.567610 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 412 total_train_loss: 0.708181 Total_test_loss: 0.678471 Total_BCE_test_loss: 0.567516 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 413 total_train_loss: 0.708221 Total_test_loss: 0.678369 Total_BCE_test_loss: 0.567412 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 414 total_train_loss: 0.706085 Total_test_loss: 0.678317 Total_BCE_test_loss: 0.567318 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107308\n",
      "====> Epoch: 415 total_train_loss: 0.708285 Total_test_loss: 0.677519 Total_BCE_test_loss: 0.566513 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 416 total_train_loss: 0.708805 Total_test_loss: 0.678208 Total_BCE_test_loss: 0.567203 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107310\n",
      "====> Epoch: 417 total_train_loss: 0.706620 Total_test_loss: 0.678325 Total_BCE_test_loss: 0.567317 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107322\n",
      "====> Epoch: 418 total_train_loss: 0.707945 Total_test_loss: 0.678352 Total_BCE_test_loss: 0.567373 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107293\n",
      "====> Epoch: 419 total_train_loss: 0.709263 Total_test_loss: 0.677759 Total_BCE_test_loss: 0.566733 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107304\n",
      "====> Epoch: 420 total_train_loss: 0.707535 Total_test_loss: 0.678739 Total_BCE_test_loss: 0.567766 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 421 total_train_loss: 0.707216 Total_test_loss: 0.678621 Total_BCE_test_loss: 0.567639 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 422 total_train_loss: 0.711434 Total_test_loss: 0.677573 Total_BCE_test_loss: 0.566580 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107300\n",
      "====> Epoch: 423 total_train_loss: 0.708842 Total_test_loss: 0.678185 Total_BCE_test_loss: 0.567233 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 424 total_train_loss: 0.708819 Total_test_loss: 0.677920 Total_BCE_test_loss: 0.566942 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 425 total_train_loss: 0.707349 Total_test_loss: 0.678855 Total_BCE_test_loss: 0.567914 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 426 total_train_loss: 0.708806 Total_test_loss: 0.678363 Total_BCE_test_loss: 0.567424 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 427 total_train_loss: 0.708283 Total_test_loss: 0.678521 Total_BCE_test_loss: 0.567611 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 428 total_train_loss: 0.707005 Total_test_loss: 0.679393 Total_BCE_test_loss: 0.568549 Total_KLD_test_loss: 0.003620 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 429 total_train_loss: 0.707811 Total_test_loss: 0.678378 Total_BCE_test_loss: 0.567465 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 430 total_train_loss: 0.707453 Total_test_loss: 0.678423 Total_BCE_test_loss: 0.567549 Total_KLD_test_loss: 0.003628 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 431 total_train_loss: 0.708156 Total_test_loss: 0.678575 Total_BCE_test_loss: 0.567656 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 432 total_train_loss: 0.708465 Total_test_loss: 0.678284 Total_BCE_test_loss: 0.567356 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 433 total_train_loss: 0.709395 Total_test_loss: 0.678020 Total_BCE_test_loss: 0.567090 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 434 total_train_loss: 0.708520 Total_test_loss: 0.678275 Total_BCE_test_loss: 0.567326 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 435 total_train_loss: 0.707478 Total_test_loss: 0.677747 Total_BCE_test_loss: 0.566707 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 436 total_train_loss: 0.709809 Total_test_loss: 0.677808 Total_BCE_test_loss: 0.566834 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 437 total_train_loss: 0.708470 Total_test_loss: 0.678706 Total_BCE_test_loss: 0.567725 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 438 total_train_loss: 0.707600 Total_test_loss: 0.678690 Total_BCE_test_loss: 0.567715 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 439 total_train_loss: 0.707689 Total_test_loss: 0.678373 Total_BCE_test_loss: 0.567397 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 440 total_train_loss: 0.708422 Total_test_loss: 0.677832 Total_BCE_test_loss: 0.566822 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 441 total_train_loss: 0.707293 Total_test_loss: 0.677953 Total_BCE_test_loss: 0.566997 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 442 total_train_loss: 0.706517 Total_test_loss: 0.678048 Total_BCE_test_loss: 0.567070 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 443 total_train_loss: 0.708736 Total_test_loss: 0.678389 Total_BCE_test_loss: 0.567460 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107212\n",
      "====> Epoch: 444 total_train_loss: 0.706643 Total_test_loss: 0.678229 Total_BCE_test_loss: 0.567316 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 445 total_train_loss: 0.707883 Total_test_loss: 0.678459 Total_BCE_test_loss: 0.567454 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 446 total_train_loss: 0.707505 Total_test_loss: 0.678260 Total_BCE_test_loss: 0.567287 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 447 total_train_loss: 0.707257 Total_test_loss: 0.677866 Total_BCE_test_loss: 0.566891 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 448 total_train_loss: 0.708288 Total_test_loss: 0.678067 Total_BCE_test_loss: 0.567129 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 449 total_train_loss: 0.707728 Total_test_loss: 0.677311 Total_BCE_test_loss: 0.566329 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107234\n",
      "====> Epoch: 450 total_train_loss: 0.708156 Total_test_loss: 0.677532 Total_BCE_test_loss: 0.566600 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 451 total_train_loss: 0.706488 Total_test_loss: 0.678186 Total_BCE_test_loss: 0.567257 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 452 total_train_loss: 0.710313 Total_test_loss: 0.677350 Total_BCE_test_loss: 0.566436 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107223\n",
      "====> Epoch: 453 total_train_loss: 0.707979 Total_test_loss: 0.677478 Total_BCE_test_loss: 0.566587 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 454 total_train_loss: 0.706991 Total_test_loss: 0.677989 Total_BCE_test_loss: 0.567054 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 455 total_train_loss: 0.705733 Total_test_loss: 0.677218 Total_BCE_test_loss: 0.566216 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 456 total_train_loss: 0.709686 Total_test_loss: 0.677510 Total_BCE_test_loss: 0.566527 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 457 total_train_loss: 0.708497 Total_test_loss: 0.678221 Total_BCE_test_loss: 0.567271 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 458 total_train_loss: 0.706675 Total_test_loss: 0.678341 Total_BCE_test_loss: 0.567354 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 459 total_train_loss: 0.708698 Total_test_loss: 0.677350 Total_BCE_test_loss: 0.566386 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 460 total_train_loss: 0.707987 Total_test_loss: 0.676902 Total_BCE_test_loss: 0.565938 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 461 total_train_loss: 0.706958 Total_test_loss: 0.677591 Total_BCE_test_loss: 0.566643 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 462 total_train_loss: 0.708418 Total_test_loss: 0.677190 Total_BCE_test_loss: 0.566308 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107222\n",
      "====> Epoch: 463 total_train_loss: 0.706797 Total_test_loss: 0.677335 Total_BCE_test_loss: 0.566448 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 464 total_train_loss: 0.709251 Total_test_loss: 0.677871 Total_BCE_test_loss: 0.566925 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 465 total_train_loss: 0.708075 Total_test_loss: 0.677624 Total_BCE_test_loss: 0.566670 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 466 total_train_loss: 0.708879 Total_test_loss: 0.677976 Total_BCE_test_loss: 0.567028 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 467 total_train_loss: 0.707748 Total_test_loss: 0.677031 Total_BCE_test_loss: 0.566064 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107222\n",
      "====> Epoch: 468 total_train_loss: 0.706468 Total_test_loss: 0.677124 Total_BCE_test_loss: 0.566174 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 469 total_train_loss: 0.705972 Total_test_loss: 0.677264 Total_BCE_test_loss: 0.566309 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 470 total_train_loss: 0.707626 Total_test_loss: 0.677690 Total_BCE_test_loss: 0.566723 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 471 total_train_loss: 0.705961 Total_test_loss: 0.677382 Total_BCE_test_loss: 0.566380 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 472 total_train_loss: 0.708661 Total_test_loss: 0.677294 Total_BCE_test_loss: 0.566289 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 473 total_train_loss: 0.710199 Total_test_loss: 0.677270 Total_BCE_test_loss: 0.566288 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 474 total_train_loss: 0.707646 Total_test_loss: 0.677386 Total_BCE_test_loss: 0.566402 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 475 total_train_loss: 0.705816 Total_test_loss: 0.677079 Total_BCE_test_loss: 0.566003 Total_KLD_test_loss: 0.003770 Total_CEP_test_loss: 0.107306\n",
      "====> Epoch: 476 total_train_loss: 0.707978 Total_test_loss: 0.677545 Total_BCE_test_loss: 0.566537 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 477 total_train_loss: 0.707513 Total_test_loss: 0.677272 Total_BCE_test_loss: 0.566182 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107334\n",
      "====> Epoch: 478 total_train_loss: 0.705663 Total_test_loss: 0.677662 Total_BCE_test_loss: 0.566671 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 479 total_train_loss: 0.706240 Total_test_loss: 0.677385 Total_BCE_test_loss: 0.566363 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 480 total_train_loss: 0.706849 Total_test_loss: 0.677568 Total_BCE_test_loss: 0.566596 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107303\n",
      "====> Epoch: 481 total_train_loss: 0.709298 Total_test_loss: 0.677240 Total_BCE_test_loss: 0.566276 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 482 total_train_loss: 0.708365 Total_test_loss: 0.677440 Total_BCE_test_loss: 0.566492 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 483 total_train_loss: 0.706186 Total_test_loss: 0.677350 Total_BCE_test_loss: 0.566402 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 484 total_train_loss: 0.709106 Total_test_loss: 0.677230 Total_BCE_test_loss: 0.566249 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 485 total_train_loss: 0.707324 Total_test_loss: 0.677070 Total_BCE_test_loss: 0.566129 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 486 total_train_loss: 0.705815 Total_test_loss: 0.676753 Total_BCE_test_loss: 0.565772 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 487 total_train_loss: 0.708312 Total_test_loss: 0.677942 Total_BCE_test_loss: 0.567012 Total_KLD_test_loss: 0.003656 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 488 total_train_loss: 0.708495 Total_test_loss: 0.676845 Total_BCE_test_loss: 0.565904 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 489 total_train_loss: 0.706928 Total_test_loss: 0.676771 Total_BCE_test_loss: 0.565768 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 490 total_train_loss: 0.709008 Total_test_loss: 0.676512 Total_BCE_test_loss: 0.565514 Total_KLD_test_loss: 0.003725 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 491 total_train_loss: 0.708424 Total_test_loss: 0.676755 Total_BCE_test_loss: 0.565799 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 492 total_train_loss: 0.709211 Total_test_loss: 0.676197 Total_BCE_test_loss: 0.565146 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107300\n",
      "====> Epoch: 493 total_train_loss: 0.706400 Total_test_loss: 0.676681 Total_BCE_test_loss: 0.565689 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 494 total_train_loss: 0.706420 Total_test_loss: 0.676847 Total_BCE_test_loss: 0.565880 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 495 total_train_loss: 0.706592 Total_test_loss: 0.676906 Total_BCE_test_loss: 0.565988 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 496 total_train_loss: 0.706607 Total_test_loss: 0.677663 Total_BCE_test_loss: 0.566686 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107306\n",
      "====> Epoch: 497 total_train_loss: 0.706403 Total_test_loss: 0.677842 Total_BCE_test_loss: 0.566850 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 498 total_train_loss: 0.707705 Total_test_loss: 0.677616 Total_BCE_test_loss: 0.566657 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 499 total_train_loss: 0.706923 Total_test_loss: 0.677612 Total_BCE_test_loss: 0.566625 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 500 total_train_loss: 0.707208 Total_test_loss: 0.676708 Total_BCE_test_loss: 0.565729 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107249\n",
      "1e-05\n",
      "====> Epoch: 1 total_train_loss: 0.707158 Total_test_loss: 0.676782 Total_BCE_test_loss: 0.565783 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 2 total_train_loss: 0.705505 Total_test_loss: 0.677158 Total_BCE_test_loss: 0.566157 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 3 total_train_loss: 0.707233 Total_test_loss: 0.677188 Total_BCE_test_loss: 0.566194 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107293\n",
      "====> Epoch: 4 total_train_loss: 0.706769 Total_test_loss: 0.676496 Total_BCE_test_loss: 0.565552 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 5 total_train_loss: 0.706891 Total_test_loss: 0.676496 Total_BCE_test_loss: 0.565500 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 6 total_train_loss: 0.705792 Total_test_loss: 0.676924 Total_BCE_test_loss: 0.565962 Total_KLD_test_loss: 0.003719 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 7 total_train_loss: 0.704732 Total_test_loss: 0.676650 Total_BCE_test_loss: 0.565676 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 8 total_train_loss: 0.708076 Total_test_loss: 0.676045 Total_BCE_test_loss: 0.565071 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 9 total_train_loss: 0.705789 Total_test_loss: 0.676554 Total_BCE_test_loss: 0.565595 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 10 total_train_loss: 0.706101 Total_test_loss: 0.676598 Total_BCE_test_loss: 0.565621 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 11 total_train_loss: 0.707680 Total_test_loss: 0.676358 Total_BCE_test_loss: 0.565373 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 12 total_train_loss: 0.707592 Total_test_loss: 0.676392 Total_BCE_test_loss: 0.565448 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107214\n",
      "====> Epoch: 13 total_train_loss: 0.706049 Total_test_loss: 0.676581 Total_BCE_test_loss: 0.565592 Total_KLD_test_loss: 0.003743 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 14 total_train_loss: 0.706378 Total_test_loss: 0.676592 Total_BCE_test_loss: 0.565562 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107299\n",
      "====> Epoch: 15 total_train_loss: 0.704779 Total_test_loss: 0.677346 Total_BCE_test_loss: 0.566390 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 16 total_train_loss: 0.707334 Total_test_loss: 0.676970 Total_BCE_test_loss: 0.565966 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 17 total_train_loss: 0.706563 Total_test_loss: 0.676733 Total_BCE_test_loss: 0.565767 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 18 total_train_loss: 0.708751 Total_test_loss: 0.676526 Total_BCE_test_loss: 0.565528 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 19 total_train_loss: 0.706543 Total_test_loss: 0.675904 Total_BCE_test_loss: 0.564918 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 20 total_train_loss: 0.705913 Total_test_loss: 0.675888 Total_BCE_test_loss: 0.564925 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 21 total_train_loss: 0.709677 Total_test_loss: 0.676023 Total_BCE_test_loss: 0.564992 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 22 total_train_loss: 0.706475 Total_test_loss: 0.676288 Total_BCE_test_loss: 0.565205 Total_KLD_test_loss: 0.003787 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 23 total_train_loss: 0.707394 Total_test_loss: 0.676342 Total_BCE_test_loss: 0.565299 Total_KLD_test_loss: 0.003781 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 24 total_train_loss: 0.706999 Total_test_loss: 0.676338 Total_BCE_test_loss: 0.565436 Total_KLD_test_loss: 0.003659 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 25 total_train_loss: 0.705568 Total_test_loss: 0.676871 Total_BCE_test_loss: 0.565897 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 26 total_train_loss: 0.704511 Total_test_loss: 0.677080 Total_BCE_test_loss: 0.566141 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 27 total_train_loss: 0.709248 Total_test_loss: 0.676907 Total_BCE_test_loss: 0.565958 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 28 total_train_loss: 0.705839 Total_test_loss: 0.676449 Total_BCE_test_loss: 0.565446 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 29 total_train_loss: 0.706292 Total_test_loss: 0.676206 Total_BCE_test_loss: 0.565191 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 30 total_train_loss: 0.707202 Total_test_loss: 0.677142 Total_BCE_test_loss: 0.566172 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 31 total_train_loss: 0.706269 Total_test_loss: 0.676788 Total_BCE_test_loss: 0.565759 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 32 total_train_loss: 0.708414 Total_test_loss: 0.676853 Total_BCE_test_loss: 0.565858 Total_KLD_test_loss: 0.003738 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 33 total_train_loss: 0.705732 Total_test_loss: 0.676899 Total_BCE_test_loss: 0.565959 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 34 total_train_loss: 0.708550 Total_test_loss: 0.676702 Total_BCE_test_loss: 0.565717 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107301\n",
      "====> Epoch: 35 total_train_loss: 0.706235 Total_test_loss: 0.676697 Total_BCE_test_loss: 0.565725 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 36 total_train_loss: 0.707367 Total_test_loss: 0.676634 Total_BCE_test_loss: 0.565656 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107237\n",
      "====> Epoch: 37 total_train_loss: 0.706292 Total_test_loss: 0.677095 Total_BCE_test_loss: 0.566146 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 38 total_train_loss: 0.706154 Total_test_loss: 0.676932 Total_BCE_test_loss: 0.565926 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 39 total_train_loss: 0.704847 Total_test_loss: 0.676739 Total_BCE_test_loss: 0.565753 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 40 total_train_loss: 0.705521 Total_test_loss: 0.676794 Total_BCE_test_loss: 0.565777 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 41 total_train_loss: 0.706335 Total_test_loss: 0.676817 Total_BCE_test_loss: 0.565837 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 42 total_train_loss: 0.707305 Total_test_loss: 0.676123 Total_BCE_test_loss: 0.565145 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 43 total_train_loss: 0.706271 Total_test_loss: 0.676185 Total_BCE_test_loss: 0.565105 Total_KLD_test_loss: 0.003792 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 44 total_train_loss: 0.705123 Total_test_loss: 0.676554 Total_BCE_test_loss: 0.565484 Total_KLD_test_loss: 0.003798 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 45 total_train_loss: 0.707452 Total_test_loss: 0.677027 Total_BCE_test_loss: 0.566040 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 46 total_train_loss: 0.707432 Total_test_loss: 0.676902 Total_BCE_test_loss: 0.565927 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 47 total_train_loss: 0.708415 Total_test_loss: 0.676261 Total_BCE_test_loss: 0.565192 Total_KLD_test_loss: 0.003776 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 48 total_train_loss: 0.707259 Total_test_loss: 0.676053 Total_BCE_test_loss: 0.565028 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 49 total_train_loss: 0.706392 Total_test_loss: 0.676629 Total_BCE_test_loss: 0.565635 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 50 total_train_loss: 0.706197 Total_test_loss: 0.676864 Total_BCE_test_loss: 0.565910 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 51 total_train_loss: 0.708804 Total_test_loss: 0.676169 Total_BCE_test_loss: 0.565134 Total_KLD_test_loss: 0.003772 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 52 total_train_loss: 0.708035 Total_test_loss: 0.676144 Total_BCE_test_loss: 0.565168 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 53 total_train_loss: 0.709081 Total_test_loss: 0.676708 Total_BCE_test_loss: 0.565723 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 54 total_train_loss: 0.707182 Total_test_loss: 0.676787 Total_BCE_test_loss: 0.565827 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 55 total_train_loss: 0.708355 Total_test_loss: 0.677097 Total_BCE_test_loss: 0.566177 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 56 total_train_loss: 0.707083 Total_test_loss: 0.676323 Total_BCE_test_loss: 0.565386 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107206\n",
      "====> Epoch: 57 total_train_loss: 0.705679 Total_test_loss: 0.676464 Total_BCE_test_loss: 0.565488 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 58 total_train_loss: 0.706218 Total_test_loss: 0.676483 Total_BCE_test_loss: 0.565523 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 59 total_train_loss: 0.707541 Total_test_loss: 0.676443 Total_BCE_test_loss: 0.565459 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 60 total_train_loss: 0.707841 Total_test_loss: 0.676929 Total_BCE_test_loss: 0.565957 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 61 total_train_loss: 0.709622 Total_test_loss: 0.676398 Total_BCE_test_loss: 0.565418 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 62 total_train_loss: 0.705297 Total_test_loss: 0.675965 Total_BCE_test_loss: 0.565034 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 63 total_train_loss: 0.707078 Total_test_loss: 0.676353 Total_BCE_test_loss: 0.565338 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 64 total_train_loss: 0.707724 Total_test_loss: 0.676050 Total_BCE_test_loss: 0.565061 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 65 total_train_loss: 0.705860 Total_test_loss: 0.676414 Total_BCE_test_loss: 0.565402 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 66 total_train_loss: 0.705787 Total_test_loss: 0.676722 Total_BCE_test_loss: 0.565698 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107295\n",
      "====> Epoch: 67 total_train_loss: 0.705110 Total_test_loss: 0.676622 Total_BCE_test_loss: 0.565650 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 68 total_train_loss: 0.706750 Total_test_loss: 0.676650 Total_BCE_test_loss: 0.565676 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 69 total_train_loss: 0.705755 Total_test_loss: 0.676119 Total_BCE_test_loss: 0.565099 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 70 total_train_loss: 0.704212 Total_test_loss: 0.677011 Total_BCE_test_loss: 0.566086 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 71 total_train_loss: 0.708226 Total_test_loss: 0.676730 Total_BCE_test_loss: 0.565841 Total_KLD_test_loss: 0.003643 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 72 total_train_loss: 0.708659 Total_test_loss: 0.676627 Total_BCE_test_loss: 0.565654 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 73 total_train_loss: 0.707852 Total_test_loss: 0.676160 Total_BCE_test_loss: 0.565125 Total_KLD_test_loss: 0.003772 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 74 total_train_loss: 0.707940 Total_test_loss: 0.677130 Total_BCE_test_loss: 0.566137 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 75 total_train_loss: 0.708173 Total_test_loss: 0.677649 Total_BCE_test_loss: 0.566696 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 76 total_train_loss: 0.705763 Total_test_loss: 0.676667 Total_BCE_test_loss: 0.565630 Total_KLD_test_loss: 0.003771 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 77 total_train_loss: 0.704497 Total_test_loss: 0.676066 Total_BCE_test_loss: 0.565022 Total_KLD_test_loss: 0.003812 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 78 total_train_loss: 0.707836 Total_test_loss: 0.676950 Total_BCE_test_loss: 0.565943 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 79 total_train_loss: 0.706737 Total_test_loss: 0.676708 Total_BCE_test_loss: 0.565714 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 80 total_train_loss: 0.706687 Total_test_loss: 0.676661 Total_BCE_test_loss: 0.565664 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 81 total_train_loss: 0.706922 Total_test_loss: 0.676604 Total_BCE_test_loss: 0.565534 Total_KLD_test_loss: 0.003767 Total_CEP_test_loss: 0.107304\n",
      "====> Epoch: 82 total_train_loss: 0.704906 Total_test_loss: 0.676668 Total_BCE_test_loss: 0.565657 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 83 total_train_loss: 0.705430 Total_test_loss: 0.676181 Total_BCE_test_loss: 0.565201 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 84 total_train_loss: 0.706472 Total_test_loss: 0.676728 Total_BCE_test_loss: 0.565784 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 85 total_train_loss: 0.708751 Total_test_loss: 0.676721 Total_BCE_test_loss: 0.565720 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 86 total_train_loss: 0.706407 Total_test_loss: 0.676854 Total_BCE_test_loss: 0.565872 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 87 total_train_loss: 0.706856 Total_test_loss: 0.677175 Total_BCE_test_loss: 0.566214 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 88 total_train_loss: 0.705940 Total_test_loss: 0.677029 Total_BCE_test_loss: 0.566098 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 89 total_train_loss: 0.706832 Total_test_loss: 0.676294 Total_BCE_test_loss: 0.565341 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 90 total_train_loss: 0.706042 Total_test_loss: 0.676602 Total_BCE_test_loss: 0.565668 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 91 total_train_loss: 0.705785 Total_test_loss: 0.677589 Total_BCE_test_loss: 0.566684 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 92 total_train_loss: 0.707383 Total_test_loss: 0.677166 Total_BCE_test_loss: 0.566211 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 93 total_train_loss: 0.705469 Total_test_loss: 0.676908 Total_BCE_test_loss: 0.565935 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 94 total_train_loss: 0.709078 Total_test_loss: 0.677246 Total_BCE_test_loss: 0.566308 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 95 total_train_loss: 0.706145 Total_test_loss: 0.677182 Total_BCE_test_loss: 0.566235 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 96 total_train_loss: 0.707828 Total_test_loss: 0.677125 Total_BCE_test_loss: 0.566198 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 97 total_train_loss: 0.706627 Total_test_loss: 0.676557 Total_BCE_test_loss: 0.565633 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 98 total_train_loss: 0.706062 Total_test_loss: 0.677297 Total_BCE_test_loss: 0.566328 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 99 total_train_loss: 0.708114 Total_test_loss: 0.676413 Total_BCE_test_loss: 0.565440 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 100 total_train_loss: 0.706989 Total_test_loss: 0.677074 Total_BCE_test_loss: 0.566159 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 101 total_train_loss: 0.708604 Total_test_loss: 0.676689 Total_BCE_test_loss: 0.565722 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 102 total_train_loss: 0.705801 Total_test_loss: 0.677260 Total_BCE_test_loss: 0.566332 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 103 total_train_loss: 0.706346 Total_test_loss: 0.677373 Total_BCE_test_loss: 0.566406 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 104 total_train_loss: 0.706724 Total_test_loss: 0.677610 Total_BCE_test_loss: 0.566715 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 105 total_train_loss: 0.706124 Total_test_loss: 0.676817 Total_BCE_test_loss: 0.565914 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 106 total_train_loss: 0.706361 Total_test_loss: 0.676466 Total_BCE_test_loss: 0.565474 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 107 total_train_loss: 0.708430 Total_test_loss: 0.676548 Total_BCE_test_loss: 0.565555 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 108 total_train_loss: 0.706499 Total_test_loss: 0.676477 Total_BCE_test_loss: 0.565494 Total_KLD_test_loss: 0.003734 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 109 total_train_loss: 0.706944 Total_test_loss: 0.676916 Total_BCE_test_loss: 0.565928 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 110 total_train_loss: 0.704099 Total_test_loss: 0.676452 Total_BCE_test_loss: 0.565437 Total_KLD_test_loss: 0.003766 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 111 total_train_loss: 0.707691 Total_test_loss: 0.676628 Total_BCE_test_loss: 0.565650 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 112 total_train_loss: 0.707749 Total_test_loss: 0.676205 Total_BCE_test_loss: 0.565124 Total_KLD_test_loss: 0.003801 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 113 total_train_loss: 0.706507 Total_test_loss: 0.676587 Total_BCE_test_loss: 0.565549 Total_KLD_test_loss: 0.003776 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 114 total_train_loss: 0.704240 Total_test_loss: 0.676797 Total_BCE_test_loss: 0.565842 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 115 total_train_loss: 0.705755 Total_test_loss: 0.677604 Total_BCE_test_loss: 0.566677 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 116 total_train_loss: 0.705086 Total_test_loss: 0.676764 Total_BCE_test_loss: 0.565860 Total_KLD_test_loss: 0.003647 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 117 total_train_loss: 0.706867 Total_test_loss: 0.677212 Total_BCE_test_loss: 0.566344 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 118 total_train_loss: 0.705184 Total_test_loss: 0.677328 Total_BCE_test_loss: 0.566437 Total_KLD_test_loss: 0.003630 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 119 total_train_loss: 0.706563 Total_test_loss: 0.677210 Total_BCE_test_loss: 0.566276 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 120 total_train_loss: 0.707601 Total_test_loss: 0.676372 Total_BCE_test_loss: 0.565401 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 121 total_train_loss: 0.707714 Total_test_loss: 0.676067 Total_BCE_test_loss: 0.565044 Total_KLD_test_loss: 0.003753 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 122 total_train_loss: 0.707299 Total_test_loss: 0.676540 Total_BCE_test_loss: 0.565552 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 123 total_train_loss: 0.708084 Total_test_loss: 0.676180 Total_BCE_test_loss: 0.565193 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 124 total_train_loss: 0.705911 Total_test_loss: 0.676955 Total_BCE_test_loss: 0.566005 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 125 total_train_loss: 0.706579 Total_test_loss: 0.677039 Total_BCE_test_loss: 0.566140 Total_KLD_test_loss: 0.003651 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 126 total_train_loss: 0.706093 Total_test_loss: 0.676798 Total_BCE_test_loss: 0.565873 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 127 total_train_loss: 0.707391 Total_test_loss: 0.676771 Total_BCE_test_loss: 0.565789 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 128 total_train_loss: 0.705602 Total_test_loss: 0.676661 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 129 total_train_loss: 0.707686 Total_test_loss: 0.676776 Total_BCE_test_loss: 0.565809 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 130 total_train_loss: 0.707527 Total_test_loss: 0.676934 Total_BCE_test_loss: 0.565944 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 131 total_train_loss: 0.708095 Total_test_loss: 0.676393 Total_BCE_test_loss: 0.565346 Total_KLD_test_loss: 0.003768 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 132 total_train_loss: 0.704793 Total_test_loss: 0.676271 Total_BCE_test_loss: 0.565232 Total_KLD_test_loss: 0.003756 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 133 total_train_loss: 0.706310 Total_test_loss: 0.676473 Total_BCE_test_loss: 0.565470 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 134 total_train_loss: 0.706210 Total_test_loss: 0.676434 Total_BCE_test_loss: 0.565454 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 135 total_train_loss: 0.703765 Total_test_loss: 0.676171 Total_BCE_test_loss: 0.565192 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 136 total_train_loss: 0.706538 Total_test_loss: 0.676393 Total_BCE_test_loss: 0.565446 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 137 total_train_loss: 0.709257 Total_test_loss: 0.676285 Total_BCE_test_loss: 0.565338 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 138 total_train_loss: 0.706647 Total_test_loss: 0.676247 Total_BCE_test_loss: 0.565281 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 139 total_train_loss: 0.706932 Total_test_loss: 0.676393 Total_BCE_test_loss: 0.565370 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 140 total_train_loss: 0.704993 Total_test_loss: 0.676308 Total_BCE_test_loss: 0.565322 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 141 total_train_loss: 0.706181 Total_test_loss: 0.676617 Total_BCE_test_loss: 0.565615 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 142 total_train_loss: 0.705504 Total_test_loss: 0.677010 Total_BCE_test_loss: 0.565983 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107295\n",
      "====> Epoch: 143 total_train_loss: 0.707131 Total_test_loss: 0.677013 Total_BCE_test_loss: 0.566049 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 144 total_train_loss: 0.706387 Total_test_loss: 0.676893 Total_BCE_test_loss: 0.565952 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 145 total_train_loss: 0.708128 Total_test_loss: 0.676716 Total_BCE_test_loss: 0.565728 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 146 total_train_loss: 0.706029 Total_test_loss: 0.676834 Total_BCE_test_loss: 0.565858 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 147 total_train_loss: 0.707660 Total_test_loss: 0.676957 Total_BCE_test_loss: 0.565977 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 148 total_train_loss: 0.705593 Total_test_loss: 0.677521 Total_BCE_test_loss: 0.566583 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 149 total_train_loss: 0.706166 Total_test_loss: 0.676957 Total_BCE_test_loss: 0.566031 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 150 total_train_loss: 0.707602 Total_test_loss: 0.676643 Total_BCE_test_loss: 0.565716 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 151 total_train_loss: 0.706205 Total_test_loss: 0.676477 Total_BCE_test_loss: 0.565499 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 152 total_train_loss: 0.706262 Total_test_loss: 0.676802 Total_BCE_test_loss: 0.565822 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 153 total_train_loss: 0.706431 Total_test_loss: 0.676878 Total_BCE_test_loss: 0.565907 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 154 total_train_loss: 0.705155 Total_test_loss: 0.676941 Total_BCE_test_loss: 0.565990 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 155 total_train_loss: 0.705798 Total_test_loss: 0.677112 Total_BCE_test_loss: 0.566199 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 156 total_train_loss: 0.704926 Total_test_loss: 0.676239 Total_BCE_test_loss: 0.565291 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 157 total_train_loss: 0.707527 Total_test_loss: 0.676810 Total_BCE_test_loss: 0.565880 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 158 total_train_loss: 0.706814 Total_test_loss: 0.676772 Total_BCE_test_loss: 0.565825 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 159 total_train_loss: 0.708174 Total_test_loss: 0.676591 Total_BCE_test_loss: 0.565600 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 160 total_train_loss: 0.706214 Total_test_loss: 0.677395 Total_BCE_test_loss: 0.566397 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 161 total_train_loss: 0.704793 Total_test_loss: 0.677312 Total_BCE_test_loss: 0.566341 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 162 total_train_loss: 0.705415 Total_test_loss: 0.677065 Total_BCE_test_loss: 0.566094 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 163 total_train_loss: 0.706062 Total_test_loss: 0.676863 Total_BCE_test_loss: 0.565898 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 164 total_train_loss: 0.705362 Total_test_loss: 0.677023 Total_BCE_test_loss: 0.566053 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 165 total_train_loss: 0.706452 Total_test_loss: 0.676866 Total_BCE_test_loss: 0.565916 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 166 total_train_loss: 0.704959 Total_test_loss: 0.676654 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 167 total_train_loss: 0.706505 Total_test_loss: 0.675932 Total_BCE_test_loss: 0.564915 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 168 total_train_loss: 0.707062 Total_test_loss: 0.675988 Total_BCE_test_loss: 0.564973 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 169 total_train_loss: 0.707083 Total_test_loss: 0.676499 Total_BCE_test_loss: 0.565525 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 170 total_train_loss: 0.708328 Total_test_loss: 0.676488 Total_BCE_test_loss: 0.565523 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 171 total_train_loss: 0.707613 Total_test_loss: 0.677143 Total_BCE_test_loss: 0.566248 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 172 total_train_loss: 0.708650 Total_test_loss: 0.677083 Total_BCE_test_loss: 0.566133 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 173 total_train_loss: 0.706509 Total_test_loss: 0.677360 Total_BCE_test_loss: 0.566359 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 174 total_train_loss: 0.706817 Total_test_loss: 0.677125 Total_BCE_test_loss: 0.566089 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107307\n",
      "====> Epoch: 175 total_train_loss: 0.705383 Total_test_loss: 0.677149 Total_BCE_test_loss: 0.566191 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 176 total_train_loss: 0.703840 Total_test_loss: 0.677035 Total_BCE_test_loss: 0.566069 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 177 total_train_loss: 0.706087 Total_test_loss: 0.676686 Total_BCE_test_loss: 0.565711 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 178 total_train_loss: 0.710553 Total_test_loss: 0.676639 Total_BCE_test_loss: 0.565648 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 179 total_train_loss: 0.707160 Total_test_loss: 0.676597 Total_BCE_test_loss: 0.565557 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 180 total_train_loss: 0.707450 Total_test_loss: 0.676126 Total_BCE_test_loss: 0.565086 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 181 total_train_loss: 0.707554 Total_test_loss: 0.677101 Total_BCE_test_loss: 0.566103 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 182 total_train_loss: 0.709828 Total_test_loss: 0.676175 Total_BCE_test_loss: 0.565204 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 183 total_train_loss: 0.707331 Total_test_loss: 0.676754 Total_BCE_test_loss: 0.565787 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 184 total_train_loss: 0.706634 Total_test_loss: 0.676740 Total_BCE_test_loss: 0.565772 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 185 total_train_loss: 0.707561 Total_test_loss: 0.676178 Total_BCE_test_loss: 0.565159 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 186 total_train_loss: 0.704400 Total_test_loss: 0.676678 Total_BCE_test_loss: 0.565707 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 187 total_train_loss: 0.703728 Total_test_loss: 0.676610 Total_BCE_test_loss: 0.565668 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 188 total_train_loss: 0.706649 Total_test_loss: 0.676747 Total_BCE_test_loss: 0.565811 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 189 total_train_loss: 0.706548 Total_test_loss: 0.675977 Total_BCE_test_loss: 0.564932 Total_KLD_test_loss: 0.003784 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 190 total_train_loss: 0.707327 Total_test_loss: 0.676677 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 191 total_train_loss: 0.708968 Total_test_loss: 0.676550 Total_BCE_test_loss: 0.565555 Total_KLD_test_loss: 0.003734 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 192 total_train_loss: 0.707086 Total_test_loss: 0.676929 Total_BCE_test_loss: 0.565947 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 193 total_train_loss: 0.709090 Total_test_loss: 0.676601 Total_BCE_test_loss: 0.565538 Total_KLD_test_loss: 0.003778 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 194 total_train_loss: 0.707129 Total_test_loss: 0.676672 Total_BCE_test_loss: 0.565639 Total_KLD_test_loss: 0.003769 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 195 total_train_loss: 0.704708 Total_test_loss: 0.677287 Total_BCE_test_loss: 0.566356 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107295\n",
      "====> Epoch: 196 total_train_loss: 0.704266 Total_test_loss: 0.676913 Total_BCE_test_loss: 0.565920 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 197 total_train_loss: 0.705728 Total_test_loss: 0.677104 Total_BCE_test_loss: 0.566194 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 198 total_train_loss: 0.704549 Total_test_loss: 0.676206 Total_BCE_test_loss: 0.565202 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 199 total_train_loss: 0.706432 Total_test_loss: 0.676178 Total_BCE_test_loss: 0.565170 Total_KLD_test_loss: 0.003779 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 200 total_train_loss: 0.707177 Total_test_loss: 0.675949 Total_BCE_test_loss: 0.564957 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 201 total_train_loss: 0.706884 Total_test_loss: 0.676836 Total_BCE_test_loss: 0.565816 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 202 total_train_loss: 0.709631 Total_test_loss: 0.676904 Total_BCE_test_loss: 0.565894 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 203 total_train_loss: 0.705849 Total_test_loss: 0.676815 Total_BCE_test_loss: 0.565831 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 204 total_train_loss: 0.705993 Total_test_loss: 0.676235 Total_BCE_test_loss: 0.565239 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 205 total_train_loss: 0.706535 Total_test_loss: 0.676536 Total_BCE_test_loss: 0.565543 Total_KLD_test_loss: 0.003738 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 206 total_train_loss: 0.706104 Total_test_loss: 0.675666 Total_BCE_test_loss: 0.564706 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 207 total_train_loss: 0.707707 Total_test_loss: 0.677060 Total_BCE_test_loss: 0.566104 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 208 total_train_loss: 0.710418 Total_test_loss: 0.676229 Total_BCE_test_loss: 0.565241 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 209 total_train_loss: 0.706157 Total_test_loss: 0.675965 Total_BCE_test_loss: 0.564947 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 210 total_train_loss: 0.704801 Total_test_loss: 0.676263 Total_BCE_test_loss: 0.565240 Total_KLD_test_loss: 0.003775 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 211 total_train_loss: 0.705545 Total_test_loss: 0.676740 Total_BCE_test_loss: 0.565748 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 212 total_train_loss: 0.704483 Total_test_loss: 0.676746 Total_BCE_test_loss: 0.565754 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 213 total_train_loss: 0.705738 Total_test_loss: 0.676800 Total_BCE_test_loss: 0.565864 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 214 total_train_loss: 0.707242 Total_test_loss: 0.676483 Total_BCE_test_loss: 0.565529 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 215 total_train_loss: 0.705996 Total_test_loss: 0.676284 Total_BCE_test_loss: 0.565276 Total_KLD_test_loss: 0.003738 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 216 total_train_loss: 0.705359 Total_test_loss: 0.676953 Total_BCE_test_loss: 0.565923 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 217 total_train_loss: 0.706815 Total_test_loss: 0.676457 Total_BCE_test_loss: 0.565455 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 218 total_train_loss: 0.705576 Total_test_loss: 0.676251 Total_BCE_test_loss: 0.565228 Total_KLD_test_loss: 0.003762 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 219 total_train_loss: 0.705645 Total_test_loss: 0.676590 Total_BCE_test_loss: 0.565591 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 220 total_train_loss: 0.707315 Total_test_loss: 0.676739 Total_BCE_test_loss: 0.565764 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 221 total_train_loss: 0.707699 Total_test_loss: 0.676356 Total_BCE_test_loss: 0.565367 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 222 total_train_loss: 0.706079 Total_test_loss: 0.676017 Total_BCE_test_loss: 0.565026 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 223 total_train_loss: 0.705378 Total_test_loss: 0.676714 Total_BCE_test_loss: 0.565748 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 224 total_train_loss: 0.704767 Total_test_loss: 0.677025 Total_BCE_test_loss: 0.566068 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 225 total_train_loss: 0.706108 Total_test_loss: 0.676753 Total_BCE_test_loss: 0.565811 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 226 total_train_loss: 0.707995 Total_test_loss: 0.677519 Total_BCE_test_loss: 0.566580 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 227 total_train_loss: 0.707268 Total_test_loss: 0.676834 Total_BCE_test_loss: 0.565899 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 228 total_train_loss: 0.704658 Total_test_loss: 0.676762 Total_BCE_test_loss: 0.565797 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 229 total_train_loss: 0.706651 Total_test_loss: 0.677045 Total_BCE_test_loss: 0.566116 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 230 total_train_loss: 0.708605 Total_test_loss: 0.676924 Total_BCE_test_loss: 0.565941 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 231 total_train_loss: 0.708419 Total_test_loss: 0.675677 Total_BCE_test_loss: 0.564630 Total_KLD_test_loss: 0.003768 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 232 total_train_loss: 0.706886 Total_test_loss: 0.675628 Total_BCE_test_loss: 0.564601 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 233 total_train_loss: 0.707741 Total_test_loss: 0.675698 Total_BCE_test_loss: 0.564685 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 234 total_train_loss: 0.706104 Total_test_loss: 0.676430 Total_BCE_test_loss: 0.565476 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 235 total_train_loss: 0.706681 Total_test_loss: 0.676181 Total_BCE_test_loss: 0.565146 Total_KLD_test_loss: 0.003758 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 236 total_train_loss: 0.705393 Total_test_loss: 0.676547 Total_BCE_test_loss: 0.565584 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 237 total_train_loss: 0.705296 Total_test_loss: 0.677009 Total_BCE_test_loss: 0.566029 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107303\n",
      "====> Epoch: 238 total_train_loss: 0.707073 Total_test_loss: 0.677953 Total_BCE_test_loss: 0.567049 Total_KLD_test_loss: 0.003640 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 239 total_train_loss: 0.707582 Total_test_loss: 0.676606 Total_BCE_test_loss: 0.565625 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 240 total_train_loss: 0.708045 Total_test_loss: 0.677086 Total_BCE_test_loss: 0.566109 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 241 total_train_loss: 0.705877 Total_test_loss: 0.676314 Total_BCE_test_loss: 0.565363 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 242 total_train_loss: 0.706933 Total_test_loss: 0.676218 Total_BCE_test_loss: 0.565235 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 243 total_train_loss: 0.706888 Total_test_loss: 0.676157 Total_BCE_test_loss: 0.565183 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 244 total_train_loss: 0.706872 Total_test_loss: 0.676298 Total_BCE_test_loss: 0.565286 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 245 total_train_loss: 0.706563 Total_test_loss: 0.676789 Total_BCE_test_loss: 0.565856 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 246 total_train_loss: 0.705531 Total_test_loss: 0.676695 Total_BCE_test_loss: 0.565737 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 247 total_train_loss: 0.703917 Total_test_loss: 0.677593 Total_BCE_test_loss: 0.566672 Total_KLD_test_loss: 0.003651 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 248 total_train_loss: 0.705877 Total_test_loss: 0.677397 Total_BCE_test_loss: 0.566484 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 249 total_train_loss: 0.706728 Total_test_loss: 0.676569 Total_BCE_test_loss: 0.565630 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 250 total_train_loss: 0.707983 Total_test_loss: 0.676724 Total_BCE_test_loss: 0.565764 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 251 total_train_loss: 0.705636 Total_test_loss: 0.676574 Total_BCE_test_loss: 0.565638 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 252 total_train_loss: 0.705127 Total_test_loss: 0.676348 Total_BCE_test_loss: 0.565341 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 253 total_train_loss: 0.706938 Total_test_loss: 0.676748 Total_BCE_test_loss: 0.565708 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 254 total_train_loss: 0.705098 Total_test_loss: 0.676669 Total_BCE_test_loss: 0.565661 Total_KLD_test_loss: 0.003756 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 255 total_train_loss: 0.706465 Total_test_loss: 0.676555 Total_BCE_test_loss: 0.565594 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 256 total_train_loss: 0.706350 Total_test_loss: 0.676986 Total_BCE_test_loss: 0.566008 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 257 total_train_loss: 0.704983 Total_test_loss: 0.676539 Total_BCE_test_loss: 0.565550 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 258 total_train_loss: 0.705499 Total_test_loss: 0.676267 Total_BCE_test_loss: 0.565265 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 259 total_train_loss: 0.706494 Total_test_loss: 0.676640 Total_BCE_test_loss: 0.565713 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 260 total_train_loss: 0.705116 Total_test_loss: 0.676766 Total_BCE_test_loss: 0.565825 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 261 total_train_loss: 0.705907 Total_test_loss: 0.676600 Total_BCE_test_loss: 0.565644 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 262 total_train_loss: 0.707591 Total_test_loss: 0.677241 Total_BCE_test_loss: 0.566260 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 263 total_train_loss: 0.705725 Total_test_loss: 0.676637 Total_BCE_test_loss: 0.565686 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 264 total_train_loss: 0.706419 Total_test_loss: 0.675892 Total_BCE_test_loss: 0.564914 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 265 total_train_loss: 0.707719 Total_test_loss: 0.676288 Total_BCE_test_loss: 0.565299 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 266 total_train_loss: 0.705283 Total_test_loss: 0.676175 Total_BCE_test_loss: 0.565216 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 267 total_train_loss: 0.707351 Total_test_loss: 0.676560 Total_BCE_test_loss: 0.565607 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 268 total_train_loss: 0.706674 Total_test_loss: 0.676769 Total_BCE_test_loss: 0.565817 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 269 total_train_loss: 0.704733 Total_test_loss: 0.676555 Total_BCE_test_loss: 0.565570 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 270 total_train_loss: 0.702986 Total_test_loss: 0.677296 Total_BCE_test_loss: 0.566329 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 271 total_train_loss: 0.707360 Total_test_loss: 0.676522 Total_BCE_test_loss: 0.565522 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 272 total_train_loss: 0.708037 Total_test_loss: 0.675919 Total_BCE_test_loss: 0.564833 Total_KLD_test_loss: 0.003800 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 273 total_train_loss: 0.705662 Total_test_loss: 0.676412 Total_BCE_test_loss: 0.565378 Total_KLD_test_loss: 0.003759 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 274 total_train_loss: 0.706750 Total_test_loss: 0.676329 Total_BCE_test_loss: 0.565318 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107297\n",
      "====> Epoch: 275 total_train_loss: 0.706518 Total_test_loss: 0.677146 Total_BCE_test_loss: 0.566227 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 276 total_train_loss: 0.705569 Total_test_loss: 0.677150 Total_BCE_test_loss: 0.566182 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 277 total_train_loss: 0.708879 Total_test_loss: 0.676356 Total_BCE_test_loss: 0.565360 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 278 total_train_loss: 0.706778 Total_test_loss: 0.676466 Total_BCE_test_loss: 0.565469 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 279 total_train_loss: 0.707428 Total_test_loss: 0.676571 Total_BCE_test_loss: 0.565607 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 280 total_train_loss: 0.705984 Total_test_loss: 0.676568 Total_BCE_test_loss: 0.565614 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 281 total_train_loss: 0.708655 Total_test_loss: 0.677216 Total_BCE_test_loss: 0.566278 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 282 total_train_loss: 0.706611 Total_test_loss: 0.676642 Total_BCE_test_loss: 0.565704 Total_KLD_test_loss: 0.003655 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 283 total_train_loss: 0.706609 Total_test_loss: 0.676737 Total_BCE_test_loss: 0.565840 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107221\n",
      "====> Epoch: 284 total_train_loss: 0.706778 Total_test_loss: 0.675938 Total_BCE_test_loss: 0.564931 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 285 total_train_loss: 0.706802 Total_test_loss: 0.676380 Total_BCE_test_loss: 0.565421 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 286 total_train_loss: 0.707100 Total_test_loss: 0.676235 Total_BCE_test_loss: 0.565248 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 287 total_train_loss: 0.708538 Total_test_loss: 0.676420 Total_BCE_test_loss: 0.565424 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 288 total_train_loss: 0.708569 Total_test_loss: 0.676579 Total_BCE_test_loss: 0.565566 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 289 total_train_loss: 0.706563 Total_test_loss: 0.676574 Total_BCE_test_loss: 0.565590 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 290 total_train_loss: 0.705808 Total_test_loss: 0.676841 Total_BCE_test_loss: 0.565917 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 291 total_train_loss: 0.705945 Total_test_loss: 0.676914 Total_BCE_test_loss: 0.565934 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 292 total_train_loss: 0.708301 Total_test_loss: 0.676226 Total_BCE_test_loss: 0.565240 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 293 total_train_loss: 0.704432 Total_test_loss: 0.676268 Total_BCE_test_loss: 0.565299 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 294 total_train_loss: 0.707003 Total_test_loss: 0.677031 Total_BCE_test_loss: 0.566074 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 295 total_train_loss: 0.704799 Total_test_loss: 0.676973 Total_BCE_test_loss: 0.566039 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 296 total_train_loss: 0.707690 Total_test_loss: 0.676924 Total_BCE_test_loss: 0.566005 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 297 total_train_loss: 0.705372 Total_test_loss: 0.676787 Total_BCE_test_loss: 0.565812 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 298 total_train_loss: 0.706703 Total_test_loss: 0.676829 Total_BCE_test_loss: 0.565871 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 299 total_train_loss: 0.706303 Total_test_loss: 0.676846 Total_BCE_test_loss: 0.565909 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 300 total_train_loss: 0.705591 Total_test_loss: 0.676683 Total_BCE_test_loss: 0.565692 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 301 total_train_loss: 0.705465 Total_test_loss: 0.676446 Total_BCE_test_loss: 0.565436 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 302 total_train_loss: 0.705863 Total_test_loss: 0.676686 Total_BCE_test_loss: 0.565715 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 303 total_train_loss: 0.705027 Total_test_loss: 0.676431 Total_BCE_test_loss: 0.565431 Total_KLD_test_loss: 0.003725 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 304 total_train_loss: 0.706593 Total_test_loss: 0.675494 Total_BCE_test_loss: 0.564470 Total_KLD_test_loss: 0.003776 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 305 total_train_loss: 0.708477 Total_test_loss: 0.675697 Total_BCE_test_loss: 0.564683 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 306 total_train_loss: 0.702718 Total_test_loss: 0.676169 Total_BCE_test_loss: 0.565209 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 307 total_train_loss: 0.706959 Total_test_loss: 0.676503 Total_BCE_test_loss: 0.565559 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 308 total_train_loss: 0.708034 Total_test_loss: 0.676738 Total_BCE_test_loss: 0.565745 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 309 total_train_loss: 0.708831 Total_test_loss: 0.676392 Total_BCE_test_loss: 0.565411 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 310 total_train_loss: 0.707041 Total_test_loss: 0.676401 Total_BCE_test_loss: 0.565372 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 311 total_train_loss: 0.705753 Total_test_loss: 0.676908 Total_BCE_test_loss: 0.565939 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 312 total_train_loss: 0.705722 Total_test_loss: 0.676322 Total_BCE_test_loss: 0.565339 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 313 total_train_loss: 0.706909 Total_test_loss: 0.676124 Total_BCE_test_loss: 0.565178 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 314 total_train_loss: 0.705350 Total_test_loss: 0.676617 Total_BCE_test_loss: 0.565656 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 315 total_train_loss: 0.704629 Total_test_loss: 0.677325 Total_BCE_test_loss: 0.566382 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 316 total_train_loss: 0.706232 Total_test_loss: 0.676730 Total_BCE_test_loss: 0.565722 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 317 total_train_loss: 0.707394 Total_test_loss: 0.676053 Total_BCE_test_loss: 0.565053 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 318 total_train_loss: 0.705899 Total_test_loss: 0.676977 Total_BCE_test_loss: 0.566012 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 319 total_train_loss: 0.707654 Total_test_loss: 0.676971 Total_BCE_test_loss: 0.566014 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 320 total_train_loss: 0.705581 Total_test_loss: 0.676248 Total_BCE_test_loss: 0.565269 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 321 total_train_loss: 0.708856 Total_test_loss: 0.676019 Total_BCE_test_loss: 0.565056 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 322 total_train_loss: 0.707158 Total_test_loss: 0.676336 Total_BCE_test_loss: 0.565323 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 323 total_train_loss: 0.707306 Total_test_loss: 0.676767 Total_BCE_test_loss: 0.565821 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 324 total_train_loss: 0.705778 Total_test_loss: 0.676676 Total_BCE_test_loss: 0.565731 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 325 total_train_loss: 0.707512 Total_test_loss: 0.676484 Total_BCE_test_loss: 0.565538 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 326 total_train_loss: 0.707354 Total_test_loss: 0.676986 Total_BCE_test_loss: 0.566008 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 327 total_train_loss: 0.706152 Total_test_loss: 0.676227 Total_BCE_test_loss: 0.565234 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 328 total_train_loss: 0.705564 Total_test_loss: 0.676935 Total_BCE_test_loss: 0.566012 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 329 total_train_loss: 0.707223 Total_test_loss: 0.677492 Total_BCE_test_loss: 0.566578 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 330 total_train_loss: 0.706526 Total_test_loss: 0.676651 Total_BCE_test_loss: 0.565679 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 331 total_train_loss: 0.708525 Total_test_loss: 0.676030 Total_BCE_test_loss: 0.564983 Total_KLD_test_loss: 0.003760 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 332 total_train_loss: 0.706958 Total_test_loss: 0.676822 Total_BCE_test_loss: 0.565836 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 333 total_train_loss: 0.706141 Total_test_loss: 0.676815 Total_BCE_test_loss: 0.565823 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 334 total_train_loss: 0.705661 Total_test_loss: 0.676789 Total_BCE_test_loss: 0.565818 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 335 total_train_loss: 0.704889 Total_test_loss: 0.677089 Total_BCE_test_loss: 0.566133 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 336 total_train_loss: 0.704493 Total_test_loss: 0.676678 Total_BCE_test_loss: 0.565663 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 337 total_train_loss: 0.707323 Total_test_loss: 0.676426 Total_BCE_test_loss: 0.565406 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 338 total_train_loss: 0.704532 Total_test_loss: 0.677143 Total_BCE_test_loss: 0.566193 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 339 total_train_loss: 0.704733 Total_test_loss: 0.676800 Total_BCE_test_loss: 0.565841 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 340 total_train_loss: 0.706381 Total_test_loss: 0.676776 Total_BCE_test_loss: 0.565803 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 341 total_train_loss: 0.707374 Total_test_loss: 0.676638 Total_BCE_test_loss: 0.565703 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 342 total_train_loss: 0.705655 Total_test_loss: 0.676268 Total_BCE_test_loss: 0.565316 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 343 total_train_loss: 0.705584 Total_test_loss: 0.676107 Total_BCE_test_loss: 0.565163 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 344 total_train_loss: 0.707471 Total_test_loss: 0.675872 Total_BCE_test_loss: 0.564853 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 345 total_train_loss: 0.706200 Total_test_loss: 0.676987 Total_BCE_test_loss: 0.566043 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 346 total_train_loss: 0.706956 Total_test_loss: 0.676747 Total_BCE_test_loss: 0.565790 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 347 total_train_loss: 0.706337 Total_test_loss: 0.676738 Total_BCE_test_loss: 0.565814 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107225\n",
      "====> Epoch: 348 total_train_loss: 0.707267 Total_test_loss: 0.676509 Total_BCE_test_loss: 0.565526 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 349 total_train_loss: 0.705791 Total_test_loss: 0.676465 Total_BCE_test_loss: 0.565490 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 350 total_train_loss: 0.705098 Total_test_loss: 0.676310 Total_BCE_test_loss: 0.565289 Total_KLD_test_loss: 0.003733 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 351 total_train_loss: 0.704386 Total_test_loss: 0.676242 Total_BCE_test_loss: 0.565238 Total_KLD_test_loss: 0.003733 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 352 total_train_loss: 0.706003 Total_test_loss: 0.677485 Total_BCE_test_loss: 0.566490 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107298\n",
      "====> Epoch: 353 total_train_loss: 0.708115 Total_test_loss: 0.676699 Total_BCE_test_loss: 0.565724 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 354 total_train_loss: 0.706685 Total_test_loss: 0.676570 Total_BCE_test_loss: 0.565635 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 355 total_train_loss: 0.703560 Total_test_loss: 0.676416 Total_BCE_test_loss: 0.565443 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 356 total_train_loss: 0.705982 Total_test_loss: 0.676232 Total_BCE_test_loss: 0.565211 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 357 total_train_loss: 0.707698 Total_test_loss: 0.676620 Total_BCE_test_loss: 0.565632 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 358 total_train_loss: 0.706781 Total_test_loss: 0.677045 Total_BCE_test_loss: 0.566107 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 359 total_train_loss: 0.707287 Total_test_loss: 0.676309 Total_BCE_test_loss: 0.565357 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 360 total_train_loss: 0.704794 Total_test_loss: 0.676709 Total_BCE_test_loss: 0.565771 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 361 total_train_loss: 0.704477 Total_test_loss: 0.676257 Total_BCE_test_loss: 0.565287 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 362 total_train_loss: 0.707563 Total_test_loss: 0.676514 Total_BCE_test_loss: 0.565560 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 363 total_train_loss: 0.709161 Total_test_loss: 0.676081 Total_BCE_test_loss: 0.565079 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 364 total_train_loss: 0.709262 Total_test_loss: 0.675542 Total_BCE_test_loss: 0.564609 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107207\n",
      "====> Epoch: 365 total_train_loss: 0.705721 Total_test_loss: 0.676216 Total_BCE_test_loss: 0.565256 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 366 total_train_loss: 0.705836 Total_test_loss: 0.676690 Total_BCE_test_loss: 0.565709 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 367 total_train_loss: 0.706883 Total_test_loss: 0.676327 Total_BCE_test_loss: 0.565318 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 368 total_train_loss: 0.706692 Total_test_loss: 0.676261 Total_BCE_test_loss: 0.565309 Total_KLD_test_loss: 0.003733 Total_CEP_test_loss: 0.107219\n",
      "====> Epoch: 369 total_train_loss: 0.708173 Total_test_loss: 0.676082 Total_BCE_test_loss: 0.565073 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 370 total_train_loss: 0.707407 Total_test_loss: 0.676343 Total_BCE_test_loss: 0.565375 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 371 total_train_loss: 0.706532 Total_test_loss: 0.676875 Total_BCE_test_loss: 0.565861 Total_KLD_test_loss: 0.003758 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 372 total_train_loss: 0.704941 Total_test_loss: 0.677057 Total_BCE_test_loss: 0.566063 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 373 total_train_loss: 0.704196 Total_test_loss: 0.676697 Total_BCE_test_loss: 0.565778 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 374 total_train_loss: 0.706758 Total_test_loss: 0.676889 Total_BCE_test_loss: 0.565992 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 375 total_train_loss: 0.706389 Total_test_loss: 0.676257 Total_BCE_test_loss: 0.565291 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 376 total_train_loss: 0.709514 Total_test_loss: 0.675585 Total_BCE_test_loss: 0.564504 Total_KLD_test_loss: 0.003835 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 377 total_train_loss: 0.705193 Total_test_loss: 0.676208 Total_BCE_test_loss: 0.565201 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 378 total_train_loss: 0.704064 Total_test_loss: 0.677031 Total_BCE_test_loss: 0.566081 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 379 total_train_loss: 0.706835 Total_test_loss: 0.675911 Total_BCE_test_loss: 0.564943 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 380 total_train_loss: 0.703880 Total_test_loss: 0.676161 Total_BCE_test_loss: 0.565198 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 381 total_train_loss: 0.705577 Total_test_loss: 0.677052 Total_BCE_test_loss: 0.566085 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 382 total_train_loss: 0.705538 Total_test_loss: 0.677648 Total_BCE_test_loss: 0.566685 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 383 total_train_loss: 0.704589 Total_test_loss: 0.677001 Total_BCE_test_loss: 0.566016 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 384 total_train_loss: 0.707902 Total_test_loss: 0.676310 Total_BCE_test_loss: 0.565296 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 385 total_train_loss: 0.705420 Total_test_loss: 0.676700 Total_BCE_test_loss: 0.565664 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 386 total_train_loss: 0.707555 Total_test_loss: 0.676257 Total_BCE_test_loss: 0.565219 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107291\n",
      "====> Epoch: 387 total_train_loss: 0.705823 Total_test_loss: 0.676329 Total_BCE_test_loss: 0.565361 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 388 total_train_loss: 0.705804 Total_test_loss: 0.676348 Total_BCE_test_loss: 0.565370 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 389 total_train_loss: 0.707081 Total_test_loss: 0.676973 Total_BCE_test_loss: 0.566034 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 390 total_train_loss: 0.706284 Total_test_loss: 0.676582 Total_BCE_test_loss: 0.565601 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 391 total_train_loss: 0.706041 Total_test_loss: 0.676839 Total_BCE_test_loss: 0.565917 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 392 total_train_loss: 0.706897 Total_test_loss: 0.676678 Total_BCE_test_loss: 0.565788 Total_KLD_test_loss: 0.003659 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 393 total_train_loss: 0.707034 Total_test_loss: 0.677126 Total_BCE_test_loss: 0.566204 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 394 total_train_loss: 0.707192 Total_test_loss: 0.676740 Total_BCE_test_loss: 0.565749 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 395 total_train_loss: 0.705813 Total_test_loss: 0.677176 Total_BCE_test_loss: 0.566188 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 396 total_train_loss: 0.705746 Total_test_loss: 0.677224 Total_BCE_test_loss: 0.566257 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 397 total_train_loss: 0.706707 Total_test_loss: 0.676690 Total_BCE_test_loss: 0.565647 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 398 total_train_loss: 0.705396 Total_test_loss: 0.676491 Total_BCE_test_loss: 0.565478 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107297\n",
      "====> Epoch: 399 total_train_loss: 0.706177 Total_test_loss: 0.676596 Total_BCE_test_loss: 0.565635 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 400 total_train_loss: 0.706850 Total_test_loss: 0.676176 Total_BCE_test_loss: 0.565184 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 401 total_train_loss: 0.704636 Total_test_loss: 0.676070 Total_BCE_test_loss: 0.565144 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107215\n",
      "====> Epoch: 402 total_train_loss: 0.705032 Total_test_loss: 0.677024 Total_BCE_test_loss: 0.566101 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 403 total_train_loss: 0.707364 Total_test_loss: 0.676858 Total_BCE_test_loss: 0.565939 Total_KLD_test_loss: 0.003646 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 404 total_train_loss: 0.706670 Total_test_loss: 0.677075 Total_BCE_test_loss: 0.566099 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 405 total_train_loss: 0.705783 Total_test_loss: 0.677020 Total_BCE_test_loss: 0.566017 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 406 total_train_loss: 0.706613 Total_test_loss: 0.676896 Total_BCE_test_loss: 0.565907 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 407 total_train_loss: 0.706644 Total_test_loss: 0.676162 Total_BCE_test_loss: 0.565173 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 408 total_train_loss: 0.708031 Total_test_loss: 0.676133 Total_BCE_test_loss: 0.565209 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 409 total_train_loss: 0.705480 Total_test_loss: 0.676362 Total_BCE_test_loss: 0.565341 Total_KLD_test_loss: 0.003759 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 410 total_train_loss: 0.705625 Total_test_loss: 0.676351 Total_BCE_test_loss: 0.565327 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 411 total_train_loss: 0.706451 Total_test_loss: 0.676169 Total_BCE_test_loss: 0.565138 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107299\n",
      "====> Epoch: 412 total_train_loss: 0.706600 Total_test_loss: 0.677050 Total_BCE_test_loss: 0.566079 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 413 total_train_loss: 0.704637 Total_test_loss: 0.675936 Total_BCE_test_loss: 0.564968 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 414 total_train_loss: 0.708179 Total_test_loss: 0.675925 Total_BCE_test_loss: 0.564953 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 415 total_train_loss: 0.707498 Total_test_loss: 0.676307 Total_BCE_test_loss: 0.565375 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 416 total_train_loss: 0.707424 Total_test_loss: 0.675999 Total_BCE_test_loss: 0.565012 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 417 total_train_loss: 0.706776 Total_test_loss: 0.676863 Total_BCE_test_loss: 0.565892 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 418 total_train_loss: 0.705342 Total_test_loss: 0.676499 Total_BCE_test_loss: 0.565552 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 419 total_train_loss: 0.705822 Total_test_loss: 0.676054 Total_BCE_test_loss: 0.565068 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 420 total_train_loss: 0.705528 Total_test_loss: 0.676191 Total_BCE_test_loss: 0.565224 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 421 total_train_loss: 0.705858 Total_test_loss: 0.675600 Total_BCE_test_loss: 0.564590 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 422 total_train_loss: 0.707076 Total_test_loss: 0.676038 Total_BCE_test_loss: 0.565064 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 423 total_train_loss: 0.705968 Total_test_loss: 0.676252 Total_BCE_test_loss: 0.565279 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 424 total_train_loss: 0.705986 Total_test_loss: 0.676589 Total_BCE_test_loss: 0.565609 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 425 total_train_loss: 0.706516 Total_test_loss: 0.676369 Total_BCE_test_loss: 0.565436 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 426 total_train_loss: 0.705809 Total_test_loss: 0.676012 Total_BCE_test_loss: 0.565054 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 427 total_train_loss: 0.707101 Total_test_loss: 0.676431 Total_BCE_test_loss: 0.565430 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 428 total_train_loss: 0.706804 Total_test_loss: 0.676162 Total_BCE_test_loss: 0.565230 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 429 total_train_loss: 0.706239 Total_test_loss: 0.675553 Total_BCE_test_loss: 0.564577 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107231\n",
      "====> Epoch: 430 total_train_loss: 0.706319 Total_test_loss: 0.675916 Total_BCE_test_loss: 0.564884 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 431 total_train_loss: 0.706663 Total_test_loss: 0.676461 Total_BCE_test_loss: 0.565493 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 432 total_train_loss: 0.706522 Total_test_loss: 0.676479 Total_BCE_test_loss: 0.565526 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 433 total_train_loss: 0.707061 Total_test_loss: 0.676894 Total_BCE_test_loss: 0.565937 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 434 total_train_loss: 0.706458 Total_test_loss: 0.676825 Total_BCE_test_loss: 0.565804 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 435 total_train_loss: 0.705562 Total_test_loss: 0.676687 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 436 total_train_loss: 0.705242 Total_test_loss: 0.677085 Total_BCE_test_loss: 0.566107 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 437 total_train_loss: 0.707180 Total_test_loss: 0.676262 Total_BCE_test_loss: 0.565220 Total_KLD_test_loss: 0.003773 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 438 total_train_loss: 0.706751 Total_test_loss: 0.675989 Total_BCE_test_loss: 0.564939 Total_KLD_test_loss: 0.003809 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 439 total_train_loss: 0.706048 Total_test_loss: 0.675623 Total_BCE_test_loss: 0.564586 Total_KLD_test_loss: 0.003764 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 440 total_train_loss: 0.706239 Total_test_loss: 0.676152 Total_BCE_test_loss: 0.565140 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 441 total_train_loss: 0.708330 Total_test_loss: 0.676110 Total_BCE_test_loss: 0.565083 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 442 total_train_loss: 0.705399 Total_test_loss: 0.676834 Total_BCE_test_loss: 0.565834 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 443 total_train_loss: 0.706542 Total_test_loss: 0.676371 Total_BCE_test_loss: 0.565396 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 444 total_train_loss: 0.704681 Total_test_loss: 0.676325 Total_BCE_test_loss: 0.565305 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 445 total_train_loss: 0.708094 Total_test_loss: 0.676495 Total_BCE_test_loss: 0.565446 Total_KLD_test_loss: 0.003768 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 446 total_train_loss: 0.709347 Total_test_loss: 0.676297 Total_BCE_test_loss: 0.565335 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 447 total_train_loss: 0.710515 Total_test_loss: 0.676540 Total_BCE_test_loss: 0.565616 Total_KLD_test_loss: 0.003664 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 448 total_train_loss: 0.706232 Total_test_loss: 0.677278 Total_BCE_test_loss: 0.566358 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 449 total_train_loss: 0.706518 Total_test_loss: 0.677866 Total_BCE_test_loss: 0.566951 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 450 total_train_loss: 0.705813 Total_test_loss: 0.677629 Total_BCE_test_loss: 0.566717 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 451 total_train_loss: 0.704865 Total_test_loss: 0.676442 Total_BCE_test_loss: 0.565483 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 452 total_train_loss: 0.706100 Total_test_loss: 0.676923 Total_BCE_test_loss: 0.565949 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 453 total_train_loss: 0.705170 Total_test_loss: 0.676201 Total_BCE_test_loss: 0.565255 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 454 total_train_loss: 0.708609 Total_test_loss: 0.676039 Total_BCE_test_loss: 0.565045 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 455 total_train_loss: 0.705843 Total_test_loss: 0.676943 Total_BCE_test_loss: 0.566010 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 456 total_train_loss: 0.706091 Total_test_loss: 0.676421 Total_BCE_test_loss: 0.565404 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 457 total_train_loss: 0.706707 Total_test_loss: 0.677113 Total_BCE_test_loss: 0.566157 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 458 total_train_loss: 0.708392 Total_test_loss: 0.677376 Total_BCE_test_loss: 0.566432 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 459 total_train_loss: 0.705673 Total_test_loss: 0.677533 Total_BCE_test_loss: 0.566609 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 460 total_train_loss: 0.708589 Total_test_loss: 0.676547 Total_BCE_test_loss: 0.565559 Total_KLD_test_loss: 0.003719 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 461 total_train_loss: 0.706889 Total_test_loss: 0.675544 Total_BCE_test_loss: 0.564507 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 462 total_train_loss: 0.705278 Total_test_loss: 0.676298 Total_BCE_test_loss: 0.565290 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 463 total_train_loss: 0.705513 Total_test_loss: 0.676149 Total_BCE_test_loss: 0.565163 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 464 total_train_loss: 0.707475 Total_test_loss: 0.676287 Total_BCE_test_loss: 0.565269 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 465 total_train_loss: 0.707846 Total_test_loss: 0.676558 Total_BCE_test_loss: 0.565556 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 466 total_train_loss: 0.707479 Total_test_loss: 0.676888 Total_BCE_test_loss: 0.565934 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 467 total_train_loss: 0.708136 Total_test_loss: 0.676955 Total_BCE_test_loss: 0.566001 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 468 total_train_loss: 0.706938 Total_test_loss: 0.676411 Total_BCE_test_loss: 0.565459 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 469 total_train_loss: 0.707292 Total_test_loss: 0.676811 Total_BCE_test_loss: 0.565866 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 470 total_train_loss: 0.707277 Total_test_loss: 0.677156 Total_BCE_test_loss: 0.566199 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 471 total_train_loss: 0.704621 Total_test_loss: 0.676953 Total_BCE_test_loss: 0.566003 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 472 total_train_loss: 0.706217 Total_test_loss: 0.676697 Total_BCE_test_loss: 0.565757 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 473 total_train_loss: 0.704249 Total_test_loss: 0.677107 Total_BCE_test_loss: 0.566193 Total_KLD_test_loss: 0.003640 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 474 total_train_loss: 0.707382 Total_test_loss: 0.676756 Total_BCE_test_loss: 0.565801 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107226\n",
      "====> Epoch: 475 total_train_loss: 0.708854 Total_test_loss: 0.676364 Total_BCE_test_loss: 0.565370 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 476 total_train_loss: 0.706507 Total_test_loss: 0.676048 Total_BCE_test_loss: 0.565019 Total_KLD_test_loss: 0.003753 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 477 total_train_loss: 0.705705 Total_test_loss: 0.676671 Total_BCE_test_loss: 0.565672 Total_KLD_test_loss: 0.003743 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 478 total_train_loss: 0.705944 Total_test_loss: 0.676632 Total_BCE_test_loss: 0.565665 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 479 total_train_loss: 0.709972 Total_test_loss: 0.676119 Total_BCE_test_loss: 0.565219 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107201\n",
      "====> Epoch: 480 total_train_loss: 0.704446 Total_test_loss: 0.676347 Total_BCE_test_loss: 0.565376 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 481 total_train_loss: 0.704927 Total_test_loss: 0.676329 Total_BCE_test_loss: 0.565391 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 482 total_train_loss: 0.705319 Total_test_loss: 0.676505 Total_BCE_test_loss: 0.565571 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 483 total_train_loss: 0.706774 Total_test_loss: 0.676784 Total_BCE_test_loss: 0.565837 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 484 total_train_loss: 0.703828 Total_test_loss: 0.676867 Total_BCE_test_loss: 0.565908 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 485 total_train_loss: 0.705485 Total_test_loss: 0.676306 Total_BCE_test_loss: 0.565282 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 486 total_train_loss: 0.707722 Total_test_loss: 0.676179 Total_BCE_test_loss: 0.565192 Total_KLD_test_loss: 0.003733 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 487 total_train_loss: 0.707005 Total_test_loss: 0.676338 Total_BCE_test_loss: 0.565311 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 488 total_train_loss: 0.706455 Total_test_loss: 0.675999 Total_BCE_test_loss: 0.564994 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 489 total_train_loss: 0.708948 Total_test_loss: 0.676956 Total_BCE_test_loss: 0.565999 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 490 total_train_loss: 0.707044 Total_test_loss: 0.676649 Total_BCE_test_loss: 0.565696 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 491 total_train_loss: 0.705794 Total_test_loss: 0.676394 Total_BCE_test_loss: 0.565432 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 492 total_train_loss: 0.707548 Total_test_loss: 0.676439 Total_BCE_test_loss: 0.565516 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 493 total_train_loss: 0.705921 Total_test_loss: 0.676696 Total_BCE_test_loss: 0.565739 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 494 total_train_loss: 0.704550 Total_test_loss: 0.676321 Total_BCE_test_loss: 0.565333 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 495 total_train_loss: 0.707377 Total_test_loss: 0.676114 Total_BCE_test_loss: 0.565136 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 496 total_train_loss: 0.706184 Total_test_loss: 0.676208 Total_BCE_test_loss: 0.565196 Total_KLD_test_loss: 0.003774 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 497 total_train_loss: 0.705287 Total_test_loss: 0.676841 Total_BCE_test_loss: 0.565867 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 498 total_train_loss: 0.705934 Total_test_loss: 0.677248 Total_BCE_test_loss: 0.566283 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 499 total_train_loss: 0.708790 Total_test_loss: 0.676510 Total_BCE_test_loss: 0.565546 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 500 total_train_loss: 0.707438 Total_test_loss: 0.676164 Total_BCE_test_loss: 0.565220 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107245\n",
      "5e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 total_train_loss: 0.705086 Total_test_loss: 0.676658 Total_BCE_test_loss: 0.565737 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 2 total_train_loss: 0.706298 Total_test_loss: 0.676613 Total_BCE_test_loss: 0.565663 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 3 total_train_loss: 0.706472 Total_test_loss: 0.676127 Total_BCE_test_loss: 0.565162 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 4 total_train_loss: 0.705754 Total_test_loss: 0.676099 Total_BCE_test_loss: 0.565114 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 5 total_train_loss: 0.710987 Total_test_loss: 0.675929 Total_BCE_test_loss: 0.564871 Total_KLD_test_loss: 0.003791 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 6 total_train_loss: 0.706265 Total_test_loss: 0.676084 Total_BCE_test_loss: 0.565054 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 7 total_train_loss: 0.707809 Total_test_loss: 0.676118 Total_BCE_test_loss: 0.565106 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 8 total_train_loss: 0.703936 Total_test_loss: 0.675989 Total_BCE_test_loss: 0.564962 Total_KLD_test_loss: 0.003771 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 9 total_train_loss: 0.708178 Total_test_loss: 0.676686 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 10 total_train_loss: 0.705411 Total_test_loss: 0.676888 Total_BCE_test_loss: 0.565912 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 11 total_train_loss: 0.705032 Total_test_loss: 0.677034 Total_BCE_test_loss: 0.566066 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 12 total_train_loss: 0.705169 Total_test_loss: 0.677191 Total_BCE_test_loss: 0.566241 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 13 total_train_loss: 0.707574 Total_test_loss: 0.676396 Total_BCE_test_loss: 0.565377 Total_KLD_test_loss: 0.003759 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 14 total_train_loss: 0.706202 Total_test_loss: 0.676504 Total_BCE_test_loss: 0.565561 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 15 total_train_loss: 0.707043 Total_test_loss: 0.676255 Total_BCE_test_loss: 0.565281 Total_KLD_test_loss: 0.003725 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 16 total_train_loss: 0.706572 Total_test_loss: 0.676787 Total_BCE_test_loss: 0.565801 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 17 total_train_loss: 0.707719 Total_test_loss: 0.676428 Total_BCE_test_loss: 0.565399 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 18 total_train_loss: 0.705140 Total_test_loss: 0.676433 Total_BCE_test_loss: 0.565417 Total_KLD_test_loss: 0.003774 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 19 total_train_loss: 0.708336 Total_test_loss: 0.676520 Total_BCE_test_loss: 0.565500 Total_KLD_test_loss: 0.003766 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 20 total_train_loss: 0.706261 Total_test_loss: 0.677146 Total_BCE_test_loss: 0.566128 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 21 total_train_loss: 0.705043 Total_test_loss: 0.677025 Total_BCE_test_loss: 0.566061 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 22 total_train_loss: 0.704321 Total_test_loss: 0.677471 Total_BCE_test_loss: 0.566488 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 23 total_train_loss: 0.705422 Total_test_loss: 0.677068 Total_BCE_test_loss: 0.566115 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 24 total_train_loss: 0.707451 Total_test_loss: 0.676598 Total_BCE_test_loss: 0.565633 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 25 total_train_loss: 0.705344 Total_test_loss: 0.676911 Total_BCE_test_loss: 0.565950 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 26 total_train_loss: 0.707635 Total_test_loss: 0.676939 Total_BCE_test_loss: 0.565942 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107304\n",
      "====> Epoch: 27 total_train_loss: 0.706558 Total_test_loss: 0.676636 Total_BCE_test_loss: 0.565603 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 28 total_train_loss: 0.707021 Total_test_loss: 0.675706 Total_BCE_test_loss: 0.564760 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107229\n",
      "====> Epoch: 29 total_train_loss: 0.708877 Total_test_loss: 0.676140 Total_BCE_test_loss: 0.565116 Total_KLD_test_loss: 0.003764 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 30 total_train_loss: 0.705224 Total_test_loss: 0.676128 Total_BCE_test_loss: 0.565136 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 31 total_train_loss: 0.704749 Total_test_loss: 0.676567 Total_BCE_test_loss: 0.565577 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 32 total_train_loss: 0.707515 Total_test_loss: 0.676139 Total_BCE_test_loss: 0.565137 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 33 total_train_loss: 0.705778 Total_test_loss: 0.675929 Total_BCE_test_loss: 0.564994 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 34 total_train_loss: 0.704702 Total_test_loss: 0.676005 Total_BCE_test_loss: 0.565054 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 35 total_train_loss: 0.705738 Total_test_loss: 0.676173 Total_BCE_test_loss: 0.565178 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107240\n",
      "====> Epoch: 36 total_train_loss: 0.706037 Total_test_loss: 0.676255 Total_BCE_test_loss: 0.565285 Total_KLD_test_loss: 0.003729 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 37 total_train_loss: 0.708582 Total_test_loss: 0.676194 Total_BCE_test_loss: 0.565255 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 38 total_train_loss: 0.706681 Total_test_loss: 0.675952 Total_BCE_test_loss: 0.564954 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 39 total_train_loss: 0.706694 Total_test_loss: 0.675783 Total_BCE_test_loss: 0.564751 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 40 total_train_loss: 0.706928 Total_test_loss: 0.676091 Total_BCE_test_loss: 0.565112 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 41 total_train_loss: 0.705434 Total_test_loss: 0.676920 Total_BCE_test_loss: 0.565968 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 42 total_train_loss: 0.704785 Total_test_loss: 0.676296 Total_BCE_test_loss: 0.565298 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 43 total_train_loss: 0.704071 Total_test_loss: 0.676350 Total_BCE_test_loss: 0.565377 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 44 total_train_loss: 0.706359 Total_test_loss: 0.676372 Total_BCE_test_loss: 0.565373 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 45 total_train_loss: 0.707091 Total_test_loss: 0.676027 Total_BCE_test_loss: 0.565041 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 46 total_train_loss: 0.706415 Total_test_loss: 0.676450 Total_BCE_test_loss: 0.565463 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 47 total_train_loss: 0.706401 Total_test_loss: 0.677106 Total_BCE_test_loss: 0.566148 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 48 total_train_loss: 0.704445 Total_test_loss: 0.677389 Total_BCE_test_loss: 0.566451 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 49 total_train_loss: 0.704090 Total_test_loss: 0.677138 Total_BCE_test_loss: 0.566206 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 50 total_train_loss: 0.706420 Total_test_loss: 0.676707 Total_BCE_test_loss: 0.565756 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 51 total_train_loss: 0.707579 Total_test_loss: 0.677322 Total_BCE_test_loss: 0.566326 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 52 total_train_loss: 0.706804 Total_test_loss: 0.676368 Total_BCE_test_loss: 0.565358 Total_KLD_test_loss: 0.003722 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 53 total_train_loss: 0.705845 Total_test_loss: 0.675980 Total_BCE_test_loss: 0.565010 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 54 total_train_loss: 0.707683 Total_test_loss: 0.676214 Total_BCE_test_loss: 0.565238 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 55 total_train_loss: 0.706399 Total_test_loss: 0.676110 Total_BCE_test_loss: 0.565129 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 56 total_train_loss: 0.706630 Total_test_loss: 0.675840 Total_BCE_test_loss: 0.564817 Total_KLD_test_loss: 0.003770 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 57 total_train_loss: 0.705716 Total_test_loss: 0.676451 Total_BCE_test_loss: 0.565511 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 58 total_train_loss: 0.705985 Total_test_loss: 0.677001 Total_BCE_test_loss: 0.565975 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 59 total_train_loss: 0.706614 Total_test_loss: 0.677205 Total_BCE_test_loss: 0.566215 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 60 total_train_loss: 0.705556 Total_test_loss: 0.676934 Total_BCE_test_loss: 0.565944 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107286\n",
      "====> Epoch: 61 total_train_loss: 0.705347 Total_test_loss: 0.677131 Total_BCE_test_loss: 0.566164 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 62 total_train_loss: 0.705490 Total_test_loss: 0.676653 Total_BCE_test_loss: 0.565675 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 63 total_train_loss: 0.704946 Total_test_loss: 0.677015 Total_BCE_test_loss: 0.566050 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 64 total_train_loss: 0.705299 Total_test_loss: 0.676602 Total_BCE_test_loss: 0.565644 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 65 total_train_loss: 0.706706 Total_test_loss: 0.676595 Total_BCE_test_loss: 0.565631 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 66 total_train_loss: 0.706809 Total_test_loss: 0.676122 Total_BCE_test_loss: 0.565117 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 67 total_train_loss: 0.707275 Total_test_loss: 0.675895 Total_BCE_test_loss: 0.564893 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 68 total_train_loss: 0.706944 Total_test_loss: 0.676450 Total_BCE_test_loss: 0.565479 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 69 total_train_loss: 0.705840 Total_test_loss: 0.676768 Total_BCE_test_loss: 0.565845 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 70 total_train_loss: 0.706387 Total_test_loss: 0.676384 Total_BCE_test_loss: 0.565383 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 71 total_train_loss: 0.706037 Total_test_loss: 0.675443 Total_BCE_test_loss: 0.564414 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 72 total_train_loss: 0.707301 Total_test_loss: 0.676561 Total_BCE_test_loss: 0.565567 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 73 total_train_loss: 0.705782 Total_test_loss: 0.675927 Total_BCE_test_loss: 0.564905 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 74 total_train_loss: 0.705155 Total_test_loss: 0.676083 Total_BCE_test_loss: 0.565053 Total_KLD_test_loss: 0.003769 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 75 total_train_loss: 0.705935 Total_test_loss: 0.675973 Total_BCE_test_loss: 0.564963 Total_KLD_test_loss: 0.003762 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 76 total_train_loss: 0.706701 Total_test_loss: 0.676207 Total_BCE_test_loss: 0.565151 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 77 total_train_loss: 0.705133 Total_test_loss: 0.676575 Total_BCE_test_loss: 0.565609 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 78 total_train_loss: 0.707073 Total_test_loss: 0.676499 Total_BCE_test_loss: 0.565534 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 79 total_train_loss: 0.706677 Total_test_loss: 0.676264 Total_BCE_test_loss: 0.565303 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 80 total_train_loss: 0.706197 Total_test_loss: 0.676546 Total_BCE_test_loss: 0.565566 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 81 total_train_loss: 0.707824 Total_test_loss: 0.677040 Total_BCE_test_loss: 0.566114 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 82 total_train_loss: 0.706990 Total_test_loss: 0.676274 Total_BCE_test_loss: 0.565284 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 83 total_train_loss: 0.708561 Total_test_loss: 0.676029 Total_BCE_test_loss: 0.565047 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 84 total_train_loss: 0.705875 Total_test_loss: 0.677000 Total_BCE_test_loss: 0.566031 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 85 total_train_loss: 0.709136 Total_test_loss: 0.676057 Total_BCE_test_loss: 0.565111 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 86 total_train_loss: 0.705366 Total_test_loss: 0.676622 Total_BCE_test_loss: 0.565627 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 87 total_train_loss: 0.706458 Total_test_loss: 0.676629 Total_BCE_test_loss: 0.565600 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 88 total_train_loss: 0.707753 Total_test_loss: 0.676385 Total_BCE_test_loss: 0.565363 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 89 total_train_loss: 0.706297 Total_test_loss: 0.676265 Total_BCE_test_loss: 0.565252 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 90 total_train_loss: 0.707215 Total_test_loss: 0.676035 Total_BCE_test_loss: 0.565040 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 91 total_train_loss: 0.707769 Total_test_loss: 0.675932 Total_BCE_test_loss: 0.564956 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 92 total_train_loss: 0.705954 Total_test_loss: 0.676310 Total_BCE_test_loss: 0.565347 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 93 total_train_loss: 0.706643 Total_test_loss: 0.676791 Total_BCE_test_loss: 0.565825 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 94 total_train_loss: 0.706737 Total_test_loss: 0.676491 Total_BCE_test_loss: 0.565481 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 95 total_train_loss: 0.708299 Total_test_loss: 0.676718 Total_BCE_test_loss: 0.565749 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 96 total_train_loss: 0.706923 Total_test_loss: 0.676757 Total_BCE_test_loss: 0.565798 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 97 total_train_loss: 0.707444 Total_test_loss: 0.676795 Total_BCE_test_loss: 0.565799 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 98 total_train_loss: 0.707579 Total_test_loss: 0.676203 Total_BCE_test_loss: 0.565248 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 99 total_train_loss: 0.706090 Total_test_loss: 0.677153 Total_BCE_test_loss: 0.566220 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 100 total_train_loss: 0.705705 Total_test_loss: 0.676202 Total_BCE_test_loss: 0.565195 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 101 total_train_loss: 0.706345 Total_test_loss: 0.676026 Total_BCE_test_loss: 0.564973 Total_KLD_test_loss: 0.003798 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 102 total_train_loss: 0.708517 Total_test_loss: 0.676663 Total_BCE_test_loss: 0.565658 Total_KLD_test_loss: 0.003764 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 103 total_train_loss: 0.707064 Total_test_loss: 0.676944 Total_BCE_test_loss: 0.565977 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 104 total_train_loss: 0.705720 Total_test_loss: 0.676923 Total_BCE_test_loss: 0.565971 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 105 total_train_loss: 0.705841 Total_test_loss: 0.677128 Total_BCE_test_loss: 0.566195 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 106 total_train_loss: 0.709043 Total_test_loss: 0.676307 Total_BCE_test_loss: 0.565349 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 107 total_train_loss: 0.705398 Total_test_loss: 0.676763 Total_BCE_test_loss: 0.565840 Total_KLD_test_loss: 0.003668 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 108 total_train_loss: 0.705972 Total_test_loss: 0.676147 Total_BCE_test_loss: 0.565165 Total_KLD_test_loss: 0.003717 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 109 total_train_loss: 0.706077 Total_test_loss: 0.677030 Total_BCE_test_loss: 0.566109 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 110 total_train_loss: 0.706498 Total_test_loss: 0.677289 Total_BCE_test_loss: 0.566360 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 111 total_train_loss: 0.707774 Total_test_loss: 0.676713 Total_BCE_test_loss: 0.565762 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 112 total_train_loss: 0.708031 Total_test_loss: 0.676187 Total_BCE_test_loss: 0.565236 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 113 total_train_loss: 0.707308 Total_test_loss: 0.675842 Total_BCE_test_loss: 0.564833 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 114 total_train_loss: 0.706314 Total_test_loss: 0.676511 Total_BCE_test_loss: 0.565542 Total_KLD_test_loss: 0.003734 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 115 total_train_loss: 0.706117 Total_test_loss: 0.676396 Total_BCE_test_loss: 0.565423 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 116 total_train_loss: 0.706481 Total_test_loss: 0.676640 Total_BCE_test_loss: 0.565693 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 117 total_train_loss: 0.704803 Total_test_loss: 0.676323 Total_BCE_test_loss: 0.565358 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 118 total_train_loss: 0.706037 Total_test_loss: 0.676546 Total_BCE_test_loss: 0.565574 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 119 total_train_loss: 0.706617 Total_test_loss: 0.676316 Total_BCE_test_loss: 0.565316 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 120 total_train_loss: 0.706621 Total_test_loss: 0.676889 Total_BCE_test_loss: 0.565952 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 121 total_train_loss: 0.704619 Total_test_loss: 0.677154 Total_BCE_test_loss: 0.566199 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 122 total_train_loss: 0.706621 Total_test_loss: 0.676926 Total_BCE_test_loss: 0.565984 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 123 total_train_loss: 0.705512 Total_test_loss: 0.677228 Total_BCE_test_loss: 0.566315 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 124 total_train_loss: 0.707593 Total_test_loss: 0.676978 Total_BCE_test_loss: 0.566045 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 125 total_train_loss: 0.706909 Total_test_loss: 0.677101 Total_BCE_test_loss: 0.566175 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 126 total_train_loss: 0.707281 Total_test_loss: 0.676832 Total_BCE_test_loss: 0.565913 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107228\n",
      "====> Epoch: 127 total_train_loss: 0.705340 Total_test_loss: 0.676844 Total_BCE_test_loss: 0.565928 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 128 total_train_loss: 0.705946 Total_test_loss: 0.676856 Total_BCE_test_loss: 0.565924 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 129 total_train_loss: 0.703200 Total_test_loss: 0.676991 Total_BCE_test_loss: 0.566052 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 130 total_train_loss: 0.706833 Total_test_loss: 0.676490 Total_BCE_test_loss: 0.565545 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 131 total_train_loss: 0.707746 Total_test_loss: 0.676711 Total_BCE_test_loss: 0.565772 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 132 total_train_loss: 0.705865 Total_test_loss: 0.676804 Total_BCE_test_loss: 0.565849 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 133 total_train_loss: 0.703920 Total_test_loss: 0.676437 Total_BCE_test_loss: 0.565456 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 134 total_train_loss: 0.706876 Total_test_loss: 0.676047 Total_BCE_test_loss: 0.565046 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107296\n",
      "====> Epoch: 135 total_train_loss: 0.704440 Total_test_loss: 0.676200 Total_BCE_test_loss: 0.565243 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 136 total_train_loss: 0.707360 Total_test_loss: 0.676584 Total_BCE_test_loss: 0.565624 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 137 total_train_loss: 0.705746 Total_test_loss: 0.676232 Total_BCE_test_loss: 0.565244 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 138 total_train_loss: 0.706870 Total_test_loss: 0.676582 Total_BCE_test_loss: 0.565631 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 139 total_train_loss: 0.705559 Total_test_loss: 0.676855 Total_BCE_test_loss: 0.565905 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 140 total_train_loss: 0.708033 Total_test_loss: 0.676630 Total_BCE_test_loss: 0.565707 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 141 total_train_loss: 0.704681 Total_test_loss: 0.676848 Total_BCE_test_loss: 0.565897 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 142 total_train_loss: 0.706520 Total_test_loss: 0.676688 Total_BCE_test_loss: 0.565682 Total_KLD_test_loss: 0.003752 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 143 total_train_loss: 0.706642 Total_test_loss: 0.676772 Total_BCE_test_loss: 0.565793 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 144 total_train_loss: 0.706995 Total_test_loss: 0.676289 Total_BCE_test_loss: 0.565296 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 145 total_train_loss: 0.705562 Total_test_loss: 0.676123 Total_BCE_test_loss: 0.565123 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 146 total_train_loss: 0.705818 Total_test_loss: 0.675987 Total_BCE_test_loss: 0.564970 Total_KLD_test_loss: 0.003756 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 147 total_train_loss: 0.707357 Total_test_loss: 0.676912 Total_BCE_test_loss: 0.565898 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 148 total_train_loss: 0.704795 Total_test_loss: 0.676387 Total_BCE_test_loss: 0.565402 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 149 total_train_loss: 0.707915 Total_test_loss: 0.675904 Total_BCE_test_loss: 0.564901 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 150 total_train_loss: 0.706369 Total_test_loss: 0.676110 Total_BCE_test_loss: 0.565090 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 151 total_train_loss: 0.706211 Total_test_loss: 0.676798 Total_BCE_test_loss: 0.565885 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 152 total_train_loss: 0.706898 Total_test_loss: 0.676210 Total_BCE_test_loss: 0.565262 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 153 total_train_loss: 0.706602 Total_test_loss: 0.675578 Total_BCE_test_loss: 0.564599 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 154 total_train_loss: 0.707340 Total_test_loss: 0.676705 Total_BCE_test_loss: 0.565741 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 155 total_train_loss: 0.705652 Total_test_loss: 0.676508 Total_BCE_test_loss: 0.565540 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 156 total_train_loss: 0.706032 Total_test_loss: 0.676026 Total_BCE_test_loss: 0.565068 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 157 total_train_loss: 0.706797 Total_test_loss: 0.676904 Total_BCE_test_loss: 0.565944 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 158 total_train_loss: 0.708294 Total_test_loss: 0.676400 Total_BCE_test_loss: 0.565458 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 159 total_train_loss: 0.706100 Total_test_loss: 0.676730 Total_BCE_test_loss: 0.565790 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 160 total_train_loss: 0.704944 Total_test_loss: 0.676704 Total_BCE_test_loss: 0.565770 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 161 total_train_loss: 0.704037 Total_test_loss: 0.676944 Total_BCE_test_loss: 0.565985 Total_KLD_test_loss: 0.003676 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 162 total_train_loss: 0.706554 Total_test_loss: 0.676647 Total_BCE_test_loss: 0.565653 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 163 total_train_loss: 0.706852 Total_test_loss: 0.676741 Total_BCE_test_loss: 0.565787 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 164 total_train_loss: 0.705735 Total_test_loss: 0.676626 Total_BCE_test_loss: 0.565690 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 165 total_train_loss: 0.706230 Total_test_loss: 0.676646 Total_BCE_test_loss: 0.565700 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 166 total_train_loss: 0.708215 Total_test_loss: 0.676659 Total_BCE_test_loss: 0.565707 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 167 total_train_loss: 0.707100 Total_test_loss: 0.676358 Total_BCE_test_loss: 0.565421 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 168 total_train_loss: 0.705820 Total_test_loss: 0.676212 Total_BCE_test_loss: 0.565259 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 169 total_train_loss: 0.707694 Total_test_loss: 0.676241 Total_BCE_test_loss: 0.565276 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 170 total_train_loss: 0.706891 Total_test_loss: 0.676029 Total_BCE_test_loss: 0.565075 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 171 total_train_loss: 0.706832 Total_test_loss: 0.675973 Total_BCE_test_loss: 0.565037 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 172 total_train_loss: 0.707189 Total_test_loss: 0.676897 Total_BCE_test_loss: 0.565921 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 173 total_train_loss: 0.706539 Total_test_loss: 0.676842 Total_BCE_test_loss: 0.565841 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 174 total_train_loss: 0.708327 Total_test_loss: 0.677127 Total_BCE_test_loss: 0.566155 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 175 total_train_loss: 0.706900 Total_test_loss: 0.676725 Total_BCE_test_loss: 0.565692 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 176 total_train_loss: 0.706070 Total_test_loss: 0.676681 Total_BCE_test_loss: 0.565655 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 177 total_train_loss: 0.705535 Total_test_loss: 0.676443 Total_BCE_test_loss: 0.565426 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 178 total_train_loss: 0.708034 Total_test_loss: 0.676355 Total_BCE_test_loss: 0.565314 Total_KLD_test_loss: 0.003792 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 179 total_train_loss: 0.705934 Total_test_loss: 0.676581 Total_BCE_test_loss: 0.565609 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 180 total_train_loss: 0.707938 Total_test_loss: 0.676342 Total_BCE_test_loss: 0.565352 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 181 total_train_loss: 0.705906 Total_test_loss: 0.676361 Total_BCE_test_loss: 0.565412 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 182 total_train_loss: 0.706479 Total_test_loss: 0.676294 Total_BCE_test_loss: 0.565350 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 183 total_train_loss: 0.706772 Total_test_loss: 0.676490 Total_BCE_test_loss: 0.565586 Total_KLD_test_loss: 0.003664 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 184 total_train_loss: 0.706118 Total_test_loss: 0.676181 Total_BCE_test_loss: 0.565195 Total_KLD_test_loss: 0.003725 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 185 total_train_loss: 0.706863 Total_test_loss: 0.676677 Total_BCE_test_loss: 0.565726 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 186 total_train_loss: 0.705992 Total_test_loss: 0.676765 Total_BCE_test_loss: 0.565863 Total_KLD_test_loss: 0.003650 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 187 total_train_loss: 0.707342 Total_test_loss: 0.676152 Total_BCE_test_loss: 0.565231 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 188 total_train_loss: 0.707996 Total_test_loss: 0.676212 Total_BCE_test_loss: 0.565201 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 189 total_train_loss: 0.704685 Total_test_loss: 0.676619 Total_BCE_test_loss: 0.565657 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 190 total_train_loss: 0.705366 Total_test_loss: 0.676853 Total_BCE_test_loss: 0.565909 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 191 total_train_loss: 0.706558 Total_test_loss: 0.676952 Total_BCE_test_loss: 0.566036 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 192 total_train_loss: 0.706171 Total_test_loss: 0.676700 Total_BCE_test_loss: 0.565712 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 193 total_train_loss: 0.705855 Total_test_loss: 0.676639 Total_BCE_test_loss: 0.565631 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 194 total_train_loss: 0.706565 Total_test_loss: 0.676421 Total_BCE_test_loss: 0.565410 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 195 total_train_loss: 0.707774 Total_test_loss: 0.676598 Total_BCE_test_loss: 0.565593 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 196 total_train_loss: 0.706945 Total_test_loss: 0.676643 Total_BCE_test_loss: 0.565658 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 197 total_train_loss: 0.706492 Total_test_loss: 0.676469 Total_BCE_test_loss: 0.565544 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 198 total_train_loss: 0.707569 Total_test_loss: 0.676483 Total_BCE_test_loss: 0.565545 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 199 total_train_loss: 0.705259 Total_test_loss: 0.676822 Total_BCE_test_loss: 0.565825 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 200 total_train_loss: 0.705636 Total_test_loss: 0.676110 Total_BCE_test_loss: 0.565094 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 201 total_train_loss: 0.705657 Total_test_loss: 0.676238 Total_BCE_test_loss: 0.565252 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 202 total_train_loss: 0.709268 Total_test_loss: 0.676561 Total_BCE_test_loss: 0.565605 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 203 total_train_loss: 0.705706 Total_test_loss: 0.676966 Total_BCE_test_loss: 0.566035 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 204 total_train_loss: 0.705467 Total_test_loss: 0.676545 Total_BCE_test_loss: 0.565573 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 205 total_train_loss: 0.707869 Total_test_loss: 0.676738 Total_BCE_test_loss: 0.565726 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 206 total_train_loss: 0.705787 Total_test_loss: 0.676570 Total_BCE_test_loss: 0.565588 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 207 total_train_loss: 0.707572 Total_test_loss: 0.676030 Total_BCE_test_loss: 0.565086 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 208 total_train_loss: 0.705206 Total_test_loss: 0.676624 Total_BCE_test_loss: 0.565683 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 209 total_train_loss: 0.707967 Total_test_loss: 0.676754 Total_BCE_test_loss: 0.565770 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 210 total_train_loss: 0.708392 Total_test_loss: 0.676645 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003661 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 211 total_train_loss: 0.706528 Total_test_loss: 0.676506 Total_BCE_test_loss: 0.565550 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 212 total_train_loss: 0.704773 Total_test_loss: 0.676664 Total_BCE_test_loss: 0.565711 Total_KLD_test_loss: 0.003678 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 213 total_train_loss: 0.706369 Total_test_loss: 0.676259 Total_BCE_test_loss: 0.565324 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107232\n",
      "====> Epoch: 214 total_train_loss: 0.706091 Total_test_loss: 0.676281 Total_BCE_test_loss: 0.565317 Total_KLD_test_loss: 0.003680 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 215 total_train_loss: 0.705469 Total_test_loss: 0.676563 Total_BCE_test_loss: 0.565553 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 216 total_train_loss: 0.707594 Total_test_loss: 0.677087 Total_BCE_test_loss: 0.566171 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 217 total_train_loss: 0.705481 Total_test_loss: 0.677177 Total_BCE_test_loss: 0.566271 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 218 total_train_loss: 0.706416 Total_test_loss: 0.677476 Total_BCE_test_loss: 0.566515 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 219 total_train_loss: 0.707956 Total_test_loss: 0.676215 Total_BCE_test_loss: 0.565230 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 220 total_train_loss: 0.708473 Total_test_loss: 0.676639 Total_BCE_test_loss: 0.565684 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 221 total_train_loss: 0.706357 Total_test_loss: 0.676272 Total_BCE_test_loss: 0.565321 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 222 total_train_loss: 0.708368 Total_test_loss: 0.676227 Total_BCE_test_loss: 0.565256 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 223 total_train_loss: 0.706431 Total_test_loss: 0.676388 Total_BCE_test_loss: 0.565399 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 224 total_train_loss: 0.705085 Total_test_loss: 0.676125 Total_BCE_test_loss: 0.565143 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107284\n",
      "====> Epoch: 225 total_train_loss: 0.709800 Total_test_loss: 0.675693 Total_BCE_test_loss: 0.564737 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 226 total_train_loss: 0.705663 Total_test_loss: 0.676036 Total_BCE_test_loss: 0.565014 Total_KLD_test_loss: 0.003773 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 227 total_train_loss: 0.706075 Total_test_loss: 0.675909 Total_BCE_test_loss: 0.564887 Total_KLD_test_loss: 0.003754 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 228 total_train_loss: 0.708811 Total_test_loss: 0.675501 Total_BCE_test_loss: 0.564520 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107233\n",
      "====> Epoch: 229 total_train_loss: 0.706774 Total_test_loss: 0.675523 Total_BCE_test_loss: 0.564504 Total_KLD_test_loss: 0.003762 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 230 total_train_loss: 0.708684 Total_test_loss: 0.675962 Total_BCE_test_loss: 0.564928 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 231 total_train_loss: 0.705823 Total_test_loss: 0.676489 Total_BCE_test_loss: 0.565490 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 232 total_train_loss: 0.707464 Total_test_loss: 0.676813 Total_BCE_test_loss: 0.565782 Total_KLD_test_loss: 0.003744 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 233 total_train_loss: 0.705201 Total_test_loss: 0.676594 Total_BCE_test_loss: 0.565645 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 234 total_train_loss: 0.706820 Total_test_loss: 0.676150 Total_BCE_test_loss: 0.565148 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 235 total_train_loss: 0.706385 Total_test_loss: 0.676180 Total_BCE_test_loss: 0.565226 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 236 total_train_loss: 0.704119 Total_test_loss: 0.676582 Total_BCE_test_loss: 0.565643 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 237 total_train_loss: 0.705960 Total_test_loss: 0.676874 Total_BCE_test_loss: 0.565951 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 238 total_train_loss: 0.705989 Total_test_loss: 0.676790 Total_BCE_test_loss: 0.565848 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 239 total_train_loss: 0.707463 Total_test_loss: 0.676394 Total_BCE_test_loss: 0.565411 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 240 total_train_loss: 0.706841 Total_test_loss: 0.676869 Total_BCE_test_loss: 0.565936 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 241 total_train_loss: 0.705434 Total_test_loss: 0.676345 Total_BCE_test_loss: 0.565373 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 242 total_train_loss: 0.706428 Total_test_loss: 0.675964 Total_BCE_test_loss: 0.565025 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 243 total_train_loss: 0.707074 Total_test_loss: 0.676394 Total_BCE_test_loss: 0.565464 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 244 total_train_loss: 0.703713 Total_test_loss: 0.676963 Total_BCE_test_loss: 0.566046 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 245 total_train_loss: 0.706890 Total_test_loss: 0.676562 Total_BCE_test_loss: 0.565613 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107244\n",
      "====> Epoch: 246 total_train_loss: 0.707591 Total_test_loss: 0.676871 Total_BCE_test_loss: 0.565965 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 247 total_train_loss: 0.704004 Total_test_loss: 0.676684 Total_BCE_test_loss: 0.565694 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 248 total_train_loss: 0.706306 Total_test_loss: 0.675553 Total_BCE_test_loss: 0.564536 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 249 total_train_loss: 0.706167 Total_test_loss: 0.676074 Total_BCE_test_loss: 0.565065 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 250 total_train_loss: 0.708071 Total_test_loss: 0.675903 Total_BCE_test_loss: 0.564908 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 251 total_train_loss: 0.705581 Total_test_loss: 0.676029 Total_BCE_test_loss: 0.565011 Total_KLD_test_loss: 0.003755 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 252 total_train_loss: 0.704952 Total_test_loss: 0.676918 Total_BCE_test_loss: 0.565945 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 253 total_train_loss: 0.707111 Total_test_loss: 0.676473 Total_BCE_test_loss: 0.565444 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 254 total_train_loss: 0.705863 Total_test_loss: 0.676471 Total_BCE_test_loss: 0.565458 Total_KLD_test_loss: 0.003747 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 255 total_train_loss: 0.705825 Total_test_loss: 0.676704 Total_BCE_test_loss: 0.565731 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 256 total_train_loss: 0.705056 Total_test_loss: 0.676068 Total_BCE_test_loss: 0.565052 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 257 total_train_loss: 0.706249 Total_test_loss: 0.676364 Total_BCE_test_loss: 0.565370 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 258 total_train_loss: 0.706868 Total_test_loss: 0.676038 Total_BCE_test_loss: 0.565046 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 259 total_train_loss: 0.705510 Total_test_loss: 0.676378 Total_BCE_test_loss: 0.565358 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107304\n",
      "====> Epoch: 260 total_train_loss: 0.708246 Total_test_loss: 0.676608 Total_BCE_test_loss: 0.565638 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 261 total_train_loss: 0.705850 Total_test_loss: 0.677246 Total_BCE_test_loss: 0.566328 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 262 total_train_loss: 0.707871 Total_test_loss: 0.676066 Total_BCE_test_loss: 0.565090 Total_KLD_test_loss: 0.003724 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 263 total_train_loss: 0.705911 Total_test_loss: 0.676143 Total_BCE_test_loss: 0.565167 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 264 total_train_loss: 0.707336 Total_test_loss: 0.676135 Total_BCE_test_loss: 0.565164 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 265 total_train_loss: 0.707564 Total_test_loss: 0.675532 Total_BCE_test_loss: 0.564506 Total_KLD_test_loss: 0.003765 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 266 total_train_loss: 0.704927 Total_test_loss: 0.676069 Total_BCE_test_loss: 0.565048 Total_KLD_test_loss: 0.003748 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 267 total_train_loss: 0.706029 Total_test_loss: 0.676588 Total_BCE_test_loss: 0.565640 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 268 total_train_loss: 0.707797 Total_test_loss: 0.676104 Total_BCE_test_loss: 0.565165 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 269 total_train_loss: 0.704911 Total_test_loss: 0.676225 Total_BCE_test_loss: 0.565226 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 270 total_train_loss: 0.707229 Total_test_loss: 0.676799 Total_BCE_test_loss: 0.565869 Total_KLD_test_loss: 0.003670 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 271 total_train_loss: 0.705647 Total_test_loss: 0.676882 Total_BCE_test_loss: 0.565961 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 272 total_train_loss: 0.707047 Total_test_loss: 0.676190 Total_BCE_test_loss: 0.565243 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 273 total_train_loss: 0.704854 Total_test_loss: 0.676566 Total_BCE_test_loss: 0.565587 Total_KLD_test_loss: 0.003728 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 274 total_train_loss: 0.707591 Total_test_loss: 0.676336 Total_BCE_test_loss: 0.565325 Total_KLD_test_loss: 0.003720 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 275 total_train_loss: 0.706287 Total_test_loss: 0.676610 Total_BCE_test_loss: 0.565605 Total_KLD_test_loss: 0.003727 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 276 total_train_loss: 0.704964 Total_test_loss: 0.676685 Total_BCE_test_loss: 0.565726 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 277 total_train_loss: 0.707050 Total_test_loss: 0.676417 Total_BCE_test_loss: 0.565434 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 278 total_train_loss: 0.706488 Total_test_loss: 0.676355 Total_BCE_test_loss: 0.565388 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 279 total_train_loss: 0.705419 Total_test_loss: 0.676073 Total_BCE_test_loss: 0.565096 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 280 total_train_loss: 0.705657 Total_test_loss: 0.676565 Total_BCE_test_loss: 0.565566 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107294\n",
      "====> Epoch: 281 total_train_loss: 0.706810 Total_test_loss: 0.676478 Total_BCE_test_loss: 0.565504 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 282 total_train_loss: 0.705019 Total_test_loss: 0.676776 Total_BCE_test_loss: 0.565747 Total_KLD_test_loss: 0.003753 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 283 total_train_loss: 0.705559 Total_test_loss: 0.676245 Total_BCE_test_loss: 0.565231 Total_KLD_test_loss: 0.003776 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 284 total_train_loss: 0.705488 Total_test_loss: 0.676624 Total_BCE_test_loss: 0.565665 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 285 total_train_loss: 0.707746 Total_test_loss: 0.676430 Total_BCE_test_loss: 0.565448 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 286 total_train_loss: 0.704361 Total_test_loss: 0.676697 Total_BCE_test_loss: 0.565696 Total_KLD_test_loss: 0.003739 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 287 total_train_loss: 0.705491 Total_test_loss: 0.676232 Total_BCE_test_loss: 0.565227 Total_KLD_test_loss: 0.003749 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 288 total_train_loss: 0.707529 Total_test_loss: 0.676146 Total_BCE_test_loss: 0.565135 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 289 total_train_loss: 0.705111 Total_test_loss: 0.676933 Total_BCE_test_loss: 0.565990 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 290 total_train_loss: 0.708505 Total_test_loss: 0.676315 Total_BCE_test_loss: 0.565282 Total_KLD_test_loss: 0.003770 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 291 total_train_loss: 0.705620 Total_test_loss: 0.676994 Total_BCE_test_loss: 0.565974 Total_KLD_test_loss: 0.003730 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 292 total_train_loss: 0.708466 Total_test_loss: 0.676231 Total_BCE_test_loss: 0.565220 Total_KLD_test_loss: 0.003759 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 293 total_train_loss: 0.710012 Total_test_loss: 0.675768 Total_BCE_test_loss: 0.564784 Total_KLD_test_loss: 0.003746 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 294 total_train_loss: 0.705117 Total_test_loss: 0.676088 Total_BCE_test_loss: 0.565085 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 295 total_train_loss: 0.706047 Total_test_loss: 0.676156 Total_BCE_test_loss: 0.565192 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 296 total_train_loss: 0.705810 Total_test_loss: 0.676975 Total_BCE_test_loss: 0.566010 Total_KLD_test_loss: 0.003675 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 297 total_train_loss: 0.708133 Total_test_loss: 0.676748 Total_BCE_test_loss: 0.565821 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 298 total_train_loss: 0.705789 Total_test_loss: 0.676562 Total_BCE_test_loss: 0.565572 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 299 total_train_loss: 0.706742 Total_test_loss: 0.675656 Total_BCE_test_loss: 0.564655 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107292\n",
      "====> Epoch: 300 total_train_loss: 0.704664 Total_test_loss: 0.675453 Total_BCE_test_loss: 0.564422 Total_KLD_test_loss: 0.003779 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 301 total_train_loss: 0.705903 Total_test_loss: 0.676152 Total_BCE_test_loss: 0.565227 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 302 total_train_loss: 0.706187 Total_test_loss: 0.675724 Total_BCE_test_loss: 0.564725 Total_KLD_test_loss: 0.003725 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 303 total_train_loss: 0.705561 Total_test_loss: 0.676293 Total_BCE_test_loss: 0.565325 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 304 total_train_loss: 0.707052 Total_test_loss: 0.675984 Total_BCE_test_loss: 0.564991 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 305 total_train_loss: 0.706146 Total_test_loss: 0.676057 Total_BCE_test_loss: 0.565116 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 306 total_train_loss: 0.708682 Total_test_loss: 0.676434 Total_BCE_test_loss: 0.565503 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 307 total_train_loss: 0.706491 Total_test_loss: 0.676198 Total_BCE_test_loss: 0.565233 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 308 total_train_loss: 0.705308 Total_test_loss: 0.676388 Total_BCE_test_loss: 0.565447 Total_KLD_test_loss: 0.003658 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 309 total_train_loss: 0.706276 Total_test_loss: 0.676867 Total_BCE_test_loss: 0.565969 Total_KLD_test_loss: 0.003643 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 310 total_train_loss: 0.705861 Total_test_loss: 0.676395 Total_BCE_test_loss: 0.565445 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 311 total_train_loss: 0.707163 Total_test_loss: 0.676423 Total_BCE_test_loss: 0.565416 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 312 total_train_loss: 0.705686 Total_test_loss: 0.676531 Total_BCE_test_loss: 0.565569 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 313 total_train_loss: 0.706400 Total_test_loss: 0.676356 Total_BCE_test_loss: 0.565369 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 314 total_train_loss: 0.705545 Total_test_loss: 0.677083 Total_BCE_test_loss: 0.566123 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 315 total_train_loss: 0.706865 Total_test_loss: 0.676643 Total_BCE_test_loss: 0.565695 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 316 total_train_loss: 0.705240 Total_test_loss: 0.677023 Total_BCE_test_loss: 0.566098 Total_KLD_test_loss: 0.003651 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 317 total_train_loss: 0.705777 Total_test_loss: 0.676584 Total_BCE_test_loss: 0.565631 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 318 total_train_loss: 0.705401 Total_test_loss: 0.676645 Total_BCE_test_loss: 0.565723 Total_KLD_test_loss: 0.003654 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 319 total_train_loss: 0.706239 Total_test_loss: 0.676608 Total_BCE_test_loss: 0.565651 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 320 total_train_loss: 0.707399 Total_test_loss: 0.676941 Total_BCE_test_loss: 0.565996 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 321 total_train_loss: 0.707720 Total_test_loss: 0.676482 Total_BCE_test_loss: 0.565518 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 322 total_train_loss: 0.709546 Total_test_loss: 0.675544 Total_BCE_test_loss: 0.564514 Total_KLD_test_loss: 0.003760 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 323 total_train_loss: 0.705761 Total_test_loss: 0.676391 Total_BCE_test_loss: 0.565441 Total_KLD_test_loss: 0.003683 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 324 total_train_loss: 0.706285 Total_test_loss: 0.676761 Total_BCE_test_loss: 0.565809 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 325 total_train_loss: 0.705200 Total_test_loss: 0.676546 Total_BCE_test_loss: 0.565601 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 326 total_train_loss: 0.706638 Total_test_loss: 0.676053 Total_BCE_test_loss: 0.565109 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 327 total_train_loss: 0.706104 Total_test_loss: 0.676001 Total_BCE_test_loss: 0.565039 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 328 total_train_loss: 0.707776 Total_test_loss: 0.676041 Total_BCE_test_loss: 0.565080 Total_KLD_test_loss: 0.003707 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 329 total_train_loss: 0.707715 Total_test_loss: 0.675724 Total_BCE_test_loss: 0.564718 Total_KLD_test_loss: 0.003750 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 330 total_train_loss: 0.707875 Total_test_loss: 0.675807 Total_BCE_test_loss: 0.564808 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 331 total_train_loss: 0.704913 Total_test_loss: 0.675635 Total_BCE_test_loss: 0.564658 Total_KLD_test_loss: 0.003702 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 332 total_train_loss: 0.707889 Total_test_loss: 0.676154 Total_BCE_test_loss: 0.565091 Total_KLD_test_loss: 0.003784 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 333 total_train_loss: 0.705555 Total_test_loss: 0.676211 Total_BCE_test_loss: 0.565216 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 334 total_train_loss: 0.708417 Total_test_loss: 0.675934 Total_BCE_test_loss: 0.564871 Total_KLD_test_loss: 0.003799 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 335 total_train_loss: 0.706498 Total_test_loss: 0.675833 Total_BCE_test_loss: 0.564850 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 336 total_train_loss: 0.707910 Total_test_loss: 0.675675 Total_BCE_test_loss: 0.564671 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 337 total_train_loss: 0.708177 Total_test_loss: 0.676085 Total_BCE_test_loss: 0.565119 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 338 total_train_loss: 0.707556 Total_test_loss: 0.676677 Total_BCE_test_loss: 0.565731 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 339 total_train_loss: 0.708682 Total_test_loss: 0.676544 Total_BCE_test_loss: 0.565562 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 340 total_train_loss: 0.704224 Total_test_loss: 0.676958 Total_BCE_test_loss: 0.566024 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107242\n",
      "====> Epoch: 341 total_train_loss: 0.706783 Total_test_loss: 0.676930 Total_BCE_test_loss: 0.565994 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 342 total_train_loss: 0.706292 Total_test_loss: 0.676997 Total_BCE_test_loss: 0.566076 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 343 total_train_loss: 0.706940 Total_test_loss: 0.676936 Total_BCE_test_loss: 0.565990 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 344 total_train_loss: 0.706555 Total_test_loss: 0.676573 Total_BCE_test_loss: 0.565613 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 345 total_train_loss: 0.706532 Total_test_loss: 0.676561 Total_BCE_test_loss: 0.565593 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 346 total_train_loss: 0.708862 Total_test_loss: 0.676393 Total_BCE_test_loss: 0.565477 Total_KLD_test_loss: 0.003662 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 347 total_train_loss: 0.706859 Total_test_loss: 0.676294 Total_BCE_test_loss: 0.565373 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 348 total_train_loss: 0.704519 Total_test_loss: 0.675524 Total_BCE_test_loss: 0.564554 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 349 total_train_loss: 0.704658 Total_test_loss: 0.675940 Total_BCE_test_loss: 0.564992 Total_KLD_test_loss: 0.003695 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 350 total_train_loss: 0.703939 Total_test_loss: 0.676490 Total_BCE_test_loss: 0.565575 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 351 total_train_loss: 0.706959 Total_test_loss: 0.677658 Total_BCE_test_loss: 0.566752 Total_KLD_test_loss: 0.003648 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 352 total_train_loss: 0.706949 Total_test_loss: 0.676832 Total_BCE_test_loss: 0.565847 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 353 total_train_loss: 0.707714 Total_test_loss: 0.676777 Total_BCE_test_loss: 0.565864 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107210\n",
      "====> Epoch: 354 total_train_loss: 0.706807 Total_test_loss: 0.676855 Total_BCE_test_loss: 0.565904 Total_KLD_test_loss: 0.003685 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 355 total_train_loss: 0.707002 Total_test_loss: 0.676736 Total_BCE_test_loss: 0.565798 Total_KLD_test_loss: 0.003671 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 356 total_train_loss: 0.705980 Total_test_loss: 0.677074 Total_BCE_test_loss: 0.566150 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 357 total_train_loss: 0.707184 Total_test_loss: 0.676266 Total_BCE_test_loss: 0.565296 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107238\n",
      "====> Epoch: 358 total_train_loss: 0.705969 Total_test_loss: 0.676759 Total_BCE_test_loss: 0.565845 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 359 total_train_loss: 0.706936 Total_test_loss: 0.676761 Total_BCE_test_loss: 0.565807 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107288\n",
      "====> Epoch: 360 total_train_loss: 0.708206 Total_test_loss: 0.676280 Total_BCE_test_loss: 0.565288 Total_KLD_test_loss: 0.003718 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 361 total_train_loss: 0.704883 Total_test_loss: 0.676429 Total_BCE_test_loss: 0.565469 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 362 total_train_loss: 0.709371 Total_test_loss: 0.676324 Total_BCE_test_loss: 0.565322 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 363 total_train_loss: 0.707484 Total_test_loss: 0.675667 Total_BCE_test_loss: 0.564698 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 364 total_train_loss: 0.706308 Total_test_loss: 0.676829 Total_BCE_test_loss: 0.565844 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 365 total_train_loss: 0.704829 Total_test_loss: 0.677353 Total_BCE_test_loss: 0.566399 Total_KLD_test_loss: 0.003673 Total_CEP_test_loss: 0.107282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 366 total_train_loss: 0.704998 Total_test_loss: 0.677653 Total_BCE_test_loss: 0.566720 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 367 total_train_loss: 0.706850 Total_test_loss: 0.677407 Total_BCE_test_loss: 0.566468 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 368 total_train_loss: 0.705889 Total_test_loss: 0.676639 Total_BCE_test_loss: 0.565710 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107241\n",
      "====> Epoch: 369 total_train_loss: 0.706941 Total_test_loss: 0.676402 Total_BCE_test_loss: 0.565452 Total_KLD_test_loss: 0.003682 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 370 total_train_loss: 0.704900 Total_test_loss: 0.675946 Total_BCE_test_loss: 0.564969 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 371 total_train_loss: 0.705833 Total_test_loss: 0.676193 Total_BCE_test_loss: 0.565202 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 372 total_train_loss: 0.706379 Total_test_loss: 0.676059 Total_BCE_test_loss: 0.565097 Total_KLD_test_loss: 0.003701 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 373 total_train_loss: 0.706432 Total_test_loss: 0.676857 Total_BCE_test_loss: 0.565917 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 374 total_train_loss: 0.706794 Total_test_loss: 0.676169 Total_BCE_test_loss: 0.565160 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 375 total_train_loss: 0.704979 Total_test_loss: 0.676171 Total_BCE_test_loss: 0.565146 Total_KLD_test_loss: 0.003743 Total_CEP_test_loss: 0.107282\n",
      "====> Epoch: 376 total_train_loss: 0.705196 Total_test_loss: 0.677383 Total_BCE_test_loss: 0.566478 Total_KLD_test_loss: 0.003652 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 377 total_train_loss: 0.704763 Total_test_loss: 0.677086 Total_BCE_test_loss: 0.566167 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107253\n",
      "====> Epoch: 378 total_train_loss: 0.704450 Total_test_loss: 0.676343 Total_BCE_test_loss: 0.565423 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107236\n",
      "====> Epoch: 379 total_train_loss: 0.706168 Total_test_loss: 0.676230 Total_BCE_test_loss: 0.565253 Total_KLD_test_loss: 0.003705 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 380 total_train_loss: 0.706589 Total_test_loss: 0.675714 Total_BCE_test_loss: 0.564745 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 381 total_train_loss: 0.708424 Total_test_loss: 0.676018 Total_BCE_test_loss: 0.565089 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 382 total_train_loss: 0.706675 Total_test_loss: 0.676396 Total_BCE_test_loss: 0.565411 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 383 total_train_loss: 0.706660 Total_test_loss: 0.675930 Total_BCE_test_loss: 0.564940 Total_KLD_test_loss: 0.003734 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 384 total_train_loss: 0.707934 Total_test_loss: 0.675965 Total_BCE_test_loss: 0.564960 Total_KLD_test_loss: 0.003760 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 385 total_train_loss: 0.705758 Total_test_loss: 0.676372 Total_BCE_test_loss: 0.565359 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 386 total_train_loss: 0.706321 Total_test_loss: 0.675951 Total_BCE_test_loss: 0.564935 Total_KLD_test_loss: 0.003760 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 387 total_train_loss: 0.708529 Total_test_loss: 0.676178 Total_BCE_test_loss: 0.565122 Total_KLD_test_loss: 0.003787 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 388 total_train_loss: 0.706200 Total_test_loss: 0.676592 Total_BCE_test_loss: 0.565618 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 389 total_train_loss: 0.706908 Total_test_loss: 0.677070 Total_BCE_test_loss: 0.566135 Total_KLD_test_loss: 0.003653 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 390 total_train_loss: 0.704476 Total_test_loss: 0.677452 Total_BCE_test_loss: 0.566570 Total_KLD_test_loss: 0.003614 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 391 total_train_loss: 0.707624 Total_test_loss: 0.677354 Total_BCE_test_loss: 0.566436 Total_KLD_test_loss: 0.003659 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 392 total_train_loss: 0.706854 Total_test_loss: 0.676783 Total_BCE_test_loss: 0.565846 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 393 total_train_loss: 0.707065 Total_test_loss: 0.676776 Total_BCE_test_loss: 0.565861 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107243\n",
      "====> Epoch: 394 total_train_loss: 0.706898 Total_test_loss: 0.676456 Total_BCE_test_loss: 0.565492 Total_KLD_test_loss: 0.003697 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 395 total_train_loss: 0.707808 Total_test_loss: 0.676881 Total_BCE_test_loss: 0.565904 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 396 total_train_loss: 0.705373 Total_test_loss: 0.675992 Total_BCE_test_loss: 0.564980 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 397 total_train_loss: 0.703777 Total_test_loss: 0.676167 Total_BCE_test_loss: 0.565193 Total_KLD_test_loss: 0.003711 Total_CEP_test_loss: 0.107263\n",
      "====> Epoch: 398 total_train_loss: 0.703365 Total_test_loss: 0.676277 Total_BCE_test_loss: 0.565303 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 399 total_train_loss: 0.708130 Total_test_loss: 0.675881 Total_BCE_test_loss: 0.564877 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 400 total_train_loss: 0.706799 Total_test_loss: 0.676258 Total_BCE_test_loss: 0.565270 Total_KLD_test_loss: 0.003739 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 401 total_train_loss: 0.705695 Total_test_loss: 0.676979 Total_BCE_test_loss: 0.566051 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107247\n",
      "====> Epoch: 402 total_train_loss: 0.706932 Total_test_loss: 0.676633 Total_BCE_test_loss: 0.565663 Total_KLD_test_loss: 0.003688 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 403 total_train_loss: 0.706251 Total_test_loss: 0.676876 Total_BCE_test_loss: 0.565916 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107269\n",
      "====> Epoch: 404 total_train_loss: 0.705452 Total_test_loss: 0.676598 Total_BCE_test_loss: 0.565632 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 405 total_train_loss: 0.706409 Total_test_loss: 0.676141 Total_BCE_test_loss: 0.565163 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 406 total_train_loss: 0.706304 Total_test_loss: 0.676177 Total_BCE_test_loss: 0.565196 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 407 total_train_loss: 0.706255 Total_test_loss: 0.676596 Total_BCE_test_loss: 0.565676 Total_KLD_test_loss: 0.003638 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 408 total_train_loss: 0.706254 Total_test_loss: 0.676740 Total_BCE_test_loss: 0.565760 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107289\n",
      "====> Epoch: 409 total_train_loss: 0.704839 Total_test_loss: 0.676729 Total_BCE_test_loss: 0.565779 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 410 total_train_loss: 0.709462 Total_test_loss: 0.675892 Total_BCE_test_loss: 0.564908 Total_KLD_test_loss: 0.003719 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 411 total_train_loss: 0.705898 Total_test_loss: 0.676110 Total_BCE_test_loss: 0.565110 Total_KLD_test_loss: 0.003714 Total_CEP_test_loss: 0.107287\n",
      "====> Epoch: 412 total_train_loss: 0.707001 Total_test_loss: 0.676678 Total_BCE_test_loss: 0.565701 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 413 total_train_loss: 0.706766 Total_test_loss: 0.675855 Total_BCE_test_loss: 0.564806 Total_KLD_test_loss: 0.003766 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 414 total_train_loss: 0.708366 Total_test_loss: 0.676369 Total_BCE_test_loss: 0.565394 Total_KLD_test_loss: 0.003716 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 415 total_train_loss: 0.708412 Total_test_loss: 0.676276 Total_BCE_test_loss: 0.565304 Total_KLD_test_loss: 0.003712 Total_CEP_test_loss: 0.107260\n",
      "====> Epoch: 416 total_train_loss: 0.704534 Total_test_loss: 0.676335 Total_BCE_test_loss: 0.565415 Total_KLD_test_loss: 0.003664 Total_CEP_test_loss: 0.107255\n",
      "====> Epoch: 417 total_train_loss: 0.705396 Total_test_loss: 0.676493 Total_BCE_test_loss: 0.565569 Total_KLD_test_loss: 0.003637 Total_CEP_test_loss: 0.107288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 418 total_train_loss: 0.706342 Total_test_loss: 0.676847 Total_BCE_test_loss: 0.565927 Total_KLD_test_loss: 0.003642 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 419 total_train_loss: 0.706462 Total_test_loss: 0.676244 Total_BCE_test_loss: 0.565296 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 420 total_train_loss: 0.706160 Total_test_loss: 0.676388 Total_BCE_test_loss: 0.565449 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 421 total_train_loss: 0.706149 Total_test_loss: 0.676292 Total_BCE_test_loss: 0.565332 Total_KLD_test_loss: 0.003713 Total_CEP_test_loss: 0.107246\n",
      "====> Epoch: 422 total_train_loss: 0.707110 Total_test_loss: 0.676374 Total_BCE_test_loss: 0.565427 Total_KLD_test_loss: 0.003672 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 423 total_train_loss: 0.707126 Total_test_loss: 0.676243 Total_BCE_test_loss: 0.565291 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107249\n",
      "====> Epoch: 424 total_train_loss: 0.707075 Total_test_loss: 0.676529 Total_BCE_test_loss: 0.565582 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 425 total_train_loss: 0.708717 Total_test_loss: 0.676184 Total_BCE_test_loss: 0.565190 Total_KLD_test_loss: 0.003715 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 426 total_train_loss: 0.706631 Total_test_loss: 0.675834 Total_BCE_test_loss: 0.564839 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 427 total_train_loss: 0.705897 Total_test_loss: 0.676311 Total_BCE_test_loss: 0.565293 Total_KLD_test_loss: 0.003743 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 428 total_train_loss: 0.707770 Total_test_loss: 0.676644 Total_BCE_test_loss: 0.565651 Total_KLD_test_loss: 0.003735 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 429 total_train_loss: 0.706171 Total_test_loss: 0.676813 Total_BCE_test_loss: 0.565889 Total_KLD_test_loss: 0.003666 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 430 total_train_loss: 0.704959 Total_test_loss: 0.676177 Total_BCE_test_loss: 0.565178 Total_KLD_test_loss: 0.003760 Total_CEP_test_loss: 0.107239\n",
      "====> Epoch: 431 total_train_loss: 0.705216 Total_test_loss: 0.676126 Total_BCE_test_loss: 0.565102 Total_KLD_test_loss: 0.003762 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 432 total_train_loss: 0.705968 Total_test_loss: 0.676900 Total_BCE_test_loss: 0.565956 Total_KLD_test_loss: 0.003679 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 433 total_train_loss: 0.706032 Total_test_loss: 0.676873 Total_BCE_test_loss: 0.565933 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 434 total_train_loss: 0.705925 Total_test_loss: 0.676243 Total_BCE_test_loss: 0.565270 Total_KLD_test_loss: 0.003719 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 435 total_train_loss: 0.706235 Total_test_loss: 0.676542 Total_BCE_test_loss: 0.565552 Total_KLD_test_loss: 0.003739 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 436 total_train_loss: 0.706358 Total_test_loss: 0.677093 Total_BCE_test_loss: 0.566147 Total_KLD_test_loss: 0.003692 Total_CEP_test_loss: 0.107254\n",
      "====> Epoch: 437 total_train_loss: 0.707495 Total_test_loss: 0.676208 Total_BCE_test_loss: 0.565211 Total_KLD_test_loss: 0.003731 Total_CEP_test_loss: 0.107266\n",
      "====> Epoch: 438 total_train_loss: 0.706061 Total_test_loss: 0.676420 Total_BCE_test_loss: 0.565458 Total_KLD_test_loss: 0.003690 Total_CEP_test_loss: 0.107272\n",
      "====> Epoch: 439 total_train_loss: 0.704113 Total_test_loss: 0.677048 Total_BCE_test_loss: 0.566106 Total_KLD_test_loss: 0.003665 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 440 total_train_loss: 0.708962 Total_test_loss: 0.676516 Total_BCE_test_loss: 0.565547 Total_KLD_test_loss: 0.003696 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 441 total_train_loss: 0.706437 Total_test_loss: 0.677029 Total_BCE_test_loss: 0.566066 Total_KLD_test_loss: 0.003684 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 442 total_train_loss: 0.707158 Total_test_loss: 0.676126 Total_BCE_test_loss: 0.565143 Total_KLD_test_loss: 0.003733 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 443 total_train_loss: 0.707758 Total_test_loss: 0.676566 Total_BCE_test_loss: 0.565605 Total_KLD_test_loss: 0.003736 Total_CEP_test_loss: 0.107224\n",
      "====> Epoch: 444 total_train_loss: 0.706157 Total_test_loss: 0.676426 Total_BCE_test_loss: 0.565468 Total_KLD_test_loss: 0.003700 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 445 total_train_loss: 0.706119 Total_test_loss: 0.676628 Total_BCE_test_loss: 0.565649 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 446 total_train_loss: 0.706957 Total_test_loss: 0.675887 Total_BCE_test_loss: 0.564849 Total_KLD_test_loss: 0.003753 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 447 total_train_loss: 0.708571 Total_test_loss: 0.676114 Total_BCE_test_loss: 0.565129 Total_KLD_test_loss: 0.003704 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 448 total_train_loss: 0.706545 Total_test_loss: 0.676497 Total_BCE_test_loss: 0.565558 Total_KLD_test_loss: 0.003657 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 449 total_train_loss: 0.705538 Total_test_loss: 0.676447 Total_BCE_test_loss: 0.565480 Total_KLD_test_loss: 0.003687 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 450 total_train_loss: 0.710092 Total_test_loss: 0.675422 Total_BCE_test_loss: 0.564353 Total_KLD_test_loss: 0.003807 Total_CEP_test_loss: 0.107262\n",
      "====> Epoch: 451 total_train_loss: 0.706520 Total_test_loss: 0.676081 Total_BCE_test_loss: 0.565074 Total_KLD_test_loss: 0.003751 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 452 total_train_loss: 0.707090 Total_test_loss: 0.676323 Total_BCE_test_loss: 0.565343 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107259\n",
      "====> Epoch: 453 total_train_loss: 0.704867 Total_test_loss: 0.675832 Total_BCE_test_loss: 0.564806 Total_KLD_test_loss: 0.003745 Total_CEP_test_loss: 0.107281\n",
      "====> Epoch: 454 total_train_loss: 0.705999 Total_test_loss: 0.676195 Total_BCE_test_loss: 0.565243 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107265\n",
      "====> Epoch: 455 total_train_loss: 0.707883 Total_test_loss: 0.676142 Total_BCE_test_loss: 0.565168 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 456 total_train_loss: 0.707674 Total_test_loss: 0.675947 Total_BCE_test_loss: 0.564968 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107271\n",
      "====> Epoch: 457 total_train_loss: 0.708064 Total_test_loss: 0.676644 Total_BCE_test_loss: 0.565672 Total_KLD_test_loss: 0.003698 Total_CEP_test_loss: 0.107274\n",
      "====> Epoch: 458 total_train_loss: 0.707346 Total_test_loss: 0.676349 Total_BCE_test_loss: 0.565342 Total_KLD_test_loss: 0.003737 Total_CEP_test_loss: 0.107270\n",
      "====> Epoch: 459 total_train_loss: 0.706524 Total_test_loss: 0.675760 Total_BCE_test_loss: 0.564742 Total_KLD_test_loss: 0.003740 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 460 total_train_loss: 0.704403 Total_test_loss: 0.676484 Total_BCE_test_loss: 0.565480 Total_KLD_test_loss: 0.003726 Total_CEP_test_loss: 0.107277\n",
      "====> Epoch: 461 total_train_loss: 0.706332 Total_test_loss: 0.676161 Total_BCE_test_loss: 0.565121 Total_KLD_test_loss: 0.003761 Total_CEP_test_loss: 0.107279\n",
      "====> Epoch: 462 total_train_loss: 0.707576 Total_test_loss: 0.675705 Total_BCE_test_loss: 0.564634 Total_KLD_test_loss: 0.003796 Total_CEP_test_loss: 0.107276\n",
      "====> Epoch: 463 total_train_loss: 0.707570 Total_test_loss: 0.675809 Total_BCE_test_loss: 0.564777 Total_KLD_test_loss: 0.003768 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 464 total_train_loss: 0.708263 Total_test_loss: 0.675979 Total_BCE_test_loss: 0.564951 Total_KLD_test_loss: 0.003766 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 465 total_train_loss: 0.706665 Total_test_loss: 0.675682 Total_BCE_test_loss: 0.564624 Total_KLD_test_loss: 0.003806 Total_CEP_test_loss: 0.107252\n",
      "====> Epoch: 466 total_train_loss: 0.706086 Total_test_loss: 0.676873 Total_BCE_test_loss: 0.565860 Total_KLD_test_loss: 0.003756 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 467 total_train_loss: 0.706625 Total_test_loss: 0.675755 Total_BCE_test_loss: 0.564669 Total_KLD_test_loss: 0.003838 Total_CEP_test_loss: 0.107248\n",
      "====> Epoch: 468 total_train_loss: 0.706741 Total_test_loss: 0.676434 Total_BCE_test_loss: 0.565463 Total_KLD_test_loss: 0.003693 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 469 total_train_loss: 0.704675 Total_test_loss: 0.676758 Total_BCE_test_loss: 0.565791 Total_KLD_test_loss: 0.003709 Total_CEP_test_loss: 0.107258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 470 total_train_loss: 0.706965 Total_test_loss: 0.676708 Total_BCE_test_loss: 0.565748 Total_KLD_test_loss: 0.003681 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 471 total_train_loss: 0.707476 Total_test_loss: 0.677002 Total_BCE_test_loss: 0.566079 Total_KLD_test_loss: 0.003667 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 472 total_train_loss: 0.707905 Total_test_loss: 0.677252 Total_BCE_test_loss: 0.566334 Total_KLD_test_loss: 0.003645 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 473 total_train_loss: 0.706341 Total_test_loss: 0.677301 Total_BCE_test_loss: 0.566413 Total_KLD_test_loss: 0.003636 Total_CEP_test_loss: 0.107251\n",
      "====> Epoch: 474 total_train_loss: 0.706319 Total_test_loss: 0.676632 Total_BCE_test_loss: 0.565634 Total_KLD_test_loss: 0.003708 Total_CEP_test_loss: 0.107290\n",
      "====> Epoch: 475 total_train_loss: 0.708510 Total_test_loss: 0.676236 Total_BCE_test_loss: 0.565279 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107235\n",
      "====> Epoch: 476 total_train_loss: 0.705164 Total_test_loss: 0.676283 Total_BCE_test_loss: 0.565266 Total_KLD_test_loss: 0.003732 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 477 total_train_loss: 0.704669 Total_test_loss: 0.676551 Total_BCE_test_loss: 0.565607 Total_KLD_test_loss: 0.003689 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 478 total_train_loss: 0.704626 Total_test_loss: 0.677042 Total_BCE_test_loss: 0.566058 Total_KLD_test_loss: 0.003710 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 479 total_train_loss: 0.705358 Total_test_loss: 0.677195 Total_BCE_test_loss: 0.566218 Total_KLD_test_loss: 0.003674 Total_CEP_test_loss: 0.107302\n",
      "====> Epoch: 480 total_train_loss: 0.706325 Total_test_loss: 0.677075 Total_BCE_test_loss: 0.566122 Total_KLD_test_loss: 0.003686 Total_CEP_test_loss: 0.107267\n",
      "====> Epoch: 481 total_train_loss: 0.708749 Total_test_loss: 0.677282 Total_BCE_test_loss: 0.566320 Total_KLD_test_loss: 0.003677 Total_CEP_test_loss: 0.107285\n",
      "====> Epoch: 482 total_train_loss: 0.706923 Total_test_loss: 0.676410 Total_BCE_test_loss: 0.565364 Total_KLD_test_loss: 0.003768 Total_CEP_test_loss: 0.107278\n",
      "====> Epoch: 483 total_train_loss: 0.706049 Total_test_loss: 0.676168 Total_BCE_test_loss: 0.565177 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 484 total_train_loss: 0.707003 Total_test_loss: 0.676080 Total_BCE_test_loss: 0.565127 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107258\n",
      "====> Epoch: 485 total_train_loss: 0.709280 Total_test_loss: 0.675899 Total_BCE_test_loss: 0.564892 Total_KLD_test_loss: 0.003757 Total_CEP_test_loss: 0.107250\n",
      "====> Epoch: 486 total_train_loss: 0.706037 Total_test_loss: 0.675409 Total_BCE_test_loss: 0.564350 Total_KLD_test_loss: 0.003779 Total_CEP_test_loss: 0.107280\n",
      "====> Epoch: 487 total_train_loss: 0.705272 Total_test_loss: 0.676579 Total_BCE_test_loss: 0.565633 Total_KLD_test_loss: 0.003663 Total_CEP_test_loss: 0.107283\n",
      "====> Epoch: 488 total_train_loss: 0.707130 Total_test_loss: 0.676797 Total_BCE_test_loss: 0.565842 Total_KLD_test_loss: 0.003691 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 489 total_train_loss: 0.706469 Total_test_loss: 0.676264 Total_BCE_test_loss: 0.565265 Total_KLD_test_loss: 0.003742 Total_CEP_test_loss: 0.107256\n",
      "====> Epoch: 490 total_train_loss: 0.706000 Total_test_loss: 0.676837 Total_BCE_test_loss: 0.565902 Total_KLD_test_loss: 0.003660 Total_CEP_test_loss: 0.107275\n",
      "====> Epoch: 491 total_train_loss: 0.705326 Total_test_loss: 0.676831 Total_BCE_test_loss: 0.565912 Total_KLD_test_loss: 0.003650 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 492 total_train_loss: 0.707960 Total_test_loss: 0.677085 Total_BCE_test_loss: 0.566159 Total_KLD_test_loss: 0.003669 Total_CEP_test_loss: 0.107257\n",
      "====> Epoch: 493 total_train_loss: 0.706799 Total_test_loss: 0.676757 Total_BCE_test_loss: 0.565795 Total_KLD_test_loss: 0.003699 Total_CEP_test_loss: 0.107264\n",
      "====> Epoch: 494 total_train_loss: 0.707041 Total_test_loss: 0.676355 Total_BCE_test_loss: 0.565404 Total_KLD_test_loss: 0.003721 Total_CEP_test_loss: 0.107230\n",
      "====> Epoch: 495 total_train_loss: 0.705979 Total_test_loss: 0.676050 Total_BCE_test_loss: 0.565075 Total_KLD_test_loss: 0.003706 Total_CEP_test_loss: 0.107268\n",
      "====> Epoch: 496 total_train_loss: 0.705619 Total_test_loss: 0.675957 Total_BCE_test_loss: 0.565009 Total_KLD_test_loss: 0.003703 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 497 total_train_loss: 0.706711 Total_test_loss: 0.675887 Total_BCE_test_loss: 0.564886 Total_KLD_test_loss: 0.003741 Total_CEP_test_loss: 0.107261\n",
      "====> Epoch: 498 total_train_loss: 0.705273 Total_test_loss: 0.675991 Total_BCE_test_loss: 0.564995 Total_KLD_test_loss: 0.003723 Total_CEP_test_loss: 0.107273\n",
      "====> Epoch: 499 total_train_loss: 0.705198 Total_test_loss: 0.676436 Total_BCE_test_loss: 0.565497 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107245\n",
      "====> Epoch: 500 total_train_loss: 0.705518 Total_test_loss: 0.676077 Total_BCE_test_loss: 0.565118 Total_KLD_test_loss: 0.003694 Total_CEP_test_loss: 0.107264\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "if model_tobe_trained:\n",
    "    lr=1e-2\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=100,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=200,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=100,learning_rate=lr)\n",
    "\n",
    "    lr=5e-4\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=500,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "\n",
    "    lr=1e-5\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=500,learning_rate=lr)\n",
    "\n",
    "    lr=5e-6\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=500,learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5tsLzrBNa7E"
   },
   "source": [
    "# Save The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42YFEvnqbE9U",
    "outputId": "555826a8-f613-4331-b6a5-1acc5a1b82a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running the neural network\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "print(\"running the neural network\")\n",
    "#run(obj1,save_address)\n",
    "obj1.model_save(address=save_address+\".pt\")\n",
    "obj1.save_residuals(address=save_address+'_residuals.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUnPPyZ6NfDr"
   },
   "source": [
    "# Visualize Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DStavVSXYRs5",
    "outputId": "047b7d45-2f98-4854-9ea2-17f3781d9842"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD7CAYAAAB68m/qAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9NUlEQVR4nO3deXwU9f348ddnj2zukJMA4YoCAZIQbpEiKh4o4H2hVqCH9cSqRam2lvpV6/WttlZF68+zWrFa1AqW6rcg4MUZ5CYcgSQEyEHOTbLX5/fHbNYlJLAJm2t9Px8PZWfmMzPvmd285zOfmfmM0lojhBCi+zN1dgBCCCGCQxK6EEKECEnoQggRIiShCyFEiJCELoQQIUISuhBChIiAErpSaqpSaqdSardSan4z0/sppZYrpTYqpb5TSl0c/FCFEEKciDrZfehKKTOwCzgfKATWAjO11tv8yrwMbNRav6iUGgYs1VoPaLeohRBCHMcSQJlxwG6t9V4ApdS7wKXANr8yGoj1fo4DDp5soUlJSXrAgAGtClYIIX7o1q9fX6q1Tm5uWiAJvQ9Q4DdcCIxvUmYB8B+l1J1AFHBecwtSSt0M3AzQr18/1q1bF8DqhRBCNFJK7W9pWrAuis4EXtdapwEXA28ppY5bttb6Za31GK31mOTkZg8wQggh2iiQhF4E9PUbTvOO8/dT4D0ArfXXQDiQFIwAhRBCBCaQhL4WGKSUGqiUCgOuAz5uUuYAMAVAKTUUI6GXBDNQIYQQJ3bShK61dgF3AMuA7cB7WuutSqmHlVKXeIvdC/xcKbUJ+DswW0s3jkII0aECuSiK1nopsLTJuIf8Pm8DJgY3NCGEEK0hT4oKIUSIkIQuhBAhIuQS+sYDR9lSVNnZYQghRIcLqA29O7n8ha8AyH98WidHIoQQHSvkauhCCPFDJQldCCFCRMgm9HqnG4CCcjtf7S7t5GiEEKL9dbuEvnznEQbMX8Krq/fhcntaLPf88t3UO91MenI517/yLV/skgdXhRCh7aT9obeXMWPG6Lb0tvjhxiJ+uSj3mHEPXjyUCaclkp4cxbCHlrU4747/mUq41dzqdQohRFehlFqvtR7T7LTultA9Hs2VC79i44GKNq33nvMHM3fKIAD2ldZiMSn6JkS2aVlCCNHRQiqhNyqpbqCyzslf/pvHf7Ydxu4w2sxtOLg9y0PCaWP4zYdbSIq2sfi2M5n05HLfvKenRHOwos43j9ziKIToLkIyoTfl9mh2Hqqmz6s5xLnK4N6dbDhqIzu6Gkt1EQUxI/h6bxn3vf/dcfNazYr7Lszg2nF9WbWrlGnZvYIWlxBCBFNoJfQjOyBvGUy8q/npC+KMf3/6OfQdC4/0BFc9LDCeHt1TUsOU//2CAYmR5JfZm11ERmoMv50+jImnS5fuQoiu5UQJvdvd5cKe/4PPHoLt/wK3s+Vy/8/7FjxXvfHv5vcBOC05mvzHp7Fi3jls/f2F7Pifqbxww6hjZt1xqJobXvmWbQer0FpTXutg28Gq9tgaIYQImu5XQ7eXw5MDvx+OTIRxvwCLDZQykn1L5u2FqMRmJ9U2uNh4oIKPNxXx3rrCE4YwMCmKRy/P5ItdJfx3+xFy+vbggYuHEhNuYXdJDYNTYqhucBEXYW399gkhxAmEVpMLwI6l8On9UHmg+elJg6F0V/PTpj8Lo2fDvpXQUA2nnQNhUccVe3DxZt7+toXlt1JWH6MZqLbBxd7SWt/4Pj0iuGZMX1LjbDy4eAsuj2b8wATMJsXsMwfg0ZqNBRXsOlTNtWP7oZTxwNS4gQms2VdOQlQYI/vFExVm5lBVPfGRYdQ53FTWOfnPtkPMmTgQq7ntJ2GNvw2l1KntACFE0IReQvenNbgawFELNYeMppgxP4V9X8AHP4Ur/gr/+iU4a5ufP2M6XPd2s5Mq7U7MZsXhKqPZZtvBKspqGiiurOellXtPPfYO1CPSSv+ESLYXV3PesBSq6lys3l3KLZNPY+EXewDITovju8JKPrnzR6zNL+f55bsprXEw8fREUmMjOFRVx8TTkxicEkN6chR/++YAidFh3HhGfz7KLWJ/mZ2lm4t5+uoROFwexg1MwGJWKIx9+NjS7aTFR3DJiD4oBWnxEVjNJhZvLOL6cf1weTRmk8JsUhSU2+kVF47FbMLj0ZTUNNAzNrxN2661DupBqdLupNzuoH9CJCaTsVy3R3Og3M7ApOMrB11Jhd1BrcNNnx4Rx4yvbXARGWb27aeymgbyy2oZ3T+hM8I8Iafb06qKSmWds8POlmsbXJhNql2fdznlhK6Umgr8CTADr2itH28y/RngHO9gJJCite5xomUG+y6XE9IaVj8Da/4K1QebLzPn30ZNvVd2wIt1uDxYzQqlFEdrHRRV1NErLpydh6uZkJ7If3ccIb/MzviBCfxn22HGDohn8cYiMnvHsTKvhDqHmzX55cjL+lrvjPQELCYT6/aXU+889onhaJuFiacnsmzrYd84q1nRNz6SA+V2ekSGUVrTcMw8fXpE+Oapc7hx+D2FfPnIPny1p5SbzzIOfiXVx87r78Yz+mF3uFmVV0pJdQMT0hPpnxiJUorEqDAq65x8sKGQCKuZslrHMfP+6oLBPP2fXUw8PZEvd5f5xs8c14+DFXXsL6slLT6S1d6uLJKibZTWNDCkZwwHK+v48Rn9SYgKY9vBKg5X15MYZePsIckUlNcRF2Hh0y2H+HZf+THrHDsgntyCCpzuwH6EfRMiGD8wkTH943l/fSEXDO/JhPQk/rG+gJhwC88vNyoHZpPC7dG+fXvFqD6U1zqoc7gprKgjt6CCoakxFByt47TkKKJtFr7Za5x1FlXUkRYfwZSMFFbmlbLPe1Y7pn886/Yf9cVyyYjeKAVf7Slj7IB4lm4+5Jt23tCeDEmN9sVjUhBls3BacjSTBiXx3H93H7dtF2Wm8ukWYxmTBiVxw/h+LN9RwqJ1BYBxs8TQXrEs3lgEwHVj+/Lu2gJuPiudb/eWsanw+267J6QnEmYxUXjUTkF5ne/3dNbgZB68eCgDkiKxWdqW9E8poSulzMAu4HygEOOl0TO9r51rrvydwEit9U9OtNwOTej+HLXwr7uMJG8Nh41/O3b6Hesg8XSjPb4DlVQ3EB9pxWxSaI2v5tdUbYOLI9UNJEWHsS7/KKenRNM3IZI6h5tnPt/FZ9sO8+y1OSRGh2FSitKaBuIjw4iPCmPX4Wo+WF9I34RI35nGJ98Vc1pyFHtKapmWZdyuueNQFTaLmW3Fx14ITogKo7xJEhJCtN7TV4/gqtFpbZr3VBP6BGCB1vpC7/CvAbTWf2ih/FfA77TWn51ouZ2W0JuqOADPZh0//oJH4Mw7Oz6eLqjB5X1oy69G0XgaW+90+04v651ulIIws4k9JTWkxUfy9Z4yYiMshFvNVNe7GJQSTXxkGEftDuwONwVH7Zx5WhINLjc2i9n7wJeLQ5UNlNU2MKpfPKvyStlTUkOvuHDfdYbiynreXVtA7oGjxIRbGdU/nlV5JfSMCSc+yopCcfWYNJJjbPx7yyH+vuYAPSLDiLZZ6J8YiUdrymocXDA8lXX55Xy1x6gR333eYF5ZtZfqBhcA6clRhFvM7C2tod7pYe6UQXywvpBrxvTl1S/3MSE9kX9vPcRNE/rz2bbDFFfWk50Wx76SWqaP6MXhqga01izfeXxfQtOze3HJiN4s3ljkqxk2UgquGJlGTLiF17/KB4wa4o5D1QCM7h9P/8RI/rmhyDfPz340kFdW76NPjwjCLCbCzCb6J0aSHGNjUEo0/7fjCKvySomLsFJZ5+RHpydxRnoCT//n++tNo/r1YMOBCgYmRflqxo1sFhM9Y8NJjQ1nTX45PWNtDO4ZQ6+4cOqcHj7dXMy07F5U1jm5fGQfGlwePtxYxLkZKXyYW8SWouPvFEuLjyCzdxz/3nqIK0b2YXx6AovWFmAxmQizmEhPjmJd/lFf5eJHpydxsLKO6nrXCc+UmtN4q7LNYmLSoGR2Hq6itsFNcrSNyjon9S43o/vF8/XeMuwON6enRNOnRwSxEVY+23bIdyaYkRpDZZ2TqZmp9EuIZFVeKWU1Dbi1JqtPD7YVVxFjs9Aj0krh0TqsZkW/hCgqvL/5cKuJu88fTHZaj1bF3+hUE/pVwFSt9c+8wz8Gxmut72imbH/gGyBNa+1uZvrNwM0A/fr1G71///7Wbkv7Kd8LXzwFm975ftwtX0JqZufFJEKSy+2h3uUh2nb8+2UKj9pJiw/driicbg9mpVo8AxUn15EJ/X6MZH7Sqm2XqaE357v34J8/Nz6Hx4E5DKJSoP8E6JUDgy8EWyxoD4SF7h+fEKLrOVFCD+QVdEVAX7/hNO+45lwH3N668Lqg7GsgvAds+jts/acxrrYEjmw98Xx9xsAZt0Jsb1BmiEyApEHtHq4QQkBgNXQLxkXRKRiJfC1wvdZ6a5NyGcC/gYE6gFtnunQNvSm3y3jitHQXHN0HxZtg7wqoKWn5rplG1ijQbmP+mN5gCYOj+ZCaDR43mC1QWwo9h0NyBmx4A+orjTOBxNONg4KrHvpPBI8Lag4b859+PoTHQlxfMFshKtl46Co6xRhuZC8HW8yx44QQ3VYwblu8GHgW47bFV7XWjyqlHgbWaa0/9pZZAIRrrecHElS3SuiBKNsDxblgi4OGKnDWGffC1xyBuqNQvs9IwJZwMJmNJNtQA047VAT5WoItDhq+v4WKiHgjBjCerLXFGgem/hONRG+2QVwaVB8yDlCWCONA47AbB6/TzjGanuqOGgeinsPh0HfeA1OWsR3aDQnpxkGrfA/UVRgHl4SBEB5vbHNDNSSeBiarcWDTHuNMyO00lh2dYsQoDzIJ0aLQfrAoFDjrQZmguthIrPWVRhI+ss24vbI412jCiYg3EmdYFNjL4PBWI/ke3Gh0aVDvTeIeN+T9B3qPgvoKI4kf3mIk8n5nGAeXIu++t0aBNcJYHt7fQnRP40BEe/02VMvLNtuMbhwaqr6PL2mQsc3mMONg0e9MqCwAayT0HAbVh6HXCGMbnXbjeoerzjhQKDOYTMZ+PJoPBWtg2KXGQdRph0ObYcT14HZ491EM9JtgHOg8bmMeayQUrjH2nclixL/r3xCRAFFJRrw1R4yzqehUY9uqD4EtGg7mGmdPsb2Ng1n5PmO5FfshbYxx9hceZzz4Ft3TWPbBDcZF+tRso8kv50bj4Od2GDGjjGX3zITdnxkd1qWNNWJ31kJYtHGgbKgy9p3Wxnzle419s+UDqCs3fmvpZxsH5Ih4Yx0NVUaZ6mIjviPbjX1fXWzE2f9HULIdSvOMfRHRw9ieqiKjyXHPfyGuDyQNgZ1LjX1Svg8yphm/T1usEafFBpWFxn6qLIQBEyEyCRw1xnq0NpYZlQSVRUYFICoFdn4KI28w1llz2KgIlOw0KhmOWmM4tpfxu4lMhP1fGssDY18oE9RXGXe3DTzLiL9sj3EWfOBr4zuoOmhUvjxuiO9v7JuaEiPuxorNkW1GhSgmFfJXGWfCYOyzugrjYcfqg8ZvNrqnMV/FfuMp9uQMOP08Y9+05a9HEro4jtbH14QbxzX+JuorjFp6dIrxh9p7pPFHXLLT+IM0W43PxZuMhFrjTaw9+hnjd39u/MGmjTUScESC9+BRCnmfGz9yt8NIqm6H8SN3Nxjrz1/1fVyxacZ8aCjzPhBisoKnhc7ZLOHfd8rWFsoEKCPBNXfgCYs2Eo9ovbAYcFR3chAnqFB0lAsehTOPu68kIJLQRWjy+N0ZW7HfqB2bzMbBxl5uJPWwaKP5y+MyakvWcCNhmyzGsNbeaxwNxkGjtsSodcH3BziPxzh4mCzGQSu2N9SWGdMs4Ubtt6HaOABE9zSGw+OMM6deOd/H0VjLriv3XjT3nlWFxxrra6iBqkKIH2iUNZmNbQzvYRxA3E7jIKpMULrT2N76CqPmGBZt1PgcdmPeuqPGNrnqjDOM8Dhv7H2M8vWVRk00YaBxcLSXGTVxt9+DY/WVENPL2E5lMg7SShlnhXFpxrot3u4YSncZB+zIBGP7LeGAMr4LZ50xLq6vcbZkL/eehdVA7RFjXyRneJsFtRGHw26chTSeXXjc3oM6xpkGGE18DdXeM9oexv6pLDRiazyjczUY32nNYWM/pGYZ6z6y3fgt1JQYtXBlNg40Mb3g6H7jO3bWGcv3uIzrWZUFxhmG22GcyUT3NK5/hcd6z9wwKhyuBmPYWQt9vWd19lIjjvgBULQB+p/5/fa0kiR0IYQIEaHVH7oQQohmSUIXQogQIQldCCFChCR0IYQIEZLQhRAiREhCF0KIECEJXQghQoQkdCGECBGS0IUQIkRIQhdCiBARcgnd7XFTWlfa2WEIIUSHC7mEfsd/7+Cc987haL3R/3d+ZT4rC1d2clRCCNH+ul1C31G+gxdzX2xx+uqi1QAcqjXeoj7jwxnc/n/d/614QghxMt0uoa8/vJ4XNr3A4rzFVDZU4tGeZstd88k1OP36y35n+zsdFaIQQnSKQF9BNxX4E8Yr6F7RWj/eTJlrgAUYPcdv0lpff6JltrX73CpHFZPeneRL5DazjeuGXEecLQ6lFH/a8KcW5/3gkg8YHD+41esUQoiu4pT6Q1dKmTFeEn0+UIjxkuiZWuttfmUGAe8B52qtjyqlUrTWR0603FPpD/3b4m95fM3j7K7Y3ez0pIikFi+MXjnoSm4ceiPv571PVUMVc0fNJTUqtU1xCCFERzvVhD4BWKC1vtA7/GsArfUf/Mo8CezSWr8SaFDBfMGF3Wmn3l3PgaoDfL7/c2ZnzmZf5T4e/eZR/vfs/+XB1Q+ytWxrs/NGW6P5+vqvgxKHEEK0t1NN6FcBU7XWP/MO/xgYr7W+w6/Mhxi1+IkYzTILtNb/bmZZNwM3A/Tr12/0/v1Bftv9Caw9tJY3tr7BmkNrqHPVHTf9tpzbSAxP5Joh13RYTEII0VonSuiWIK3DAgwCzgbSgJVKqSytdYV/Ia31y8DLYNTQg7TugIxNHcvY1LEALM5bTJWjipTIFO5beR8AL+S+AIDT42RC7wkMjB2IavoSZSGE6MICSehFQF+/4TTvOH+FwLdaayewTym1CyPBrw1KlEF2+aDLfZ8np03mnhX38OXBLwF4fI1xvXdA7AA+vuxjSepCiG4jkNsW1wKDlFIDlVJhwHXAx03KfIhRO0cplQQMBvYGL8z2E2mNZOH5C9k8azN/OfcvvvH5Vfm8te0tCqoLaHA30Fkv0xZCiEAFetvixcCzGO3jr2qtH1VKPQys01p/rIxq7P8CUwE38KjW+t0TLTOYF0WDraimiCs+ugK7y37ctLGpY7ns9MuodlSTGpXKhF4TqHHWkBKZ0gmRCiF+aE7pomh76coJHaC8vpxl+cv46uBXFNcU48FD3tG8k8530cCL6BXVC7MyE2eLY1zqOKKsUTS4G4gNiyU+PB6zMmNSJhrcDTg8DmLDYn3zuzwuTMqESXW7Z76EEB1AEnqQeLSHqoYqtpVtY0XhCuxOOysLV3K04Wibl6lQaIzvYHji8ONur0yLTqPKUcUFAy7A4XaQX5XPEfsRJqdNxma2kRKZQpwtDrvTjkYTZg5jQOwAAOJscRyqPURCeAJ9Y/pid9qpc9cRGxZLpCWSSGtks9uoUCiljmlmkmsJQnQNktA7iNvjptZVC0CpvZTt5ds5WHOQioYKdlfsJi4sDpvFhs1sI9ISid1lp8HdwIe7P8RispAUkYTb46akrsRXQ2+pa4NgsJltAISZwugV3YuK+gqO1B3/PFiUNYpwczhVjiqykrLISMjgu5Lv2FG+g4yEDPZV7aNfTD96RvUkPS6dguoCyuvLcbgdRFgiOCvtLI7WHyXKGkVyZDKJ4Ym4PC7K68sZljiMg7UHKbWXkpOSg8VkwWKyEB8ej9vjpoetBx7twWwy+/aHSZnQWneLg4zb4/bF3lU07kPRPUlC76Ya+6Jxe9zUOGvoYetBnauOKGsUR+xHKK8vp85VR2F1IRGWCKKsUeRX5XO49jCbSzczInkEdpcdszJTUF1AmDmMsaljKawu5JO9nzAgdgDRYdFYTVZK6koorC6kf2x/EsITKK0rZVPJJgBiw2KpclQBkBKRQml96QkPNFHWKGqdtUHfHzazjQZ3A4PiB1FYXUjfmL5EWCKocdQwOH4wO4/upMHdQA9bDzSaGkcNSikiLZHUu+vZV7kPMO5g2l+133dmlBKZgkd7fE8Xj+81nriwOEzKxBH7Eaqd1eQdzeP0HqcDxoX0neXGumKsMbi127ef+8X2A6CguoAYa4zv7C0hPIHTe5xOUU0RLo+Lw/bD9I7qzZG6I2QmZlLnqiM+PJ5DtYc4VHuIenc9vaJ6UVxbTGJ4ImZlxmq24tZuHG6jmS41KpVvir8BoE90H8LN4VQ7q40zMGsk+6v2MzplNFHWKN/32zOqJ+sPr/ft00HxgyirK/M9LV3jqCE+PJ7KhkqGJQ7jm+JvSIpIotZZS5wtjhHJI8g7msfOozuJMEcQbgnHarKyp3IPZ/Q6g2+KvyEhPIGhCUP58uCXWE1WnB4nmYmZVDuribZGE2WNQilFeX05+yr34fK4yErKIj48HrvTTnRYNLXOWuxOO0kRSRTVFGE1WYkJi2HNoTVMT5+Oy+OiuLaYXUd3UeeqY0DsAJweJ+X15fSN6Uu4JZyE8AS+Lf6WGGsM9e56spOz0WjyjuZxxH6ECwdcSEJ4AvmV+dS56sgtyT3mN1znqiM9Lt0oU5VPYngisbZYqhqq2F6+nbGpY+kZ2ZMNhzf4KmHDE4eTV5FHYngiBdUFWEwWekX1osHdQL2rnkHxg+gT3YdrM65leOLwNv0dSEIXp8zpdmI2GW3/DrcDi8mCy+NCKUVZXRkRlgjsTjspkSmYTWacHifFNcXsKN9B/9j+5Ffl0y+mH3WuOkrqSsg7mkdMWAwF1QXkHc1jdM/RWEwWHG4H8eHx/H3H34mzxfn+kD3ag0d7+Kb4G0zKxKiUUZTXl1NeX+67CykxIpGimiIyEjKoaqjiYO1BTMpEamQqta5aKhsqASO5OtwOXB4X9e56+sX041DtISKsEVQ7qslIyPA1YSWGJ1JWX4bb4yYmLMaXiLeUbWl2P4WbwwGod9f7xllNVt92HLYfPqZsvbuenpE9CbeEY3faqXHW+B58MykTHu0hJSIFlzbOaPxFWCJ8ZW1mG8MTh1NWX0ats/aYri962Hrg9ripdlb7ltkozhZHhCWCWoeRsAtrCn3fbYQlArMyExMW40uWFmXB4XEQZ4ujsqGSOFscFmWhrL6sxdgah9Pj0okNi6WopogoaxR5R/NwaRcA6XHpOD1O42Bli2VfxT7SYtKwu+xUNlQSExZDtDWa/Kp8LMoCCvrH9GdP5Z5mv4eekT2xmW0cqD5wzPiUyBSO2I+gUJhNZlweFymRKVhNVopqioi2RlPvqifWFnvM/u4f259DtYeIC4vzncXG2eKoc9bh8DgA40y38XMji8lC35i+lNhLqHHW+A7ST01+iqkDpjYb+8lIQheiAzRtBnK4HYSZwwKe3+1x48GD1WQNqGzjgTOQ8mBccLeYLC0On4rWbqu/1jYBebQHt3b7ttt/O5xuJ1bzsftDa43D4yDMFOb7flweF2ZlRqN9TXiNTXv+32NLTXstjW9wN2BRFmOZ3mU3rs/hdhBpjcTutBNhiWhzk2FHPCkqxA9e0z/Q1iY4s8mMmcDa2xvb5QNN5sBxyTtYyRxav63+Wtue3/QuMP/taJrMwfheGq8XNZ1HoXxlzMrs++w/b3NaGu+/nsZlN66vcZ3N3YwQLHJlRAghQoQkdCGECBGS0IUQIkRIQhdCiBAhCV0IIUKE3OUiRDfidDopLCykvr7+5IVFtxYeHk5aWhpWayvuZGrHeIQQQVZYWEhMTAwDBgzoFl0fiLbRWlNWVkZhYSEDBw4MeD5pchGiG6mvrycxMVGSeYhTSpGYmNjqMzFJ6EJ0M5LMfxja8j1LQhdCiBAhCV0IEZCysjJycnLIyckhNTWVPn36+IYdDscJ5123bh1z58496TrOPPPMU45z2bJlvriio6MZMmQIOTk53HTTTceVraio4IUXXghoudHR0a0a3ym01if9D+PVcjuB3cD8ZqbPBkqAXO9/PzvZMkePHq2FEK2zbdu2zg5Ba6317373O/3UU08dM87pdHZSNC2bPHmyXrt2bYvT9+3bp4cPHx7QsqKiolo1Phia+74xXv3ZbF49aQ1dKWUGngcuAoYBM5VSw5opukhrneP975VTPdAIIbq+2bNnc8sttzB+/Hjuu+8+1qxZw4QJExg5ciRnnnkmO3fuBGDFihVMnz4dgAULFvCTn/yEs88+m/T0dP785z/7ltdY212xYgVnn302V111FRkZGdxwww2+N2gtXbqUjIwMRo8ezdy5c33LPZk//vGPZGZmkpmZybPPPgvA/Pnz2bNnDzk5OcybN4+amhqmTJnCqFGjyMrK4qOPPgp4X2itmTdvHpmZmWRlZbFo0SIAiouLOeuss8jJySEzM5NVq1bhdruZPXu2r+wzzzwT8HpOJJDbFscBu7XWewGUUu8ClwLbghKBEKJNfv+vrWw7WBXUZQ7rHcvvZrTuxQuFhYV89dVXmM1mqqqqWLVqFRaLhc8//5wHHniADz744Lh5duzYwfLly6murmbIkCHceuutx91vvXHjRrZu3Urv3r2ZOHEiX375JWPGjOEXv/gFK1euZODAgcycOTOgGNevX89rr73Gt99+i9aa8ePHM3nyZB5//HG2bNlCbm4uAC6Xi8WLFxMbG0tpaSlnnHEGl1xySUAXKP/5z3+Sm5vLpk2bKC0tZezYsZx11lm88847XHjhhTz44IO43W7sdju5ubkUFRWxZYvRr35FRUVA23EygbSh9wEK/IYLveOaulIp9Z1S6n2lVN/mFqSUulkptU4pta6kpKQN4Qohupqrr74as9noerayspKrr76azMxM7r77brZu3drsPNOmTcNms5GUlERKSgqHDx8+rsy4ceNIS0vDZDKRk5NDfn4+O3bsID093XdvdqAJffXq1Vx++eVERUURHR3NFVdcwapVq44rp7XmgQceIDs7m/POO4+ioqJmY2tpHTNnzsRsNtOzZ08mT57M2rVrGTt2LK+99hoLFixg8+bNxMTEkJ6ezt69e7nzzjv597//TWxs7MlXEIBgPVj0L+DvWusGpdQvgDeAc5sW0lq/DLwMxgsugrRuIX6QWluTbi9RUVG+z7/97W8555xzWLx4Mfn5+Zx99tnNzmOzfd9vuNlsxuVytalMsL399tuUlJSwfv16rFYrAwYMOOWncs866yxWrlzJkiVLmD17Nvfccw833XQTmzZtYtmyZSxcuJD33nuPV1999ZTjD6SGXgT417jTvON8tNZlWusG7+ArwOhTjkwI0e1UVlbSp49xAv/6668HfflDhgxh79695OfnA/jaqU9m0qRJfPjhh9jtdmpra1m8eDGTJk0iJiaG6upqX7nKykpSUlKwWq0sX76c/fv3BxzbpEmTWLRoEW63m5KSElauXMm4cePYv38/PXv25Oc//zk/+9nP2LBhA6WlpXg8Hq688koeeeQRNmzY0Kr90JJAauhrgUFKqYEYifw64Hr/AkqpXlrrYu/gJcD2oEQnhOhW7rvvPmbNmsUjjzzCtGnTgr78iIgIXnjhBaZOnUpUVBRjx44NaL5Ro0Yxe/Zsxo0bB8DPfvYzRo4cCcDEiRPJzMzkoosu4v7772fGjBlkZWUxZswYMjIyAo7t8ssv5+uvv2bEiBEopXjyySdJTU3ljTfe4KmnnsJqtRIdHc2bb75JUVERc+bMweMx3u/6hz/8oZV7onkBvVNUKXUx8CxgBl7VWj+qlHoY4/aZj5VSf8BI5C6gHLhVa73jRMuUd4oK0Xrbt29n6NChnR1Gp6qpqSE6OhqtNbfffjuDBg3i7rvv7uyw2kVz3/cpv1NUa70UWNpk3EN+n38N/LrV0QohRCv99a9/5Y033sDhcDBy5Eh+8YtfdHZIXYb0tiiE6FbuvvvukK2Rnyp59F8IIUKEJHQhhAgRktCFECJESEIXQogQIQldCBGQU+k+F4wOt7766qvjxr/22mu+5YSFhZGVlUVOTg7z588/rmx+fj7vvPPOSdeVn59PZmZmwONDhdzlIoQISGJioq8TqwULFhAdHc2vfvWrgOdfsWIF0dHRx/V5PmfOHObMmQPAgAEDWL58OUlJSc0uozGhX3/99c1O/6GTGroQos3Wr1/P5MmTGT16NBdeeCHFxcYD43/+858ZNmwY2dnZXHfddeTn57Nw4UKeeeYZcnJymu0Yy19LXdHOnz+fVatWkZOTwzPPPEN+fj6TJk1i1KhRjBo1qtkzgJbU19czZ84csrKyGDlyJMuXLwdg69atjBs3jpycHLKzs8nLy6O2tpZp06YxYsQIMjMzA+5yoKNJDV2I7urT+XBoc3CXmZoFFz0eUFGtNXfeeScfffQRycnJLFq0iAcffJBXX32Vxx9/nH379mGz2aioqKBHjx7ccsstAdfqW+qK9vHHH+fpp5/mk08+AcBut/PZZ58RHh5OXl4eM2fOJNAn0J9//nmUUmzevJkdO3ZwwQUXsGvXLhYuXMhdd93FDTfcgMPhwO12s3TpUnr37s2SJUsAo8+XrkgSuhCiTRoaGtiyZQvnn38+AG63m169egGQnZ3NDTfcwGWXXcZll13W6mW31BVt025mnU4nd9xxB7m5uZjNZnbt2tWqddx5550AZGRk0L9/f3bt2sWECRN49NFHKSws5IorrmDQoEFkZWVx7733cv/99zN9+nQmTZrU6m3qCJLQheiuAqxJtxetNcOHD+frr78+btqSJUtYuXIl//rXv3j00UfZvDnIZxJezzzzDD179mTTpk14PB7Cw8NPeZnXX38948ePZ8mSJVx88cW89NJLnHvuuWzYsIGlS5fym9/8hilTpvDQQw+dfGEdTNrQhRBtYrPZKCkp8SV0p9PJ1q1b8Xg8FBQUcM455/DEE09QWVlJTU3NcV3VnkhLXdE2191tr169MJlMvPXWW7jd7oDjnzRpEm+//TYAu3bt4sCBA77uedPT05k7dy6XXnop3333HQcPHiQyMpIbb7yRefPmBa2722CTGroQok1MJhPvv/8+c+fOpbKyEpfLxS9/+UsGDx7MjTfeSGVlJVpr5s6dS48ePZgxYwZXXXUVH330Ec8999wJmy1a6oo2MTERs9nMiBEjmD17NrfddhtXXnklb775pq9L3UDddttt3HrrrWRlZWGxWHj99dex2Wy89957vPXWW1itVlJTU3nggQdYu3Yt8+bNw2QyYbVaefHFF4OxC4MuoO5z24N0nytE60n3uT8sre0+V5pchBAiREhCF0KIECEJXQghQkRACV0pNVUptVMptVspdXwHC9+Xu1IppZVSzbbvCCGEaD8nTehKKTPwPHARMAyYqZQa1ky5GOAu4NtgBymEEOLkAqmhjwN2a633aq0dwLvApc2U+x/gCaA+iPEJIYQIUCAJvQ9Q4Ddc6B3no5QaBfTVWi850YKUUjcrpdYppdaVlJS0OlghROdpr+5zAV5//XWSk5PJyclh+PDhXHXVVdjtdt/0p59+moyMDHJychg7dixvvvkmAGeffTZDhgzxxXHVVVc1u+w77rijjVvdvZzyg0VKKRPwR2D2ycpqrV8GXgbjPvRTXbcQouO0V/e5ja699lr+8pe/AMbj94sWLWLOnDksXLiQzz77jDVr1hAbG0tVVRWLFy/2zff2228zZoxctoPAauhFQF+/4TTvuEYxQCawQimVD5wBfCwXRoUIfe3Rfa7L5aK2tpb4+HgAHnvsMV588UVfx1yxsbHMmjWrTfHm5+dz7rnnkp2dzZQpUzhw4AAA//jHP8jMzGTEiBGcddZZQPPd6HZ1gdTQ1wKDlFIDMRL5dYCvd3mtdSXg641eKbUC+JXWWh4DFaIdPbHmCXaU7wjqMjMSMrh/3P0BlQ1297mLFi1i9erVFBcXM3jwYGbMmEFVVRXV1dWkp6e3GMcNN9xAREQEAOeffz5PPfVUi2XvvPNOZs2axaxZs3j11VeZO3cuH374IQ8//DDLli2jT58+VFRUADTbjW5Xd9IautbaBdwBLAO2A+9prbcqpR5WSl3S3gEKIbom/+5zc3JyeOSRRygsLAS+7z73b3/7GxZLYC271157Lbm5uRw6dIisrKwTJmZ/b7/9Nrm5ueTm5p50nq+//tr3tqMf//jHrF69GoCJEycye/Zs/vrXv/oS94QJE3jsscd44okn2L9/v++g0ZUFtKe11kuBpU3GNdt3pNb67FMPSwhxMoHWpNtLe3Wfq5RixowZPPfcc8yfP5/o6GhfD4jtZeHChXz77bcsWbKE0aNHs379+ha70e3K5ElRIUSbtGf3uatXr+a0004D4Ne//jW33347VVVVANTU1PjucmmtM888k3fffRcwavaNPT7u2bOH8ePH8/DDD5OcnExBQUGz3eh2ddJ9rhCiTYLdfW5jG7rH4yEtLY3XX38dgFtvvZWamhrGjh2L1WrFarVy7733+ubzb0NPSkri888/bzHm5557jjlz5vDUU0+RnJzMa6+9BsC8efPIy8tDa82UKVMYMWIETzzxxHHd6HZ10n2uEN2IdJ/7wyLd5wohxA+UJHQhhAgRktCFECJESEIXQogQIQldCCFChCR0IYQIEZLQhRABae/ucxu7uPV4PMyaNYuf/OQnaK0ZMGAApaWlx5VPTk5m5MiRDBo0iAsvvLDFZS9YsICnn366lVvbPcmDRUKIgLR397lgdCdwyy234HQ6ee2111BKtVjWv7vd5cuXc8UVV7B8+fIf9H36UkMXQrRZsLvPnTt3LmVlZbz55puYTIGnp3POOYebb76Zl19++YTlcnNzOeOMM8jOzubyyy/n6NGjzcYL8MUXX/jOQEaOHBlwtwWdSWroQnRThx57jIbtwe0+1zY0g9QAH3EPdve577zzDkOHDmXFihUB99Dob9SoUbz00ksnLHPTTTfx3HPPMXnyZB566CF+//vf8+yzzx4XLxhvSXr++eeZOHEiNTU1hIeHtzqmjiY1dCFEmwS7+9xRo0axf/9+1qxZ06Z4TtaNSWVlJRUVFUyePBmAWbNmsXLlyhbjnThxIvfccw9//vOfqaioaNNBpqN1/QiFEM0KtCbdXoLdfW5GRgYPP/ww11xzDcuWLWP48OGtimfjxo1tbj9vLt758+czbdo0li5dysSJE1m2bBkZGRltWn5HkRq6EKJN2qP73DPPPJMXX3yR6dOn+14PF4gvvviCl19+mZ///OctlomLiyM+Pt7Xfv/WW28xefLkFuPds2cPWVlZ3H///YwdO5YdO4LbvNUepIYuhGiTYHef22jGjBmUlpYydepUX/LNzs72XSS95ppryM7O9nW3a7fbGThwIB988MFJa+hvvPEGt9xyC3a7nfT0dF577TXcbnez8f72t79l+fLlmEwmhg8fzkUXXRTcHdgOAuo+Vyk1FfgTYAZe0Vo/3mT6LcDtgBuoAW7WWm870TKl+1whWk+6z/1hCXr3uUopM/A8cBEwDJiplBrWpNg7WussrXUO8CTwxzbELoQQ4hQE0oY+Dtittd6rtXYA7wKX+hfQWlf5DUYBnfPWDCGE+AELpA29D1DgN1wIjG9aSCl1O3APEAY0+yZVpdTNwM0A/fr1a22sQgghTiBod7lorZ/XWp8G3A/8poUyL2utx2itxyQnJwdr1UIIIQgsoRcBff2G07zjWvIucNkpxCSEEKINAknoa4FBSqmBSqkw4DrgY/8CSqlBfoPTgLzghSiEECIQJ03oWmsXcAewDNgOvKe13qqUelgpdYm32B1Kqa1KqVyMdvRZ7RWwEKJztGf3uQCffvopY8aMYdiwYYwcOZJ7770XMHp29F9XTk4OFRUVrFixgri4OHJychg6dCi///3vj1tmfn4+mZmZbd/obiagB4u01kuBpU3GPeT3+a4gxyWE6GLas/vcLVu2cMcdd7BkyRIyMjJwu93H9Jx49913N7uuSZMm8cknn1BbW0tOTg4zZsxg1KhRrd+4ECGP/gsh2ixY3ec++eSTPPjgg76+UsxmM7feemvAcURFRTF69Gh2797dYpn6+nrmzJlDVlYWI0eOZPny5QBs3bqVcePGkZOTQ3Z2Nnl5edTW1jJt2jRGjBhBZmYmixYtau2u6RTy6L8Q3dSq93ZRWlAT1GUm9Y1m0jWDAyobzO5zt2zZ4mtiac4zzzzD3/72NwDi4+N9ybhRWVkZ33zzDb/97W9bXMbzzz+PUorNmzezY8cOLrjgAnbt2sXChQu56667uOGGG3A4HLjdbpYuXUrv3r1ZsmQJYPTU2B1IQhdCtIl/97kAbrebXr16Ad93R3vZZZdx2WWXnfK6WmpyWbVqFSNHjsRkMjF//vwT9tC4evVq7rzzTsDo2bF///7s2rWLCRMm8Oijj1JYWMgVV1zBoEGDyMrK4t577+X+++9n+vTpLfY709VIQheimwq0Jt1egtl97vDhw1m/fj0jRoxoVQyNbein4vrrr2f8+PEsWbKEiy++mJdeeolzzz2XDRs2sHTpUn7zm98wZcoUHnrooZMvrJNJG7oQok2C2X3uvHnzeOyxx9i1axdgvCh64cKFQY130qRJvP322wDs2rWLAwcOMGTIEPbu3Ut6ejpz587l0ksv5bvvvuPgwYNERkZy4403Mm/ePDZs2BDUWNqL1NCFEG0SzO5zs7OzefbZZ5k5cyZ2ux2lFNOnT/dN929DB/jwww9bHe9tt93GrbfeSlZWFhaLhddffx2bzcZ7773HW2+9hdVqJTU1lQceeIC1a9cyb948TCYTVquVF1988ZT2VUcJqPvc9iDd5wrRetJ97g9L0LvPFUII0T1IQhdCiBAhCV2IbqazmklFx2rL9ywJXYhuJDw8nLKyMknqIU5rTVlZGeHh4a2aT+5yEaIbSUtLo7CwkJKSks4ORbSz8PBw0tLSWjWPJHQhuhGr1crAgQM7OwzRRUmTixBChAhJ6EIIESIkoQshRIiQhC6EECEioISulJqqlNqplNqtlJrfzPR7lFLblFLfKaX+TynVP/ihCiGEOJGTJnSllBl4HrgIGAbMVEoNa1JsIzBGa50NvA88GexAhRBCnFggNfRxwG6t9V6ttQN4F7jUv4DWernW2u4d/AZo3c2TQgghTlkgCb0PUOA3XOgd15KfAp82N0EpdbNSap1Sap08GCGEEMEV1IuiSqkbgTHAU81N11q/rLUeo7Uek5ycHMxVCyHED14gT4oWAX39htO8446hlDoPeBCYrLVuCE54QgghAhVIDX0tMEgpNVApFQZcB3zsX0ApNRJ4CbhEa30k+GEKIYQ4mZMmdK21C7gDWAZsB97TWm9VSj2slLrEW+wpIBr4h1IqVyn1cQuLE0II0U4C6pxLa70UWNpk3EN+n88LclxCCCFaSZ4UFUKIECEJXQghQoQkdCGECBGS0IUQIkRIQhdCiBAhCV0IIUKEJHQhhAgRktCFECJESEIXQogQIQldCCFChCR0IYQIEZLQhRAiREhCF0KIECEJXQghQoQkdCGECBGS0IUQIkRIQhdCiBAR0BuLlFJTgT8BZuAVrfXjTaafBTwLZAPXaa3fD3KcPt8tL2DNJ/tAg1IKZQKtAQ1aa1wODwBhEWZQCo/bg8etsVhNmK0mYz6T8g2bTAqtwWRWrYpDtVjcf4I2/q+PLeEbbjqh+VEtr89vZMvxtJ+WYm15ghDtJ5CfXdO/k6bzdNTf0agL+3PaqJSgL/ekCV0pZQaeB84HCoG1SqmPtdbb/IodAGYDvwp6hE3Ep0YxeGyqMaA1Hm2kUKUAk8JiMeF2e3A5jcRuNimUWeFyePC4PKBAe8DtMsp43B6Mkc0k15aCaOGX09xo3w/E+8H3ezn+g1/Z5gM5dvn6xB+1DuqvM6DjV2ATuihNY8ydcXAUhu9/499/H81p8Ts60XenW/iz8BvWnpMsP0jM1vZpHAmkhj4O2K213guglHoXuBTwJXStdb53mqcdYjxG36EJ9B2a0N6rEUKIbieQw0QfoMBvuNA7rtWUUjcrpdYppdaVlJS0ZRFCCCFa0KEXRbXWL2utx2itxyQnJ3fkqoUQIuQFktCLgL5+w2necUIIIbqQQBL6WmCQUmqgUioMuA74uH3DEkII0VonTehaaxdwB7AM2A68p7XeqpR6WCl1CYBSaqxSqhC4GnhJKbW1PYMWQghxvIDuQ9daLwWWNhn3kN/ntRhNMUIIITqJPCkqhBAhQhK6EEKECEnoQggRIiShCyFEiJCELoQQIUISuhBChAhJ6EIIESIkoQshRIiQhC6EECFCEroQQoQISehCCBEiJKELIUSIkIQuhBAhQhK6EEKECEnoQggRIiShCyFEiJCELoQQISKghK6UmqqU2qmU2q2Umt/MdJtSapF3+rdKqQFBj/QktMeDdjjQDgfOw4fRDocx3u1GezwdHY4QQnS4k76CTillBp4HzgcKgbVKqY+11tv8iv0UOKq1Pl0pdR3wBHBtewRc9vrrlDz7J5TJOBZ5nE7QGtxu418/pthYdH092unEFBmJstlAKTApFArtcvnm0R6PsUyTySjTnMbxvn+alGuctzEOv38138emUEa55tZzonmOiaHJv42fvftCN42hpW1pbt1N4hDdhHxX3UbKfffR44rLg77cQN4pOg7YrbXeC6CUehe4FPBP6JcCC7yf3wf+opRSWgf/FxY+dBjx119vJHBAhYWByYQym4zPWmOKisZdU42r+BCmqChMkRF4amvxNDQYC/Fo0B6wWFCNic1kBo8HrT3G9Kb5rnFLWkx02kiijfN6l+tbPn4JXGtjPU1pDUodP49vXd6DT3Mx+H1UZhMo03EHoObKNq7z+5n9ph23DaLrk++qOwjr17ddlhtIQu8DFPgNFwLjWyqjtXYppSqBRKDUv5BS6mbgZoB+/fq1KeCo8eOIGj+uTfMKIUQo69CLolrrl7XWY7TWY5KTkzty1UIIEfICSehFgP/5QZp3XLNllFIWIA4oC0aAQgghAhNIQl8LDFJKDVRKhQHXAR83KfMxMMv7+Srgv+3Rfi6EEKJlJ21D97aJ3wEsA8zAq1rrrUqph4F1WuuPgf8HvKWU2g2UYyR9IYQQHSiQi6JorZcCS5uMe8jvcz1wdXBDE0II0RrypKgQQoQISehCCBEiJKELIUSIUJ11M4pSqgTY38bZk2jy0FIXJ/G2n+4UK0i87ak7xQptj7e/1rrZB3k6LaGfCqXUOq31mM6OI1ASb/vpTrGCxNueulOs0D7xSpOLEEKECEnoQggRIrprQn+5swNoJYm3/XSnWEHibU/dKVZoh3i7ZRu6EEKI43XXGroQQogmJKELIUSI6HYJ/WTvN+2EePoqpZYrpbYppbYqpe7yjl+glCpSSuV6/7vYb55fe+PfqZS6sBNizldKbfbGtc47LkEp9ZlSKs/7b7x3vFJK/dkb73dKqVEdHOsQv32Yq5SqUkr9sqvsX6XUq0qpI0qpLX7jWr0vlVKzvOXzlFKzmltXO8b7lFJqhzemxUqpHt7xA5RSdX77eKHfPKO9v6Hd3m1ql1cltRBvq7/7jsgbLcS6yC/OfKVUrnd8++xbrXW3+Q+jt8c9QDoQBmwChnVyTL2AUd7PMcAuYBjGK/l+1Uz5Yd64bcBA7/aYOzjmfCCpybgngfnez/OBJ7yfLwY+xXi32RnAt538/R8C+neV/QucBYwCtrR1XwIJwF7vv/Hez/EdGO8FgMX7+Qm/eAf4l2uynDXebVDebbqoA+Nt1XffUXmjuVibTP9f4KH23LfdrYbue7+p1toBNL7ftNNorYu11hu8n6uB7Riv5GvJpcC7WusGrfU+YDfGdnW2S4E3vJ/fAC7zG/+mNnwD9FBK9eqE+ACmAHu01id6wrhD96/WeiVGl9FNY2jNvrwQ+ExrXa61Pgp8BkztqHi11v/RWru8g99gvMSmRd6YY7XW32gjA73J99sYVC3s35a09N13SN44UazeWvY1wN9PtIxT3bfdLaE3937TEyXPDqWUGgCMBL71jrrDexr7auNpN11jGzTwH6XUemW85xWgp9a62Pv5ENDT+7krxNvoOo79g+iq+7e1+7IrxNzoJxi1wkYDlVIblVJfKKUmecf1wYixUWfE25rvvivs30nAYa11nt+4oO/b7pbQuyylVDTwAfBLrXUV8CJwGpADFGOcbnUVP9JajwIuAm5XSp3lP9FbM+hS97Mq421ZlwD/8I7qyvvXpyvuy5YopR4EXMDb3lHFQD+t9UjgHuAdpVRsZ8Xnp1t8903M5NjKSLvs2+6W0AN5v2mHU0pZMZL521rrfwJorQ9rrd1aaw/wV74/7e/0bdBaF3n/PQIs9sZ2uLEpxfvvEW/xTo/X6yJgg9b6MHTt/Uvr92Wnx6yUmg1MB27wHoTwNl2UeT+vx2iHHuyNzb9ZpkPjbcN336n7VxnvWb4CWNQ4rr32bXdL6IG837RDedvG/h+wXWv9R7/x/u3MlwONV74/Bq5TStmUUgOBQRgXQToq3iilVEzjZ4wLYls49r2ws4CP/OK9yXuHxhlApV9zQkc6pobTVfevXwyt2ZfLgAuUUvHe5oMLvOM6hFJqKnAfcInW2u43PlkpZfZ+TsfYl3u9MVcppc7w/v5v8tvGjoi3td99Z+eN84AdWmtfU0q77dtgX+lt7/8w7hTYhXFEe7ALxPMjjFPq74Bc738XA28Bm73jPwZ6+c3zoDf+nbTT3QEniDcd4yr/JmBr4z4EEoH/A/KAz4EE73gFPO+NdzMwphP2cRRQBsT5jesS+xfjIFMMODHaO3/aln2J0Xa92/vfnA6OdzdGG3Pj73eht+yV3t9ILrABmOG3nDEYiXQP8Be8T513ULyt/u47Im80F6t3/OvALU3Ktsu+lUf/hRAiRHS3JhchhBAtkIQuhBAhQhK6EEKECEnoQggRIiShCyFEiJCELoQQIUISuhBChIj/D7IYvzH6B+DmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obj1.plot_residuals(init_index=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkjkH6AjN5pq"
   },
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DBW6HgBINu9v"
   },
   "outputs": [],
   "source": [
    "from ci_vae import ivae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "#import umap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqNZPHbPOkoh",
    "outputId": "35686f32-9d60-4ae3-b7b6-a634658bb1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the code\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000099139</th>\n",
       "      <th>ENSG00000269028</th>\n",
       "      <th>ENSG00000225840</th>\n",
       "      <th>ENSG00000166710</th>\n",
       "      <th>ENSG00000198804</th>\n",
       "      <th>ENSG00000167996</th>\n",
       "      <th>ENSG00000198727</th>\n",
       "      <th>ENSG00000130066</th>\n",
       "      <th>ENSG00000122862</th>\n",
       "      <th>ENSG00000231500</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000213639</th>\n",
       "      <th>ENSG00000156508</th>\n",
       "      <th>ENSG00000070756</th>\n",
       "      <th>ENSG00000063046</th>\n",
       "      <th>ENSG00000135968</th>\n",
       "      <th>ENSG00000198888</th>\n",
       "      <th>ENSG00000090104</th>\n",
       "      <th>ENSG00000147604</th>\n",
       "      <th>Y</th>\n",
       "      <th>YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.142779</td>\n",
       "      <td>-0.232256</td>\n",
       "      <td>-0.510939</td>\n",
       "      <td>-0.577439</td>\n",
       "      <td>-0.500473</td>\n",
       "      <td>-0.363331</td>\n",
       "      <td>0.762842</td>\n",
       "      <td>-0.144618</td>\n",
       "      <td>-0.099679</td>\n",
       "      <td>0.158276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.506358</td>\n",
       "      <td>0.450110</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>-0.108881</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>0.396830</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.137531</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.143735</td>\n",
       "      <td>-0.275919</td>\n",
       "      <td>-0.589931</td>\n",
       "      <td>-0.682540</td>\n",
       "      <td>0.166552</td>\n",
       "      <td>-0.401349</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>-0.199836</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.097916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352869</td>\n",
       "      <td>1.375910</td>\n",
       "      <td>-0.250887</td>\n",
       "      <td>-0.136809</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>0.026834</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.206215</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.140000</td>\n",
       "      <td>-0.369762</td>\n",
       "      <td>-0.523606</td>\n",
       "      <td>-0.449770</td>\n",
       "      <td>-0.172116</td>\n",
       "      <td>-0.444671</td>\n",
       "      <td>0.207073</td>\n",
       "      <td>-0.199836</td>\n",
       "      <td>-0.215888</td>\n",
       "      <td>-0.030897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278965</td>\n",
       "      <td>0.449511</td>\n",
       "      <td>-0.378783</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>-0.064337</td>\n",
       "      <td>-0.131046</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.295940</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.144317</td>\n",
       "      <td>-0.274877</td>\n",
       "      <td>-0.641722</td>\n",
       "      <td>-0.171661</td>\n",
       "      <td>-0.151140</td>\n",
       "      <td>-0.421610</td>\n",
       "      <td>-0.072508</td>\n",
       "      <td>-0.077442</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>-0.217924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.694927</td>\n",
       "      <td>-0.877009</td>\n",
       "      <td>-0.359729</td>\n",
       "      <td>-0.070910</td>\n",
       "      <td>-0.055551</td>\n",
       "      <td>-0.075671</td>\n",
       "      <td>-0.292218</td>\n",
       "      <td>-0.326305</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.145319</td>\n",
       "      <td>-0.238402</td>\n",
       "      <td>-0.533023</td>\n",
       "      <td>-0.715673</td>\n",
       "      <td>-0.110848</td>\n",
       "      <td>-0.447279</td>\n",
       "      <td>1.318530</td>\n",
       "      <td>-0.096442</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.138932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.805869</td>\n",
       "      <td>1.290240</td>\n",
       "      <td>0.064943</td>\n",
       "      <td>-0.017243</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>-0.091883</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.164941</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.145319</td>\n",
       "      <td>-0.168706</td>\n",
       "      <td>-0.115022</td>\n",
       "      <td>-0.881533</td>\n",
       "      <td>-0.480855</td>\n",
       "      <td>-0.455569</td>\n",
       "      <td>0.555907</td>\n",
       "      <td>-0.147432</td>\n",
       "      <td>-0.223550</td>\n",
       "      <td>0.228157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.806292</td>\n",
       "      <td>0.124092</td>\n",
       "      <td>-0.199385</td>\n",
       "      <td>-0.098722</td>\n",
       "      <td>-0.085594</td>\n",
       "      <td>0.204897</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.386954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.141199</td>\n",
       "      <td>-0.119064</td>\n",
       "      <td>-0.517628</td>\n",
       "      <td>-0.573159</td>\n",
       "      <td>0.101448</td>\n",
       "      <td>-0.459228</td>\n",
       "      <td>1.009727</td>\n",
       "      <td>-0.197269</td>\n",
       "      <td>-0.213005</td>\n",
       "      <td>0.179193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.547844</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>-0.348224</td>\n",
       "      <td>-0.134901</td>\n",
       "      <td>-0.074083</td>\n",
       "      <td>0.171302</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.138767</td>\n",
       "      <td>-0.277695</td>\n",
       "      <td>-0.405305</td>\n",
       "      <td>-0.850223</td>\n",
       "      <td>-0.282392</td>\n",
       "      <td>-0.437460</td>\n",
       "      <td>0.505407</td>\n",
       "      <td>-0.177188</td>\n",
       "      <td>-0.222749</td>\n",
       "      <td>0.328891</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263950</td>\n",
       "      <td>0.185749</td>\n",
       "      <td>0.190735</td>\n",
       "      <td>-0.136810</td>\n",
       "      <td>-0.080291</td>\n",
       "      <td>-0.141683</td>\n",
       "      <td>-0.447621</td>\n",
       "      <td>-0.017815</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.143859</td>\n",
       "      <td>-0.341971</td>\n",
       "      <td>-0.518436</td>\n",
       "      <td>-0.412762</td>\n",
       "      <td>-0.309528</td>\n",
       "      <td>-0.435564</td>\n",
       "      <td>-0.496546</td>\n",
       "      <td>-0.199700</td>\n",
       "      <td>-0.214390</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.806049</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-0.386615</td>\n",
       "      <td>-0.137079</td>\n",
       "      <td>-0.085386</td>\n",
       "      <td>0.450344</td>\n",
       "      <td>-0.099152</td>\n",
       "      <td>-0.233118</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.145248</td>\n",
       "      <td>-0.238803</td>\n",
       "      <td>-0.592046</td>\n",
       "      <td>-0.444801</td>\n",
       "      <td>0.858058</td>\n",
       "      <td>-0.442341</td>\n",
       "      <td>-0.169489</td>\n",
       "      <td>0.075389</td>\n",
       "      <td>-0.221574</td>\n",
       "      <td>0.063786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.620067</td>\n",
       "      <td>0.097621</td>\n",
       "      <td>-0.306195</td>\n",
       "      <td>0.053265</td>\n",
       "      <td>-0.089319</td>\n",
       "      <td>-0.037091</td>\n",
       "      <td>-0.412636</td>\n",
       "      <td>-0.055385</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000099139  ENSG00000269028  ENSG00000225840  ENSG00000166710  \\\n",
       "0        -0.142779        -0.232256        -0.510939        -0.577439   \n",
       "1        -0.143735        -0.275919        -0.589931        -0.682540   \n",
       "2        -0.140000        -0.369762        -0.523606        -0.449770   \n",
       "3        -0.144317        -0.274877        -0.641722        -0.171661   \n",
       "4        -0.145319        -0.238402        -0.533023        -0.715673   \n",
       "5        -0.145319        -0.168706        -0.115022        -0.881533   \n",
       "6        -0.141199        -0.119064        -0.517628        -0.573159   \n",
       "7        -0.138767        -0.277695        -0.405305        -0.850223   \n",
       "8        -0.143859        -0.341971        -0.518436        -0.412762   \n",
       "9        -0.145248        -0.238803        -0.592046        -0.444801   \n",
       "\n",
       "   ENSG00000198804  ENSG00000167996  ENSG00000198727  ENSG00000130066  \\\n",
       "0        -0.500473        -0.363331         0.762842        -0.144618   \n",
       "1         0.166552        -0.401349         0.022919        -0.199836   \n",
       "2        -0.172116        -0.444671         0.207073        -0.199836   \n",
       "3        -0.151140        -0.421610        -0.072508        -0.077442   \n",
       "4        -0.110848        -0.447279         1.318530        -0.096442   \n",
       "5        -0.480855        -0.455569         0.555907        -0.147432   \n",
       "6         0.101448        -0.459228         1.009727        -0.197269   \n",
       "7        -0.282392        -0.437460         0.505407        -0.177188   \n",
       "8        -0.309528        -0.435564        -0.496546        -0.199700   \n",
       "9         0.858058        -0.442341        -0.169489         0.075389   \n",
       "\n",
       "   ENSG00000122862  ENSG00000231500  ...  ENSG00000213639  ENSG00000156508  \\\n",
       "0        -0.099679         0.158276  ...        -0.506358         0.450110   \n",
       "1        -0.223550         0.097916  ...        -0.352869         1.375910   \n",
       "2        -0.215888        -0.030897  ...        -0.278965         0.449511   \n",
       "3        -0.223550        -0.217924  ...        -0.694927        -0.877009   \n",
       "4        -0.223550         0.138932  ...        -0.805869         1.290240   \n",
       "5        -0.223550         0.228157  ...        -0.806292         0.124092   \n",
       "6        -0.213005         0.179193  ...        -0.547844         0.006449   \n",
       "7        -0.222749         0.328891  ...        -0.263950         0.185749   \n",
       "8        -0.214390         0.029604  ...        -0.806049        -0.051556   \n",
       "9        -0.221574         0.063786  ...        -0.620067         0.097621   \n",
       "\n",
       "   ENSG00000070756  ENSG00000063046  ENSG00000135968  ENSG00000198888  \\\n",
       "0         0.085160        -0.108881        -0.089319         0.396830   \n",
       "1        -0.250887        -0.136809        -0.089319         0.026834   \n",
       "2        -0.378783         0.279081        -0.064337        -0.131046   \n",
       "3        -0.359729        -0.070910        -0.055551        -0.075671   \n",
       "4         0.064943        -0.017243        -0.089319        -0.091883   \n",
       "5        -0.199385        -0.098722        -0.085594         0.204897   \n",
       "6        -0.348224        -0.134901        -0.074083         0.171302   \n",
       "7         0.190735        -0.136810        -0.080291        -0.141683   \n",
       "8        -0.386615        -0.137079        -0.085386         0.450344   \n",
       "9        -0.306195         0.053265        -0.089319        -0.037091   \n",
       "\n",
       "   ENSG00000090104  ENSG00000147604  Y   YY  \n",
       "0        -0.447621        -0.137531  0  0.0  \n",
       "1        -0.447621        -0.206215  0  0.0  \n",
       "2        -0.447621        -0.295940  0  0.0  \n",
       "3        -0.292218        -0.326305  0  0.0  \n",
       "4        -0.447621        -0.164941  0  0.0  \n",
       "5        -0.447621        -0.386954  0  0.0  \n",
       "6        -0.447621         0.012308  0  0.0  \n",
       "7        -0.447621        -0.017815  0  0.0  \n",
       "8        -0.099152        -0.233118  0  0.0  \n",
       "9        -0.412636        -0.055385  0  0.0  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"start of the code\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##############################################################   \n",
    "##############################################################\n",
    "model_init=True\n",
    "model_tobe_trained=False\n",
    "\n",
    "model_init=True\n",
    "model_file_address='./bb.pt'\n",
    "save_address1=\"./\"\n",
    "\n",
    "df_XY=pd.read_csv('df_XY.csv')\n",
    "df_XY.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oDqQbsnOO6d"
   },
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fHFjOqpnigK_"
   },
   "outputs": [],
   "source": [
    "obj2 = ivae.IVAE(df_XY = df_XY,\n",
    "               reconst_coef = reconst_coef,\n",
    "               latent_size = 8,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "obj2.model_initialiaze()\n",
    "\n",
    "obj2.model_load(address=\"bb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOUIzmGTOTyG"
   },
   "source": [
    "## Print the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-H3bybRp484",
    "outputId": "fce40859-ea30-4f75-b4b5-08284301f7cf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "for param in obj2.model.parameters():\n",
    "    print(param)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi3yqTDIOoui"
   },
   "source": [
    "# Make Prediction of All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHcer1BPikHd",
    "outputId": "f72b5c62-5fd7-438d-818f-47f5dddaf3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "test data generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mnabian/Desktop/CIVAE_Studies/colorectal/ci_vae/example/ci_vae/ivae.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "\n",
    "    obj2.load_residuals(address='bb_residuals.pkl')\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    obj2.generate_test_results()\n",
    "    print(\"test data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaKllEltPf16"
   },
   "source": [
    "# Comprehensive Checking of The Prediction Values vs. True Values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK8l95VcvpJt",
    "outputId": "470c82e6-2b9a-4903-eaa1-732e46bd84c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1428, -0.2323, -0.5109,  ...,  0.3968, -0.4476, -0.1375],\n",
      "        [-0.1437, -0.2759, -0.5899,  ...,  0.0268, -0.4476, -0.2062],\n",
      "        [-0.1400, -0.3698, -0.5236,  ..., -0.1310, -0.4476, -0.2959],\n",
      "        ...,\n",
      "        [-0.1420, -0.3179, -0.3503,  ..., -0.3795,  0.3255,  0.0115],\n",
      "        [-0.1027,  1.6955,  0.6135,  ..., -0.3804,  0.3917, -0.4851],\n",
      "        [-0.1437,  1.0294,  2.0988,  ..., -0.1319, -0.4433,  0.5152]])\n"
     ]
    }
   ],
   "source": [
    "print(obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9ZitDT26ZXW",
    "outputId": "de963c21-10fd-4515-b6b5-b9a06eb3bd46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1447, -0.2132, -0.5028,  ...,  0.0453, -0.4291, -0.1369],\n",
      "        [-0.1447, -0.2204, -0.5137,  ...,  0.0374, -0.4267, -0.1417],\n",
      "        [-0.1447, -0.2175, -0.5057,  ...,  0.0413, -0.4284, -0.1380],\n",
      "        ...,\n",
      "        [-0.1420, -0.2163, -0.0944,  ..., -0.3598, -0.1819, -0.1785],\n",
      "        [-0.1440,  0.6078,  0.4164,  ...,  0.3080, -0.4300,  0.0556],\n",
      "        [-0.1456, -0.1441, -0.0792,  ..., -0.3672, -0.2666, -0.2600]])\n"
     ]
    }
   ],
   "source": [
    "print(obj2.x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ENQ8rtRHInw",
    "outputId": "959af6be-4ab0-4150-e843-b0d2297efc30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2827)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.abs(obj2.x_pred - obj2.x_last)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJJfodLhQmrD",
    "outputId": "1bc5cb45-60ba-4bc1-c812-fd51c64c41ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9352e-03,  1.9045e-02,  8.1042e-03,  ..., -3.5153e-01,\n",
       "          1.8497e-02,  5.8907e-04],\n",
       "        [-9.9091e-04,  5.5542e-02,  7.6267e-02,  ...,  1.0537e-02,\n",
       "          2.0950e-02,  6.4540e-02],\n",
       "        [-4.7193e-03,  1.5221e-01,  1.7878e-02,  ...,  1.7238e-01,\n",
       "          1.9209e-02,  1.5795e-01],\n",
       "        ...,\n",
       "        [ 8.3536e-05,  1.0163e-01,  2.5594e-01,  ...,  1.9771e-02,\n",
       "         -5.0744e-01, -1.9000e-01],\n",
       "        [-4.1285e-02, -1.0878e+00, -1.9709e-01,  ...,  6.8846e-01,\n",
       "         -8.2174e-01,  5.4073e-01],\n",
       "        [-1.8825e-03, -1.1735e+00, -2.1780e+00,  ..., -2.3528e-01,\n",
       "          1.7671e-01, -7.7521e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(obj2.x_pred-obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3q3XVFEvsEQ",
    "outputId": "4add0237-b89d-4803-983f-2d35cad6599d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(obj2.y_last)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opj3sBJX6cQH",
    "outputId": "daedae9e-1edb-4243-efa3-e54976aaee6c"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(obj2.y_pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2Yzu9Eiis50",
    "outputId": "d909e107-e05b-41a0-f075-c48541287153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1672, 30)\n",
      "Full_data_reconstructed...\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "    for x, y in obj2.testloader:\n",
    "      x = x.to(device)\n",
    "      # forward\n",
    "      x_hat,y_hat, mu, logvar,z = obj2.model(x)\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(x_hat.cpu().detach().numpy(), columns=obj1.df_XY.drop(columns=['Y']).columns)\n",
    "    print(df_reconstructed.shape)\n",
    "    df_latent=pd.DataFrame(z.cpu().detach().numpy())\n",
    "    \n",
    "    obj2.model.eval()\n",
    "    \n",
    "    df_reconstructed_decoder=pd.DataFrame(obj2.model.decoder(z).cpu().detach().numpy(), columns=obj1.df_XY.drop(columns=['Y']).columns)\n",
    "\n",
    "    df_reconstructed.to_csv('df_reconstructed.csv')\n",
    "    df_latent.to_csv('df_latent.csv')\n",
    "    df_reconstructed_decoder.to_csv('df_reconstructed_decoder.csv')\n",
    "    print(\"Full_data_reconstructed...\")\n",
    "    '''\n",
    "    print(\"========df_reconstructed========\")\n",
    "    print(df_reconstructed)\n",
    "    print(\"========df_reconstructed_decoder========\")\n",
    "    print(df_reconstructed_decoder)\n",
    "    print(\"========df_Original========\")\n",
    "    print(df_XY)\n",
    "     '''\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CciNZOW_Rc0n"
   },
   "source": [
    "# Checking Linear Separability of Data on Lower Dimensioanl Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHqfONTOlqSr",
    "outputId": "6a2fa0ee-b52d-49dd-f55a-f6cdae3ee20e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regression analysis\n",
      "0.4623205741626794\n"
     ]
    }
   ],
   "source": [
    "print(\"regression analysis\")\n",
    "obj2.regression_analysis(obj2.zs,df_XY['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBQlR5KERlL-"
   },
   "source": [
    "# Visualize Data on Lower Dimensional Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SXZtQfoj93-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"calculate tsne_umap_pca\")\n",
    "tsne_mat,umap_mat,pca_mat,Y=obj2.calculate_lower_dimensions(obj2.zs,obj2.y_last,N=100)\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='3d',save_str='tsne3d.pdf')\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='2d',save_str='tsne2d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='3d',save_str='umap3d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='2d',save_str='umap2d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='3d',save_str='pca3d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='2d',save_str='pca2d.pdf')\n",
    "\n",
    "print(\"finished\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ULT6UtRxU6"
   },
   "source": [
    "# Perform Interpolation across all groups (Y) and all features from YY=0 to YY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dXtgbd1iJ0s",
    "outputId": "4710be7c-960f-4184-bdb5-931a5699dd5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "360 55\n",
      "[[-0.1440053  -0.20668767 -0.14814503 -0.03774679  0.00448923 -0.33027728\n",
      "  -0.02258293 -0.1689754  -0.18948588 -0.0763806   0.00281458 -0.12857632\n",
      "  -0.08521586 -0.05660162 -0.05624808 -0.22463228 -0.22357547 -0.07959488\n",
      "  -0.09353294  0.04574478 -0.03409209 -0.22398838 -0.63629144  0.00714898\n",
      "  -0.16417197 -0.12585898 -0.08762588 -0.06379164 -0.39157048 -0.1378037 ]\n",
      " [-0.14391297 -0.16995329 -0.1223444   0.00107574  0.10618805 -0.28807913\n",
      "  -0.06424236 -0.14675273 -0.1463908  -0.08152454 -0.04379904 -0.17157552\n",
      "  -0.06691355 -0.05671614 -0.05942709 -0.08283095 -0.16519891 -0.08195387\n",
      "  -0.09359264  0.0287125  -0.05796688 -0.23710983 -0.46984954 -0.02780577\n",
      "  -0.15584628 -0.13161602 -0.08804777 -0.07615379 -0.30874445 -0.15618411]\n",
      " [-0.14277484 -0.18189213 -0.11017524  0.07274695 -0.02413071 -0.10328216\n",
      "  -0.26633143 -0.10022482 -0.02634448 -0.09978434 -0.26661083 -0.21537817\n",
      "  -0.04109067 -0.05682339 -0.05714214  0.00382758 -0.18064724 -0.04165647\n",
      "  -0.09384614 -0.13989408 -0.15504794 -0.24111597  0.34012124 -0.08626655\n",
      "  -0.12802343 -0.13442592 -0.0885278  -0.13521513 -0.04648329 -0.16884123]\n",
      " [-0.13853551 -0.38431695 -0.01009315 -0.02926882 -0.44462653  0.28037593\n",
      "  -0.37101452 -0.0252105   0.14245354 -0.10686844 -0.43488625 -0.20371396\n",
      "  -0.04045866 -0.0569875  -0.02968702  0.15487644 -0.4346191  -0.01190291\n",
      "  -0.09409286 -0.2181389  -0.17634214 -0.24250706  0.92609686 -0.08398214\n",
      "   0.04398186 -0.13565128 -0.08863496 -0.14976076 -0.00666011 -0.13832676]\n",
      " [-0.13799044 -0.39701205 -0.00639196 -0.03139547 -0.47213992  0.32628226\n",
      "  -0.3659687  -0.01912379  0.15978828 -0.10152145 -0.43857121 -0.19547927\n",
      "  -0.03801027 -0.05706177 -0.02299279  0.16941056 -0.45144242 -0.00130527\n",
      "  -0.09422925 -0.21226209 -0.17927739 -0.24108441  0.99682868 -0.03754112\n",
      "   0.06386687 -0.13575312 -0.08873792 -0.14727122  0.02079195 -0.13071929]]\n",
      "1\n",
      "354 54\n",
      "[[-0.14395257 -0.15777337 -0.14253373  0.10322777  0.08829233 -0.30785544\n",
      "  -0.05838105 -0.14998779 -0.16401757 -0.08444609 -0.04298511 -0.15561756\n",
      "  -0.07389257 -0.05665636 -0.06564313 -0.09255997 -0.15724355 -0.08350573\n",
      "  -0.0935325   0.04980937 -0.03830012 -0.23542562 -0.55955583 -0.07371072\n",
      "  -0.16923971 -0.12939283 -0.0877329  -0.07526474 -0.34187809 -0.17611152]\n",
      " [-0.14379968 -0.12953585 -0.14261729  0.13989924  0.25834812 -0.24629967\n",
      "  -0.12043015 -0.12722572 -0.09001618 -0.08914942 -0.09227101 -0.20084205\n",
      "  -0.05187199 -0.05668476 -0.06566083 -0.0386895  -0.12665907 -0.07832525\n",
      "  -0.09356111  0.00374862 -0.07059743 -0.24053919 -0.16472625 -0.09625792\n",
      "  -0.15636215 -0.13304203 -0.08810835 -0.09252397 -0.20866984 -0.18603951]\n",
      " [-0.14259337 -0.16721392 -0.11503492  0.10448762 -0.00731388 -0.0562283\n",
      "  -0.28115919 -0.08407815 -0.00280745 -0.10421267 -0.28268752 -0.23552977\n",
      "  -0.03974544 -0.05677924 -0.06019179  0.0253638  -0.17246617 -0.04072727\n",
      "  -0.09373753 -0.14639465 -0.15642753 -0.24167369  0.38929868 -0.12577162\n",
      "  -0.11326556 -0.13447133 -0.08850615 -0.14162472 -0.06346147 -0.18164945]\n",
      " [-0.13924973 -0.36114195 -0.02196635 -0.01083142 -0.43565977  0.20162544\n",
      "  -0.37839523 -0.03058008  0.11241785 -0.1116168  -0.43566141 -0.22307736\n",
      "  -0.04088089 -0.05692943 -0.03910248  0.13586548 -0.40517034 -0.02027969\n",
      "  -0.0939681  -0.2337238  -0.17730671 -0.24402862  0.80679029 -0.12555499\n",
      "   0.00803193 -0.13560087 -0.08861909 -0.16238534 -0.01984751 -0.15570843]\n",
      " [-0.13859318 -0.38785285 -0.00905655 -0.01831046 -0.4683428   0.28248963\n",
      "  -0.3789102  -0.02220913  0.14533943 -0.10779562 -0.44133739 -0.22014179\n",
      "  -0.0391567  -0.05698887 -0.02959407  0.15648507 -0.43919832 -0.01477667\n",
      "  -0.09406711 -0.24072684 -0.18229984 -0.24362422  0.92166269 -0.10085512\n",
      "   0.04652195 -0.13574795 -0.0886526  -0.16088546 -0.00527018 -0.145589  ]]\n",
      "2\n",
      "533 107\n",
      "[[-0.14396211 -0.17316055 -0.15416445  0.0136306   0.02217947 -0.32184197\n",
      "  -0.1948021  -0.13366467 -0.14004089 -0.12041998 -0.16351105 -0.22999836\n",
      "  -0.06676483 -0.0564702  -0.07980977 -0.10004338 -0.17717005 -0.09749223\n",
      "  -0.09310053 -0.15212725 -0.10050013 -0.23872325 -0.4682263  -0.17039574\n",
      "  -0.19234958 -0.12981084 -0.08769652 -0.13478015 -0.29590985 -0.21226139]\n",
      " [-0.14389708 -0.17554973 -0.1588803   0.10412745  0.01757264 -0.29131845\n",
      "  -0.22529948 -0.11879256 -0.12105485 -0.12336197 -0.20414538 -0.24281406\n",
      "  -0.05576301 -0.0565519  -0.08049774 -0.03376541 -0.17471241 -0.08748708\n",
      "  -0.09326808 -0.16444815 -0.13445203 -0.24309685 -0.41156213 -0.23522931\n",
      "  -0.18610289 -0.13213762 -0.08772569 -0.15059905 -0.26409438 -0.21602888]\n",
      " [-0.14352999 -0.18838535 -0.15519212  0.1082948  -0.05076364 -0.20125635\n",
      "  -0.28854921 -0.09159667 -0.06982626 -0.12208873 -0.28092644 -0.25465907\n",
      "  -0.04397677 -0.05656627 -0.07964347  0.00594889 -0.19805442 -0.07431642\n",
      "  -0.09321637 -0.20461418 -0.16403642 -0.24552272 -0.10767426 -0.26133819\n",
      "  -0.18084838 -0.13384759 -0.08793701 -0.16854198 -0.1864548  -0.21061698]\n",
      " [-0.14292374 -0.23219369 -0.15082899  0.06986485 -0.21426481 -0.1213263\n",
      "  -0.36620849 -0.07896602 -0.03576186 -0.12569791 -0.36330171 -0.26210476\n",
      "  -0.04131604 -0.05661216 -0.07891391  0.03853887 -0.25499144 -0.05996866\n",
      "  -0.09331637 -0.24568435 -0.18461854 -0.24599419  0.08393352 -0.29251073\n",
      "  -0.16478038 -0.1343549  -0.08805304 -0.17692535 -0.13143615 -0.21145181]\n",
      " [-0.14268059 -0.25789255 -0.14114864  0.0654982  -0.25662443 -0.08403417\n",
      "  -0.3768158  -0.07312945 -0.02387314 -0.127729   -0.37963843 -0.2605074\n",
      "  -0.04038163 -0.05667887 -0.07597453  0.04801298 -0.27823308 -0.05548457\n",
      "  -0.09344271 -0.24686833 -0.18600741 -0.24623057  0.16991591 -0.29516539\n",
      "  -0.14917834 -0.13448653 -0.08810645 -0.18010937 -0.11201151 -0.20622705]]\n"
     ]
    }
   ],
   "source": [
    "ff = obj2.traversal_all_groups(traversal_step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyAzr8KSAgH"
   },
   "source": [
    "# See the interpolation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lWzpnwK0g-8g"
   },
   "outputs": [],
   "source": [
    "with open('results_dict.pkl', 'rb') as f:\n",
    "    ff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "p23aMsKEknL4",
    "outputId": "29141142-bd3d-4f66-c936-9479312bcba9"
   },
   "outputs": [],
   "source": [
    "ff['med']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hMMGIGAslEoz",
    "outputId": "52e9d86d-dcb8-46c1-dfbf-1866da50861b"
   },
   "outputs": [],
   "source": [
    "ff['mean']['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d121e4bcd324a589a371764768ad7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='celltypes', options=('0', '1', '2'), value='0'), Dropdown(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_path_prediction(celltypes, genes)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, Dropdown\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_path_prediction(celltypes, genes):\n",
    "    # generate plot based on a and b\n",
    "    plt.plot(ff['mean'][celltypes][genes])\n",
    "    plt.xlabel('healthy to cancer')\n",
    "    plt.ylabel('normalized gene expression')\n",
    "    plt.title('Linear interpolation for cell-type '+celltypes+\" for gene \"+genes)\n",
    "    plt.show()\n",
    "    \n",
    "    gg=df_XY.groupby(['Y','YY']).mean().reset_index(drop=False)\n",
    "    gg=gg.loc[gg['Y']==celltype,:]\n",
    "    plt.plot(gg['YY'].tolist(),gg[genes].tolist(),'-')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# get the list of possible values for a and b\n",
    "a_values = list(set(obj2.df_XY['Y']))\n",
    "a_values = list(map(str, a_values))\n",
    "b_values = obj2.df_XY.columns.tolist()[:-1]\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# create the dropdown widgets\n",
    "a_widget = Dropdown(options=a_values)\n",
    "b_widget = Dropdown(options=b_values)\n",
    "\n",
    "# use the interact function to bind the widgets and the function\n",
    "output1 = interact(plot_path_prediction, celltypes=a_widget, genes=b_widget)\n",
    "\n",
    "# display the output in an HTML page\n",
    "display(output1)\n",
    "\n",
    "#output2 = interact(plot_path_truth, celltypes=a_widget, genes=b_widget)\n",
    "\n",
    "# display the output in an HTML page\n",
    "#display(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHBvdSESFRV"
   },
   "source": [
    "# Generate Synthetic Data for a Given Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ_3NAgi2-6L",
    "outputId": "a11c8fd5-41de-40d3-dee2-91580cb322a9"
   },
   "outputs": [],
   "source": [
    "bb = obj2.synthetic_single_group(group_id=0,nr_of_synthetic=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E330szfA6BD7",
    "outputId": "a8687ace-e5b6-46d2-904d-e854e532d0d8"
   },
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey_yS1WZ3Os2",
    "outputId": "dbeb2f28-2746-4a69-beeb-a934f19eb8a1"
   },
   "outputs": [],
   "source": [
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
