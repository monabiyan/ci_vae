{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhV9PPiQKgSg"
   },
   "source": [
    "# Generate Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9HIiKcw_PCm5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "np.random.seed(0)\n",
    "n_samples=1000\n",
    "n_features = 5\n",
    "df_XY=pd.DataFrame(data = np.random.normal(0,1, size=(n_samples, n_features)), columns = ['A','B','C','D','E'])\n",
    "df_XY['Y']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY['YY']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY\n",
    "\n",
    "##############################################################   \n",
    "df_XY.shape\n",
    "df_XY.head()\n",
    "df_XY.to_csv('df_XY.csv',index=False)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "31zCNsOv0BoE",
    "outputId": "b7af009e-8ad8-463e-805d-07a8d5e7e082"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>Y</th>\n",
       "      <th>YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.764052</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>2.240893</td>\n",
       "      <td>1.867558</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.977278</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.103219</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144044</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333674</td>\n",
       "      <td>1.494079</td>\n",
       "      <td>-0.205158</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>-0.854096</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.552990</td>\n",
       "      <td>0.653619</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>-0.742165</td>\n",
       "      <td>2.269755</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.711489</td>\n",
       "      <td>-1.820816</td>\n",
       "      <td>0.163495</td>\n",
       "      <td>-0.813117</td>\n",
       "      <td>-0.605355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.327524</td>\n",
       "      <td>-0.644172</td>\n",
       "      <td>1.908883</td>\n",
       "      <td>-0.563545</td>\n",
       "      <td>1.082473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-1.951911</td>\n",
       "      <td>2.441216</td>\n",
       "      <td>-0.017285</td>\n",
       "      <td>0.912282</td>\n",
       "      <td>1.239658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.573367</td>\n",
       "      <td>0.424889</td>\n",
       "      <td>-0.271260</td>\n",
       "      <td>-0.683568</td>\n",
       "      <td>-1.537438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.101374</td>\n",
       "      <td>0.746666</td>\n",
       "      <td>0.929182</td>\n",
       "      <td>0.229418</td>\n",
       "      <td>0.414406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         B         C         D         E  Y  YY\n",
       "0    1.764052  0.400157  0.978738  2.240893  1.867558  1   1\n",
       "1   -0.977278  0.950088 -0.151357 -0.103219  0.410599  0   0\n",
       "2    0.144044  1.454274  0.761038  0.121675  0.443863  0   0\n",
       "3    0.333674  1.494079 -0.205158  0.313068 -0.854096  1   0\n",
       "4   -2.552990  0.653619  0.864436 -0.742165  2.269755  0   1\n",
       "..        ...       ...       ...       ...       ... ..  ..\n",
       "995  1.711489 -1.820816  0.163495 -0.813117 -0.605355  0   0\n",
       "996 -1.327524 -0.644172  1.908883 -0.563545  1.082473  1   0\n",
       "997 -1.951911  2.441216 -0.017285  0.912282  1.239658  1   1\n",
       "998 -0.573367  0.424889 -0.271260 -0.683568 -1.537438  1   1\n",
       "999 -0.101374  0.746666  0.929182  0.229418  0.414406  0   1\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77NKQyeKwIj"
   },
   "source": [
    "# Download CI-VAE, other necessary packages and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cAuVLcUNETr7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: bb.pt: No such file or directory\n",
      "rm: bb_residuals.pkl: No such file or directory\n",
      "rm: df_reconstructed.csv: No such file or directory\n",
      "rm: df_reconstructed_decoder.csv: No such file or directory\n",
      "rm: residuals.pdf: No such file or directory\n",
      "rm: results_dict.pkl: No such file or directory\n",
      "rm: df_latent.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ci_vae\n",
    "! rm bb.pt\n",
    "! rm bb_residuals.pkl\n",
    "! rm df_reconstructed.csv\n",
    "! rm df_reconstructed_decoder.csv\n",
    "! rm residuals.pdf\n",
    "! rm results_dict.pkl\n",
    "! rm df_latent.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "18S0saDPLp0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ci_vae'...\n",
      "remote: Enumerating objects: 314, done.\u001b[K\n",
      "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
      "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
      "remote: Total 314 (delta 41), reused 50 (delta 22), pack-reused 245\u001b[K\n",
      "Receiving objects: 100% (314/314), 46.61 MiB | 1.59 MiB/s, done.\n",
      "Resolving deltas: 100% (197/197), done.\n",
      "Requirement already satisfied: umap-learn in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.19.2)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.51.2)\n",
      "Requirement already satisfied: tqdm in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (4.50.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.23.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: llvmlite>=0.30 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (0.34.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (50.3.1.post20201107)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/monabiyan/ci_vae.git\n",
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kzwk1I17VAQx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ci_vae import ivae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPVcV9thL8SP"
   },
   "source": [
    "# Set Necessary Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "KAVb-irtbIpc"
   },
   "outputs": [],
   "source": [
    "model_init=True\n",
    "model_tobe_trained=True\n",
    "save_address=\"bb\"\n",
    "\n",
    "kl_coef = 0.0001\n",
    "reconst_coef = 1\n",
    "classifier_coef = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRIfHjpSMKF5"
   },
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ll0w2DunMJei"
   },
   "outputs": [],
   "source": [
    "obj1 = ivae.IVAE(df_XY = df_XY,\n",
    "               latent_size = 2,\n",
    "               reconst_coef = reconst_coef,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "if model_init:\n",
    "    obj1.model_initialiaze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT-AB_8-M-tD"
   },
   "source": [
    "## See The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJXlsM8Uk2Ry",
    "outputId": "129babee-fc0e-48c5-9262-08404a9b89c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVAE_ARCH(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=20, out_features=5, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Dropout(p=0.8, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSj9WT_mNHNl"
   },
   "source": [
    "## See the Initialized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUFyVcu4ldFH",
    "outputId": "5a470060-626d-4602-dcf4-78d601febd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1116, -0.0716,  0.1774, -0.0217,  0.1232],\n",
      "        [-0.2089,  0.2904, -0.3151,  0.0018,  0.1702],\n",
      "        [ 0.4039,  0.0059, -0.3560,  0.0748,  0.2586],\n",
      "        [-0.0390, -0.4365, -0.1357,  0.1875,  0.0650],\n",
      "        [ 0.0009, -0.2495, -0.1444,  0.3428, -0.4271],\n",
      "        [-0.2311, -0.2492,  0.0734, -0.3642, -0.2027],\n",
      "        [ 0.1185, -0.0714,  0.1192, -0.3464,  0.0147],\n",
      "        [-0.4005,  0.0036, -0.4447,  0.2115, -0.2491],\n",
      "        [-0.2210,  0.1760,  0.4244,  0.0452,  0.2285],\n",
      "        [-0.2177,  0.1071,  0.1869, -0.2493,  0.0275],\n",
      "        [ 0.3529,  0.3704,  0.3200, -0.1589, -0.3206],\n",
      "        [ 0.2002, -0.1411, -0.2441,  0.3281,  0.3959],\n",
      "        [ 0.3157,  0.1002, -0.1249,  0.2184, -0.4051],\n",
      "        [-0.3974, -0.0537,  0.0541, -0.1319,  0.3722],\n",
      "        [-0.1814, -0.2998,  0.1121, -0.4305,  0.1683],\n",
      "        [-0.4133,  0.1166, -0.4253, -0.1438, -0.1429],\n",
      "        [ 0.3261,  0.3583,  0.1989, -0.1723,  0.3408],\n",
      "        [-0.0235,  0.2925, -0.0515, -0.1799, -0.3013],\n",
      "        [-0.1893,  0.3991,  0.4006,  0.1226, -0.3867],\n",
      "        [-0.4393, -0.2489, -0.3828, -0.1306, -0.0078]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3308,  0.1143, -0.0494, -0.4163, -0.3740, -0.3528,  0.1924,  0.3893,\n",
      "         0.2632,  0.0289,  0.4080, -0.0705,  0.0985,  0.1679,  0.2182,  0.0371,\n",
      "        -0.3470, -0.2060, -0.2105,  0.2108], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 4.1630e-02, -1.4847e-02,  8.0380e-02,  8.1446e-02, -8.8439e-03,\n",
      "         -3.4530e-02, -3.8285e-02,  2.0062e-01,  1.3099e-01, -3.2322e-02,\n",
      "          4.1759e-02,  1.5777e-01,  1.9599e-01, -1.4551e-01, -8.0392e-02,\n",
      "          1.5220e-01,  2.0325e-01, -1.4211e-02, -9.4149e-02,  2.3909e-02],\n",
      "        [ 3.4435e-02,  1.9863e-01,  2.1027e-01,  2.1110e-01, -4.3423e-02,\n",
      "         -8.8447e-02,  1.3791e-01,  4.3827e-03,  1.6164e-01, -1.4926e-01,\n",
      "          1.4655e-01,  2.7917e-02,  6.2779e-02,  1.4106e-01,  1.4815e-02,\n",
      "          4.1591e-02, -1.0313e-01, -3.6184e-02, -9.8564e-02,  9.6103e-02],\n",
      "        [ 2.0742e-01, -8.0601e-02, -4.9765e-02,  1.2663e-01, -1.7653e-01,\n",
      "          1.0825e-01, -1.2344e-01, -2.0533e-01,  8.5746e-03, -8.5219e-02,\n",
      "         -9.5909e-02, -9.2463e-02,  2.5738e-02,  1.5678e-01, -7.6171e-02,\n",
      "         -2.1423e-01, -3.4634e-02, -1.1630e-01,  1.3171e-01,  9.6363e-02],\n",
      "        [-1.0932e-01, -8.4379e-02, -2.6037e-02,  2.0576e-01, -1.8073e-01,\n",
      "         -8.1969e-02,  4.5273e-02,  1.8634e-02,  3.0624e-02, -5.3498e-02,\n",
      "          2.1466e-01, -2.2280e-01, -2.9759e-02,  7.1844e-02, -1.9914e-01,\n",
      "          1.9698e-01, -2.0359e-01,  4.1223e-02,  1.0763e-01,  1.9367e-01],\n",
      "        [-1.1434e-03, -7.8737e-02,  1.5304e-01, -4.4156e-03, -9.6203e-02,\n",
      "         -1.7544e-01,  1.2228e-01, -8.0791e-02, -8.2926e-02,  9.0546e-02,\n",
      "          3.2147e-02, -1.0647e-01, -3.8549e-02,  6.5838e-02, -8.7401e-02,\n",
      "          4.5504e-03, -9.6656e-02,  2.1331e-01,  4.7462e-02,  1.4846e-01],\n",
      "        [-3.5452e-02,  8.3378e-02, -8.0258e-02,  1.4269e-01,  1.4194e-01,\n",
      "          4.4700e-02,  1.2066e-02, -2.0933e-01,  1.7989e-01, -2.8650e-02,\n",
      "          3.1652e-02,  1.0837e-01, -6.0168e-02,  1.8230e-01, -1.3771e-01,\n",
      "          1.7069e-01, -2.4939e-02, -2.0172e-01,  5.6601e-02, -6.9189e-02],\n",
      "        [ 9.4523e-02,  9.2122e-02,  1.7508e-01, -5.9117e-03, -2.1669e-01,\n",
      "         -3.9565e-02, -4.9128e-02,  2.5625e-02, -2.1610e-01,  1.6882e-01,\n",
      "          1.3668e-01,  1.0648e-01,  7.0276e-02, -1.3383e-01, -6.5263e-02,\n",
      "          1.7801e-02, -8.6534e-02,  2.0565e-01, -7.1771e-02, -1.4760e-01],\n",
      "        [ 1.2051e-02,  1.3225e-01, -9.5135e-02,  1.4393e-01,  1.7161e-01,\n",
      "         -3.7290e-02,  8.6694e-02,  7.5634e-03, -2.0730e-02, -1.5297e-01,\n",
      "          1.0182e-01,  1.1187e-01, -1.0106e-01,  1.4792e-01,  1.7049e-01,\n",
      "          1.1412e-02, -1.1567e-01,  1.7881e-02,  9.6472e-03,  1.1238e-01],\n",
      "        [ 1.9789e-01, -7.0924e-04,  2.1637e-01,  1.1641e-01, -6.7974e-02,\n",
      "          1.4688e-01,  4.1576e-02, -9.7897e-02,  1.1845e-01,  9.4444e-05,\n",
      "         -8.8818e-02,  2.1441e-01,  4.6308e-02,  7.1271e-02,  1.8636e-01,\n",
      "          1.8184e-01, -1.9009e-01, -1.5148e-01,  2.0647e-01, -2.8297e-02],\n",
      "        [ 1.0905e-01,  1.6918e-01,  8.0073e-02,  2.1759e-01, -1.0444e-01,\n",
      "         -5.0495e-02,  1.6174e-02,  2.3422e-02,  5.5082e-03,  2.0400e-01,\n",
      "          1.5553e-01,  8.9200e-02, -1.7104e-01,  2.0292e-02,  1.2288e-01,\n",
      "          1.4206e-01,  2.3678e-02,  7.2541e-02, -2.1982e-02,  2.1932e-01],\n",
      "        [ 3.3315e-03, -2.1025e-01, -4.0339e-02,  1.3289e-01,  8.3860e-02,\n",
      "         -1.3002e-01, -1.4994e-01, -2.1572e-01, -8.4166e-02, -1.8546e-01,\n",
      "         -1.8834e-01, -3.5508e-03, -8.0431e-02,  2.1688e-01, -1.2877e-01,\n",
      "          3.3966e-02, -1.2411e-01, -1.3636e-01,  1.8466e-01, -1.8115e-01],\n",
      "        [-1.4595e-01,  2.1149e-01, -4.9071e-02, -1.8007e-01, -1.9775e-01,\n",
      "          1.6520e-01, -3.5528e-02,  9.2999e-02, -2.1345e-01, -1.9270e-01,\n",
      "          3.4563e-02, -1.2929e-01, -9.3613e-02,  2.1331e-01, -1.3098e-03,\n",
      "         -1.2657e-01, -6.5240e-02, -1.7612e-01, -1.5489e-01,  1.0490e-01],\n",
      "        [-1.6767e-01,  7.3367e-02, -3.4035e-03, -1.2444e-01, -4.1091e-02,\n",
      "          2.2019e-01,  9.0814e-02, -2.1719e-01,  1.7577e-03, -1.2678e-01,\n",
      "          1.3526e-01, -1.6933e-01,  1.9295e-01, -2.4709e-02, -1.7920e-01,\n",
      "         -9.3151e-02, -5.1112e-03,  4.9852e-02,  5.4270e-02,  6.9252e-02],\n",
      "        [-1.4879e-01,  6.5469e-02,  4.6340e-03, -3.0100e-02,  1.4139e-01,\n",
      "         -1.4440e-01, -1.8522e-02,  8.5324e-02, -1.4016e-03, -4.6292e-02,\n",
      "          7.1442e-03, -1.7149e-01, -1.0701e-01, -1.1142e-01,  3.8206e-02,\n",
      "          1.1532e-01,  2.2212e-01,  1.6390e-02, -3.6680e-02, -1.9797e-02],\n",
      "        [-5.1468e-02, -7.2965e-02,  3.8089e-02,  1.2343e-02,  1.5961e-01,\n",
      "          1.7242e-01,  1.7757e-01, -1.7547e-01,  1.7746e-02, -3.2583e-02,\n",
      "          1.2450e-01,  1.4247e-01,  1.3007e-02, -2.1791e-01,  1.5818e-01,\n",
      "          5.6843e-02,  1.4947e-02, -1.6406e-01, -2.1060e-01,  6.7091e-02],\n",
      "        [ 1.0477e-01, -2.1738e-01,  1.8260e-02, -1.6859e-01, -1.4738e-01,\n",
      "         -9.2340e-02,  1.4693e-01, -2.0781e-01,  1.9403e-01, -1.8001e-01,\n",
      "          1.7499e-01, -1.1458e-01,  1.4121e-01,  2.8839e-02, -9.1863e-02,\n",
      "          9.6096e-02,  4.9218e-02,  8.0736e-02, -1.4167e-02,  1.5036e-01],\n",
      "        [-5.8049e-02,  1.0356e-01,  5.5952e-02, -5.0407e-02,  1.0165e-01,\n",
      "         -1.1016e-01, -1.9793e-01, -9.5871e-02, -2.0910e-01, -1.7701e-01,\n",
      "         -1.7346e-02,  2.3000e-02,  1.2908e-02, -2.0636e-02,  1.3673e-01,\n",
      "          9.9249e-02,  1.1892e-01, -9.4481e-02, -1.5781e-01, -1.0363e-01],\n",
      "        [ 1.8016e-01, -1.9439e-01,  1.7183e-02, -3.7916e-02, -1.4291e-01,\n",
      "         -1.2660e-01,  8.2562e-02,  4.3602e-03, -8.7447e-02,  1.0543e-01,\n",
      "          3.9835e-02, -1.4069e-01,  1.2439e-01, -5.8712e-02, -1.0030e-01,\n",
      "         -7.1963e-02,  5.3554e-02,  1.9559e-01, -8.7872e-02,  1.7812e-01],\n",
      "        [ 5.4216e-02,  1.4842e-01,  6.7542e-02, -1.0783e-01, -1.2061e-01,\n",
      "         -2.1585e-01,  8.7286e-02,  6.7657e-02, -2.1677e-01, -3.0071e-02,\n",
      "         -4.8682e-02, -1.2736e-02, -1.8947e-01, -2.1234e-01,  1.2200e-01,\n",
      "          2.1734e-01, -1.4064e-01,  1.0401e-01, -1.3401e-02, -8.7607e-02],\n",
      "        [-1.9036e-01,  1.1832e-01,  3.9440e-02, -1.8603e-01, -1.9956e-01,\n",
      "         -3.2529e-02, -1.2277e-01, -1.1929e-01,  1.1464e-01, -1.0206e-01,\n",
      "         -1.2971e-01, -1.1062e-01, -2.2206e-02, -1.7933e-01, -4.5452e-02,\n",
      "          1.7496e-01,  3.9304e-02, -1.8447e-01, -8.9474e-02,  4.0803e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1544,  0.0718, -0.1614,  0.1501, -0.0452,  0.1771, -0.1950, -0.1242,\n",
      "         0.0275,  0.1757, -0.0568,  0.0933,  0.1946, -0.0635,  0.0171, -0.1511,\n",
      "        -0.1424,  0.0355,  0.1541, -0.0160], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1243, -0.0529, -0.0135,  0.1006,  0.0944,  0.1787, -0.1581,  0.0233,\n",
      "          0.1721, -0.0566,  0.0642,  0.0812,  0.1811, -0.0848,  0.1856, -0.0590,\n",
      "          0.1468, -0.1601, -0.1870, -0.0043],\n",
      "        [-0.1590, -0.0272,  0.0693,  0.0452,  0.1203,  0.1214,  0.2162,  0.0309,\n",
      "         -0.0987,  0.0182, -0.0732,  0.0479, -0.1972, -0.1910,  0.0302,  0.0224,\n",
      "         -0.2004,  0.0488, -0.0755, -0.1759],\n",
      "        [ 0.0166, -0.0425, -0.0499, -0.1090, -0.0024,  0.1321, -0.0751, -0.1951,\n",
      "         -0.0488,  0.0178, -0.0134, -0.0952,  0.1564,  0.0779,  0.1082,  0.0044,\n",
      "          0.1748,  0.1340,  0.0098,  0.2195],\n",
      "        [ 0.0173, -0.0275, -0.0747,  0.1886, -0.2198,  0.1764,  0.1791,  0.2055,\n",
      "         -0.1299, -0.2035,  0.0617, -0.1572,  0.1694, -0.1430,  0.0504, -0.1290,\n",
      "          0.1617, -0.1559, -0.0091,  0.1771],\n",
      "        [-0.0603, -0.0643, -0.0206, -0.1490, -0.0918, -0.0429, -0.0686, -0.1141,\n",
      "         -0.2023,  0.0676, -0.1799, -0.0864,  0.0348, -0.1400,  0.1993, -0.0313,\n",
      "         -0.1103,  0.1331,  0.0462,  0.1551],\n",
      "        [ 0.1398, -0.0328, -0.0632, -0.2222,  0.0969, -0.0360,  0.1854, -0.1935,\n",
      "          0.0528,  0.0309,  0.0480, -0.0965,  0.2116,  0.2061, -0.1507, -0.1221,\n",
      "         -0.1197, -0.1305,  0.1416, -0.1126],\n",
      "        [-0.0671, -0.1546,  0.1529, -0.0523, -0.1792, -0.1213,  0.2224,  0.2221,\n",
      "          0.2037,  0.0346, -0.2171,  0.1802,  0.1391, -0.1630,  0.0260, -0.0659,\n",
      "         -0.1055, -0.0033,  0.0013, -0.0468],\n",
      "        [ 0.0313, -0.2176,  0.2068,  0.0624, -0.1919,  0.1874, -0.2081,  0.2179,\n",
      "         -0.1236,  0.1947, -0.1926,  0.1986,  0.1829,  0.0251,  0.1988,  0.0253,\n",
      "          0.1588, -0.1783, -0.0845,  0.1866],\n",
      "        [ 0.0867, -0.0306, -0.0346, -0.0789,  0.1286,  0.1247, -0.1075,  0.0016,\n",
      "         -0.2159, -0.0105, -0.1532, -0.1320,  0.0175, -0.2011, -0.0349, -0.0713,\n",
      "         -0.2044,  0.0523,  0.1686,  0.0176],\n",
      "        [-0.0013,  0.0625,  0.0967,  0.2208,  0.0501,  0.0906,  0.1208,  0.1062,\n",
      "          0.1900, -0.1172,  0.1161, -0.2161, -0.1547,  0.1818, -0.0751, -0.1919,\n",
      "          0.1380, -0.0695, -0.1795,  0.0589],\n",
      "        [-0.0244,  0.0161, -0.0517,  0.0985, -0.0428, -0.0333, -0.1920, -0.0685,\n",
      "          0.0128, -0.1540, -0.2225,  0.0038,  0.2050,  0.0308, -0.1520, -0.0486,\n",
      "          0.0269,  0.0696,  0.0338,  0.1074],\n",
      "        [ 0.0868, -0.0169, -0.0458, -0.0746, -0.1613,  0.1075,  0.1907, -0.1005,\n",
      "         -0.1569, -0.1203, -0.0929,  0.0946, -0.1003, -0.0968, -0.1207, -0.1121,\n",
      "          0.0647, -0.0327, -0.0346,  0.1757],\n",
      "        [ 0.1356, -0.1603, -0.2134,  0.0920, -0.1238,  0.0028, -0.1658, -0.1548,\n",
      "          0.0533,  0.0931, -0.1265, -0.1684,  0.1606, -0.1597,  0.2108,  0.0236,\n",
      "          0.1693,  0.0017,  0.2191, -0.1422],\n",
      "        [ 0.0489,  0.0947,  0.1911,  0.1673,  0.0521,  0.0283,  0.1314, -0.0217,\n",
      "         -0.0130, -0.0455,  0.1585,  0.1258, -0.0301, -0.0782, -0.1217,  0.0021,\n",
      "          0.1764, -0.1897,  0.1849,  0.0865],\n",
      "        [ 0.2216,  0.0105, -0.0964,  0.0015,  0.1053, -0.0973, -0.0450, -0.1752,\n",
      "         -0.1002, -0.1333, -0.0579, -0.0981,  0.0703, -0.0873, -0.0032, -0.1161,\n",
      "         -0.1396, -0.0089,  0.1401, -0.1150],\n",
      "        [ 0.1532,  0.1859, -0.2069,  0.0226,  0.0645,  0.0652,  0.0379, -0.2048,\n",
      "          0.1625, -0.1107, -0.0324,  0.0271, -0.1428,  0.1821, -0.1408, -0.1497,\n",
      "          0.0355, -0.0453, -0.1752, -0.1706],\n",
      "        [-0.1824,  0.1649, -0.1739, -0.2044, -0.1873,  0.0101,  0.1625, -0.1191,\n",
      "         -0.1609,  0.0536,  0.2110,  0.2154,  0.1066,  0.1447, -0.1760, -0.0406,\n",
      "         -0.0632, -0.0546,  0.1850, -0.1342],\n",
      "        [-0.1173,  0.2109, -0.0910,  0.2084,  0.0587,  0.0372, -0.0143,  0.1123,\n",
      "          0.1158,  0.1313,  0.1807, -0.0798, -0.1324,  0.0944, -0.1508,  0.1528,\n",
      "          0.0443,  0.0841, -0.1272,  0.1180],\n",
      "        [-0.0253, -0.2092,  0.0786, -0.1928, -0.1800, -0.0831,  0.0983,  0.0076,\n",
      "          0.0602, -0.1181, -0.0598,  0.0639, -0.0390,  0.0718,  0.1986,  0.1371,\n",
      "          0.1399, -0.1214, -0.1000,  0.1995],\n",
      "        [-0.2001,  0.0123, -0.0433, -0.1340, -0.2081, -0.0754,  0.0224, -0.0822,\n",
      "          0.2219, -0.1465,  0.0936, -0.0861, -0.1702, -0.1951, -0.0969,  0.0887,\n",
      "          0.2143,  0.0332, -0.1338,  0.0395]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1512,  0.0748, -0.1848,  0.1660, -0.2091, -0.1461, -0.2131, -0.0851,\n",
      "        -0.0067, -0.1881,  0.1110,  0.1414,  0.0707,  0.1367,  0.0345,  0.2156,\n",
      "         0.0822, -0.1110, -0.2071, -0.1772], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0745,  0.2177, -0.0474,  0.0783,  0.0390,  0.1965,  0.0990, -0.1941,\n",
      "         -0.1737, -0.2186,  0.0788, -0.0788, -0.2118,  0.0766,  0.0962,  0.0641,\n",
      "          0.1887, -0.1653,  0.1890, -0.1941],\n",
      "        [-0.2154, -0.1916,  0.0019, -0.0032, -0.1566,  0.2029, -0.0855, -0.0680,\n",
      "          0.1757, -0.0300, -0.0464,  0.0326,  0.1595, -0.1095,  0.2218, -0.0528,\n",
      "         -0.1712,  0.0849, -0.1232, -0.0424],\n",
      "        [ 0.0994,  0.0127,  0.0501, -0.1220,  0.0383,  0.0937, -0.1347, -0.1036,\n",
      "          0.1766, -0.1204,  0.0919, -0.0878, -0.1579, -0.0698, -0.0884, -0.1485,\n",
      "          0.0894, -0.1201,  0.1797, -0.0050],\n",
      "        [ 0.1922,  0.1315, -0.2151, -0.1972,  0.0726, -0.0727, -0.0153,  0.1869,\n",
      "         -0.1155, -0.1096, -0.1658, -0.0038, -0.0240,  0.1246, -0.0635,  0.1604,\n",
      "          0.0328,  0.1209,  0.0933,  0.0141],\n",
      "        [ 0.1289, -0.0190, -0.2057,  0.1724, -0.2114, -0.1074, -0.1505,  0.1118,\n",
      "         -0.2062,  0.0220, -0.0178, -0.0813, -0.1496,  0.1744,  0.0947, -0.0808,\n",
      "          0.0344, -0.1199, -0.0434,  0.0928],\n",
      "        [-0.2070, -0.0627, -0.0056,  0.2114,  0.0941, -0.0598, -0.1576,  0.0531,\n",
      "          0.0269, -0.0811, -0.2124,  0.1352,  0.0474, -0.0451, -0.0716, -0.1173,\n",
      "          0.0088, -0.1703, -0.1525,  0.0957],\n",
      "        [ 0.1800,  0.1666,  0.0245, -0.1748,  0.2144, -0.0359, -0.0451, -0.0608,\n",
      "         -0.0830,  0.1693,  0.1351,  0.0869,  0.1889, -0.0520,  0.2144,  0.2160,\n",
      "          0.0724, -0.0735,  0.1469,  0.1440],\n",
      "        [-0.0466,  0.0667,  0.0630, -0.1576,  0.1057, -0.1118,  0.0051, -0.0284,\n",
      "         -0.0857, -0.0232, -0.0534, -0.1437, -0.2180,  0.0476, -0.0465, -0.1848,\n",
      "          0.0176, -0.2085, -0.0978, -0.1640],\n",
      "        [ 0.1332, -0.0601, -0.0394, -0.1356, -0.2038, -0.1700,  0.1621,  0.2158,\n",
      "         -0.1699,  0.0272,  0.1750,  0.2125, -0.2024, -0.0398, -0.1209, -0.1021,\n",
      "         -0.0848, -0.1618, -0.1400, -0.1751],\n",
      "        [-0.0981,  0.0824,  0.0924, -0.0924,  0.2107, -0.0438,  0.1187, -0.2132,\n",
      "         -0.1174, -0.0683,  0.1594, -0.2040, -0.0835, -0.0085,  0.0062,  0.1602,\n",
      "          0.0502, -0.0797, -0.0241,  0.0955],\n",
      "        [ 0.1781,  0.0564,  0.0424,  0.0580,  0.1353,  0.0097, -0.1310, -0.0454,\n",
      "         -0.0194,  0.0868,  0.0192,  0.1233, -0.0623, -0.1044,  0.2124,  0.1093,\n",
      "         -0.0279,  0.1481, -0.1030,  0.1948],\n",
      "        [-0.0779, -0.0308,  0.0312,  0.1547, -0.0062, -0.0606,  0.1628, -0.1415,\n",
      "         -0.1294,  0.1891, -0.0248, -0.0760, -0.0902,  0.1080, -0.0080, -0.0281,\n",
      "          0.0848,  0.1998, -0.1410, -0.0461],\n",
      "        [ 0.1106,  0.1197,  0.1204, -0.0868,  0.0121,  0.0771,  0.0874,  0.0831,\n",
      "         -0.0240,  0.1041,  0.1116, -0.0576, -0.1413, -0.0486,  0.0852, -0.0118,\n",
      "          0.0835, -0.0493, -0.0656, -0.1696],\n",
      "        [ 0.2179,  0.0498,  0.0034, -0.1964, -0.2027,  0.1048,  0.0936,  0.0819,\n",
      "         -0.1682, -0.1750,  0.2042,  0.1268, -0.1791,  0.1820, -0.0306, -0.0832,\n",
      "         -0.0287,  0.2112,  0.1963,  0.0252],\n",
      "        [-0.0710, -0.1169, -0.2203, -0.2040, -0.1171, -0.1641, -0.2172,  0.1986,\n",
      "          0.0206, -0.1043,  0.2046, -0.1691,  0.0188, -0.1214, -0.0437,  0.1413,\n",
      "          0.0969, -0.0227,  0.0493,  0.1502],\n",
      "        [ 0.1614, -0.0346, -0.1035, -0.1673, -0.0302,  0.1105,  0.0730,  0.1614,\n",
      "          0.1562,  0.1674,  0.0087, -0.1709, -0.0459, -0.1502, -0.2222, -0.2025,\n",
      "         -0.1713, -0.0934, -0.1112,  0.1880],\n",
      "        [-0.0084,  0.1260,  0.0986,  0.0487, -0.1092, -0.1128, -0.1062, -0.1893,\n",
      "          0.1196, -0.0743,  0.0318,  0.0541,  0.0650, -0.1965, -0.1534, -0.0401,\n",
      "          0.1672,  0.0559,  0.0448,  0.0505],\n",
      "        [ 0.0148,  0.1783, -0.1200, -0.0236, -0.1911, -0.2056,  0.1670, -0.1005,\n",
      "         -0.1854, -0.0931,  0.0075, -0.1122,  0.1841,  0.0353, -0.1896,  0.0441,\n",
      "         -0.0874,  0.1476, -0.1882, -0.0510],\n",
      "        [-0.0019,  0.0228, -0.0914,  0.1082, -0.0199,  0.0580, -0.1781,  0.0563,\n",
      "          0.0594,  0.1190,  0.0450,  0.0545,  0.1601, -0.0599,  0.1399, -0.0862,\n",
      "          0.0344,  0.0103, -0.2146, -0.2231],\n",
      "        [-0.0025, -0.1093, -0.1449,  0.2074, -0.2217,  0.1578, -0.1077,  0.2025,\n",
      "          0.0639, -0.0705,  0.1447,  0.2170, -0.1093, -0.1383,  0.2034,  0.0754,\n",
      "         -0.0535,  0.1226, -0.0099, -0.0771]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1130,  0.1330,  0.0785,  0.0639,  0.2057, -0.0050,  0.1632, -0.1693,\n",
      "         0.0222,  0.0583,  0.0812, -0.1988, -0.1420,  0.0476,  0.0483,  0.2228,\n",
      "         0.0806, -0.0252,  0.2222,  0.1237], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.3428e-01,  1.3072e-01,  1.5423e-01, -5.5840e-02,  7.9118e-02,\n",
      "         -1.8912e-01, -1.9372e-01, -1.7728e-01,  1.9580e-01, -7.7299e-02,\n",
      "         -4.0273e-02, -3.2134e-03, -2.2029e-01,  1.9788e-01, -1.0330e-01,\n",
      "         -2.1956e-02, -1.7490e-01,  8.1023e-02, -1.9251e-01, -1.2993e-01],\n",
      "        [ 1.8986e-01,  2.8806e-02, -2.1859e-01, -1.9816e-01, -8.5701e-02,\n",
      "          2.0491e-01, -1.3661e-01, -5.5499e-02, -1.3005e-01,  3.2064e-02,\n",
      "         -4.2716e-02, -2.0784e-01, -1.8210e-01,  1.0877e-01,  1.8094e-01,\n",
      "         -2.0531e-01,  2.0762e-04,  1.1852e-01, -2.0598e-01,  1.4906e-01],\n",
      "        [ 5.4062e-02, -6.8604e-03, -1.7351e-01,  1.7614e-01,  1.5701e-01,\n",
      "         -1.0280e-01,  1.7102e-01, -1.1917e-01,  1.7034e-01,  7.4235e-02,\n",
      "          2.3511e-03, -1.8256e-01,  1.1307e-01,  1.2244e-01, -4.7443e-02,\n",
      "         -8.4350e-02,  1.2001e-01, -3.6068e-02, -6.7789e-02,  1.3677e-01],\n",
      "        [-8.8980e-02, -1.0556e-01, -6.5146e-02,  1.7002e-01, -1.3048e-01,\n",
      "          1.3787e-01,  4.4644e-02,  1.4642e-01,  6.8901e-02, -9.7308e-02,\n",
      "         -5.6446e-02, -4.7008e-02,  8.0145e-02, -1.3536e-01, -1.3696e-01,\n",
      "          1.5199e-01,  1.9129e-01, -9.8898e-02,  1.3270e-01, -1.0497e-01],\n",
      "        [-1.3533e-01,  1.1830e-01, -2.5963e-02, -2.0152e-01, -3.8573e-02,\n",
      "          1.6174e-01, -1.8914e-01,  8.2350e-02, -1.0176e-01,  5.9049e-02,\n",
      "          9.4112e-03,  4.1447e-02,  1.9544e-01, -7.9304e-02,  1.8984e-01,\n",
      "          1.0753e-02,  1.3813e-01, -6.9115e-02,  1.1853e-01,  3.1200e-02],\n",
      "        [-7.3231e-02, -6.2382e-02,  7.6972e-02,  1.4263e-01,  7.3260e-02,\n",
      "          1.2711e-01,  7.3596e-02, -1.9074e-02, -3.9142e-02, -6.8559e-02,\n",
      "         -3.1675e-02,  1.6712e-01,  2.4439e-02, -2.2131e-01, -8.9327e-02,\n",
      "         -1.7165e-01, -2.0167e-01, -7.0029e-02, -1.7452e-01,  3.8548e-02],\n",
      "        [-1.6355e-01, -7.1847e-02, -1.9646e-01, -1.8999e-02,  5.0599e-02,\n",
      "         -1.8563e-03,  1.0715e-01, -4.6656e-02,  1.5136e-01,  2.3556e-02,\n",
      "          4.2526e-02,  1.5654e-01,  1.6601e-01,  6.4963e-02, -7.7090e-02,\n",
      "         -2.1011e-01,  2.1159e-01, -9.0799e-02,  1.6746e-01, -2.1001e-01],\n",
      "        [ 1.2726e-02,  1.9971e-02, -7.2830e-02, -2.9250e-03,  1.4749e-01,\n",
      "         -9.4274e-02,  1.1498e-01, -2.1261e-01,  2.2117e-01,  1.6204e-01,\n",
      "          2.0105e-01, -1.1207e-01,  1.6353e-01,  1.3641e-01,  1.6757e-01,\n",
      "         -8.8023e-02,  2.9368e-02, -1.9104e-01, -1.9749e-01, -1.8857e-01],\n",
      "        [-1.5216e-01, -1.5307e-03, -1.5968e-01,  5.0356e-02,  1.5744e-01,\n",
      "          2.0323e-01,  1.1609e-01,  8.6791e-02,  2.0253e-01,  2.1945e-01,\n",
      "          1.4161e-02, -9.1111e-02, -2.0500e-01,  8.0000e-02,  2.1358e-01,\n",
      "         -8.0506e-02, -9.0242e-03, -8.1445e-02,  2.0794e-01, -1.8621e-01],\n",
      "        [ 1.7149e-01,  2.0438e-01,  1.8016e-01,  2.0965e-01,  2.0228e-01,\n",
      "         -1.9486e-01,  4.8329e-02,  1.3921e-01,  1.8999e-01,  1.9005e-01,\n",
      "         -1.8788e-01,  8.4610e-02,  2.1002e-03,  9.3509e-02,  1.9775e-01,\n",
      "         -1.8429e-01, -1.5046e-01, -1.8005e-01,  1.1753e-01, -1.0389e-01],\n",
      "        [ 1.5643e-01, -1.5049e-01,  4.4297e-02, -3.4296e-02,  1.1689e-01,\n",
      "          5.4660e-02, -1.9789e-01,  1.4460e-01,  1.6873e-02,  8.1738e-02,\n",
      "          1.3468e-01,  8.0508e-02,  2.2016e-01,  6.2871e-02,  2.1258e-01,\n",
      "         -7.4019e-02,  2.1699e-01, -1.5314e-02, -1.9891e-01, -5.1906e-02],\n",
      "        [ 1.4174e-01,  4.6143e-02,  4.6965e-03, -2.4094e-02,  1.5024e-02,\n",
      "         -1.4263e-01, -4.9728e-03, -1.1063e-01,  1.0503e-01,  3.8092e-02,\n",
      "         -1.9144e-01, -2.2248e-01, -2.2036e-01,  9.8777e-02, -1.2351e-01,\n",
      "         -1.6973e-02,  7.0125e-02,  5.1325e-02,  3.3736e-02, -2.1638e-01],\n",
      "        [-1.4488e-01, -1.2293e-01,  1.6402e-01, -6.5230e-02,  1.2786e-01,\n",
      "         -5.6409e-02,  7.1247e-02, -1.6912e-01, -5.2821e-03,  8.3406e-02,\n",
      "         -5.0842e-02,  1.0167e-01, -1.0216e-01,  1.9887e-01,  1.6207e-02,\n",
      "          1.7313e-01,  2.1611e-01,  2.3415e-02, -1.5465e-02,  2.1712e-01],\n",
      "        [-4.1761e-02, -8.2148e-03,  4.9026e-02,  8.0564e-02, -3.6640e-02,\n",
      "         -1.2068e-01, -6.9595e-02, -2.1230e-01, -2.0920e-01, -7.9254e-02,\n",
      "         -7.8046e-02, -5.9310e-02, -2.0569e-01,  1.3368e-01, -2.0939e-01,\n",
      "          1.5483e-01,  2.1012e-01,  8.5558e-02, -1.4538e-01,  3.6569e-02],\n",
      "        [ 1.9493e-01, -9.2188e-02,  1.1859e-01, -9.6957e-02, -2.5600e-02,\n",
      "         -1.4131e-01,  8.3915e-02,  1.3787e-01, -3.8023e-02,  1.0361e-01,\n",
      "         -3.0484e-02, -1.1245e-01, -2.0793e-01,  1.8763e-01,  2.1130e-01,\n",
      "         -1.3047e-01, -9.0434e-02,  4.1936e-02, -9.8041e-02, -1.6974e-03],\n",
      "        [-2.1630e-01, -1.0416e-01,  2.1860e-01, -4.2202e-02, -2.1605e-02,\n",
      "         -7.4038e-02,  1.3856e-01,  2.1891e-01, -1.1308e-01,  1.2461e-01,\n",
      "         -1.2780e-01, -1.9026e-01,  8.7944e-03,  2.2344e-01,  1.1341e-01,\n",
      "         -4.4027e-02, -1.6939e-01, -5.4636e-02, -9.0244e-02,  2.1374e-01],\n",
      "        [-9.3143e-02, -1.9173e-01, -1.2905e-01,  2.1434e-01,  1.7117e-01,\n",
      "          1.4866e-01, -1.0445e-01, -1.5228e-01,  2.0981e-01,  1.0679e-01,\n",
      "         -1.1829e-01,  9.0043e-02,  1.4148e-02, -7.4265e-02, -1.7191e-01,\n",
      "          1.8369e-01, -1.7151e-01,  9.8109e-02, -2.1672e-01, -1.0661e-01],\n",
      "        [ 1.4985e-01, -5.6023e-02, -3.2966e-02,  6.0848e-02, -1.4519e-02,\n",
      "          1.2974e-01, -4.6633e-02,  2.0466e-01, -1.7542e-01, -1.1192e-01,\n",
      "         -1.9847e-01,  1.1180e-01,  1.6145e-01, -7.2476e-03, -1.2063e-01,\n",
      "         -9.2772e-02,  9.2573e-02, -1.1504e-01, -1.6438e-01, -1.3105e-01],\n",
      "        [ 1.4534e-01, -2.0178e-01, -3.5823e-02,  1.2355e-01, -1.8221e-01,\n",
      "         -1.7476e-01, -1.7353e-01, -1.9049e-01,  2.0155e-01,  1.1723e-01,\n",
      "          1.4045e-01, -1.5295e-01,  3.6698e-03, -7.9797e-02, -9.2832e-02,\n",
      "          2.0373e-02,  1.3436e-01,  1.5915e-01,  1.1402e-01,  7.7366e-02],\n",
      "        [-6.8361e-02, -6.8090e-02,  1.7419e-01,  1.0123e-01, -8.6479e-03,\n",
      "          1.6931e-01, -6.2536e-02, -3.9073e-02, -1.4214e-01,  1.2099e-01,\n",
      "          1.3979e-01,  2.0013e-01, -1.6421e-01,  9.1523e-02, -9.4840e-03,\n",
      "         -4.2349e-02,  9.7421e-02, -1.1042e-01, -1.2845e-01,  3.5949e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0533, -0.1248,  0.0232, -0.1659, -0.2153, -0.1164, -0.1015, -0.2130,\n",
      "         0.1843,  0.1837,  0.2221,  0.0928, -0.1126,  0.1481, -0.1774,  0.1333,\n",
      "        -0.1709, -0.1221,  0.0864,  0.1550], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1076, -0.1054,  0.1040,  0.1794,  0.0046, -0.0338, -0.1462,  0.1033,\n",
      "          0.1587,  0.0093, -0.0185, -0.1403, -0.0956, -0.1600, -0.0677,  0.2136,\n",
      "          0.0164,  0.0604,  0.0651, -0.0271],\n",
      "        [ 0.1940,  0.0312,  0.0964, -0.0945, -0.1689, -0.2174, -0.1341, -0.0848,\n",
      "         -0.1386, -0.2200, -0.1089,  0.1790, -0.0995,  0.0885,  0.1256, -0.1154,\n",
      "          0.0353,  0.1968, -0.1123, -0.1773],\n",
      "        [-0.1461,  0.1381, -0.2166, -0.2027, -0.1032, -0.0732, -0.0281,  0.1378,\n",
      "          0.0025, -0.2110, -0.0735,  0.0337, -0.0502, -0.0048, -0.0819, -0.1216,\n",
      "         -0.1421,  0.0868, -0.0109,  0.1847],\n",
      "        [-0.1799,  0.2229, -0.0880, -0.2151,  0.0980,  0.0297,  0.0211,  0.0902,\n",
      "         -0.0881,  0.0991,  0.1015, -0.1438,  0.2219, -0.0255, -0.2085, -0.1138,\n",
      "         -0.1070,  0.1138, -0.0372,  0.0058],\n",
      "        [-0.1828, -0.2128,  0.1088, -0.0659, -0.0519, -0.1961,  0.1228,  0.2076,\n",
      "         -0.0736,  0.1231, -0.0988, -0.1627,  0.1651, -0.2226, -0.0549,  0.2091,\n",
      "         -0.0458, -0.1038, -0.0974, -0.1896],\n",
      "        [-0.1215, -0.0996, -0.0302, -0.2106,  0.0273, -0.2188, -0.0614,  0.0373,\n",
      "          0.1017,  0.1248, -0.1744, -0.0996, -0.0800, -0.1478,  0.1831,  0.1415,\n",
      "          0.1421,  0.2197, -0.1664, -0.0319],\n",
      "        [-0.1923,  0.0096,  0.1936,  0.0250,  0.0115, -0.0393, -0.1411, -0.0040,\n",
      "          0.2148, -0.1037,  0.0565, -0.1054,  0.1752, -0.1046, -0.1639,  0.0443,\n",
      "         -0.1671, -0.1938, -0.1761, -0.1421],\n",
      "        [ 0.2092, -0.0195,  0.0346, -0.0441,  0.2230, -0.0811, -0.1582,  0.0827,\n",
      "         -0.1950, -0.1705, -0.0288, -0.0619, -0.0772, -0.1071, -0.1568,  0.1775,\n",
      "         -0.1316,  0.0743,  0.1446,  0.1667],\n",
      "        [ 0.0935,  0.0237,  0.1228, -0.2083,  0.0013, -0.0412,  0.0970, -0.1755,\n",
      "         -0.1245,  0.1226, -0.0324, -0.0047, -0.0426, -0.2046,  0.0698,  0.0054,\n",
      "         -0.0379, -0.0867,  0.0681, -0.1084],\n",
      "        [ 0.0790, -0.1167,  0.0395, -0.1940, -0.1513,  0.1704, -0.0108, -0.1819,\n",
      "         -0.0447,  0.1662,  0.1729, -0.1509,  0.1123,  0.0430, -0.1171, -0.0828,\n",
      "         -0.0018, -0.2236,  0.0957, -0.1169],\n",
      "        [ 0.0749, -0.0063,  0.1245, -0.1086, -0.0191,  0.0988,  0.1525,  0.1763,\n",
      "          0.0976,  0.1806, -0.0797, -0.2129, -0.0537, -0.1599, -0.2235,  0.0699,\n",
      "          0.2224, -0.0500, -0.1767, -0.0679],\n",
      "        [ 0.0280,  0.2012,  0.0441, -0.1566,  0.1067,  0.1166, -0.1564,  0.1817,\n",
      "          0.1404,  0.1796, -0.1562,  0.1241,  0.1033,  0.1936,  0.1760, -0.1630,\n",
      "         -0.0164,  0.1245, -0.0313, -0.2101],\n",
      "        [ 0.1220, -0.0769, -0.2168, -0.0395,  0.1704, -0.1186, -0.0316,  0.1574,\n",
      "          0.0440, -0.2033, -0.1946,  0.0097, -0.0864,  0.1887, -0.1271, -0.2011,\n",
      "          0.1491,  0.1800,  0.1971, -0.0795],\n",
      "        [-0.1256,  0.0531, -0.2057,  0.1755, -0.0740,  0.1118, -0.2040,  0.0090,\n",
      "          0.0383, -0.0894, -0.1063,  0.1628, -0.1736,  0.0236,  0.0941, -0.1737,\n",
      "          0.1075, -0.0127,  0.2144,  0.1775],\n",
      "        [ 0.0931,  0.0815, -0.1499, -0.0921,  0.0818, -0.2009, -0.1604,  0.0282,\n",
      "          0.1092, -0.2212, -0.1785, -0.2171,  0.1200,  0.2088, -0.1684, -0.1560,\n",
      "          0.1026, -0.0527, -0.1757, -0.1252],\n",
      "        [-0.1101, -0.0270, -0.0458, -0.0572,  0.0149, -0.0061, -0.0228, -0.1792,\n",
      "         -0.0521,  0.1758, -0.0270,  0.1500,  0.1882, -0.1601, -0.0181, -0.0503,\n",
      "         -0.0831, -0.0664, -0.1960, -0.0952],\n",
      "        [-0.2112,  0.1909,  0.0768, -0.1279, -0.1877, -0.1692, -0.0795,  0.0463,\n",
      "         -0.1447, -0.0843, -0.1849, -0.2174,  0.0181, -0.0704, -0.0583,  0.0783,\n",
      "         -0.1228,  0.0259, -0.1782, -0.1015],\n",
      "        [ 0.1197,  0.0595, -0.0753,  0.1636, -0.1653, -0.0954, -0.1661, -0.0386,\n",
      "          0.1158, -0.0478,  0.1736, -0.1374,  0.0723, -0.0556,  0.1527, -0.0128,\n",
      "          0.2170, -0.1583,  0.0131, -0.0479],\n",
      "        [ 0.0114, -0.2016,  0.1931, -0.0829,  0.1962, -0.1808,  0.1729, -0.1711,\n",
      "          0.1746,  0.1558,  0.0452, -0.0393,  0.1303,  0.1484,  0.0923, -0.1700,\n",
      "         -0.0373,  0.1178,  0.1695,  0.1997],\n",
      "        [ 0.1542, -0.1667,  0.1796, -0.1631,  0.1667,  0.0997,  0.0379, -0.1175,\n",
      "          0.0208,  0.0724, -0.1295,  0.1961,  0.0434,  0.1002,  0.2191,  0.2119,\n",
      "         -0.1270, -0.1333,  0.2013, -0.0880]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1829, -0.0165, -0.0992,  0.1786,  0.1747, -0.0324,  0.0572, -0.1586,\n",
      "         0.1907,  0.1815,  0.0550,  0.2071, -0.0730,  0.0493,  0.1529,  0.0610,\n",
      "        -0.0074, -0.0562,  0.0120,  0.0982], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0044, -0.0378,  0.2032,  0.0960,  0.1465, -0.2081, -0.0769,  0.1723,\n",
      "          0.0203, -0.1014, -0.0464,  0.0341,  0.0467, -0.1036, -0.0068,  0.1900,\n",
      "         -0.1586, -0.1022, -0.0319, -0.1303],\n",
      "        [-0.0865, -0.0150, -0.2139, -0.1992, -0.0456,  0.0916,  0.1118, -0.1639,\n",
      "         -0.1264,  0.0660, -0.1410,  0.1406, -0.0360,  0.1351,  0.0800, -0.0973,\n",
      "          0.1795,  0.0296,  0.1412, -0.0696],\n",
      "        [ 0.1960, -0.0747, -0.1856,  0.1658, -0.2175, -0.1405,  0.1085, -0.1898,\n",
      "         -0.1791,  0.1802, -0.0304,  0.2094, -0.2024,  0.1989, -0.0690, -0.1901,\n",
      "          0.0950,  0.1076,  0.0051,  0.0033],\n",
      "        [ 0.0703, -0.0132,  0.1489,  0.1446, -0.1183, -0.1350, -0.2204, -0.0633,\n",
      "         -0.1695, -0.1046,  0.1158,  0.0405, -0.2083, -0.0806,  0.0244,  0.0492,\n",
      "         -0.1497,  0.0815,  0.1201,  0.1571],\n",
      "        [ 0.1295,  0.2084, -0.1845, -0.0392, -0.0648,  0.1366,  0.1665,  0.0292,\n",
      "          0.0415,  0.0618, -0.1417,  0.1498,  0.1241, -0.1333, -0.0108, -0.1426,\n",
      "         -0.0312, -0.0237,  0.1306, -0.0272],\n",
      "        [-0.0700,  0.1888, -0.1840, -0.0911, -0.1881, -0.1489,  0.1764, -0.1395,\n",
      "         -0.1992,  0.1246,  0.1936, -0.1862, -0.0306,  0.0293,  0.0843,  0.1920,\n",
      "         -0.1036,  0.1878, -0.0577,  0.0536],\n",
      "        [ 0.2163, -0.1538, -0.1933, -0.1726,  0.0629,  0.0384,  0.1395, -0.1011,\n",
      "         -0.1400, -0.2000,  0.1200,  0.0802,  0.0585,  0.1838, -0.1871, -0.1454,\n",
      "         -0.1693,  0.1281,  0.1303, -0.1709],\n",
      "        [-0.1108, -0.1246, -0.1127, -0.0061,  0.0452, -0.0199, -0.0121, -0.0517,\n",
      "          0.1203,  0.2089,  0.0687, -0.2094,  0.0119,  0.0084,  0.0810,  0.1981,\n",
      "          0.0487, -0.1527,  0.1233, -0.1867],\n",
      "        [-0.0614,  0.1094, -0.0382, -0.0875,  0.0457, -0.1851,  0.0819, -0.1065,\n",
      "         -0.0108, -0.1471,  0.1533, -0.0134, -0.0737, -0.2036,  0.0210,  0.0270,\n",
      "          0.0457, -0.0599,  0.0943,  0.1278],\n",
      "        [-0.1216, -0.1729,  0.1140,  0.0979,  0.0069,  0.0185,  0.0376, -0.0005,\n",
      "          0.1945, -0.0028,  0.0568, -0.0782, -0.1248,  0.1551, -0.0386, -0.0922,\n",
      "         -0.1391, -0.1597,  0.1276, -0.1105],\n",
      "        [ 0.1749,  0.0144,  0.0609,  0.0751,  0.0098, -0.1024, -0.0342, -0.1434,\n",
      "         -0.1653,  0.1966, -0.1256, -0.1294, -0.0119, -0.0150,  0.1418,  0.1611,\n",
      "         -0.1432,  0.1119,  0.0595, -0.2216],\n",
      "        [ 0.1119, -0.0075,  0.0011, -0.0814,  0.2021, -0.0048, -0.1520,  0.0737,\n",
      "         -0.1175, -0.0320,  0.1285,  0.0204,  0.1531,  0.0487, -0.1871, -0.1477,\n",
      "         -0.1586,  0.1482,  0.0886, -0.2187],\n",
      "        [-0.0457, -0.0781, -0.0144, -0.0831,  0.1682, -0.0741, -0.2084,  0.1328,\n",
      "          0.1636, -0.2115,  0.0518,  0.2168,  0.1341, -0.0508,  0.0665,  0.0438,\n",
      "          0.1963,  0.1003, -0.1993, -0.0025],\n",
      "        [ 0.1643,  0.0152, -0.1955, -0.1019, -0.0552,  0.0799,  0.0657,  0.2206,\n",
      "         -0.0601,  0.0885,  0.0343,  0.2083, -0.1554,  0.1926,  0.1118, -0.1650,\n",
      "         -0.1577,  0.0633, -0.0716, -0.0107],\n",
      "        [-0.0533,  0.0186, -0.2006, -0.0942,  0.0620, -0.2110, -0.1639,  0.1731,\n",
      "          0.1335, -0.0980, -0.1102,  0.0199,  0.0504,  0.0926, -0.0007, -0.0297,\n",
      "          0.0518, -0.0693,  0.1351,  0.0449],\n",
      "        [-0.1654,  0.0007, -0.0565, -0.1456,  0.0358, -0.0780, -0.1107, -0.1547,\n",
      "          0.1722,  0.0494, -0.0369,  0.0763,  0.1762, -0.0630, -0.0549,  0.1265,\n",
      "         -0.1780, -0.2053,  0.0215, -0.1856],\n",
      "        [ 0.0944,  0.1576,  0.0989,  0.1732, -0.0145,  0.1501, -0.0775, -0.0659,\n",
      "          0.0345,  0.1622, -0.1982,  0.0026, -0.0509, -0.1743,  0.2114,  0.1914,\n",
      "         -0.0748,  0.0890,  0.0576,  0.1961],\n",
      "        [-0.1329, -0.1112, -0.1279, -0.0226,  0.0452,  0.1672,  0.1669, -0.0191,\n",
      "         -0.0712, -0.2154, -0.1454, -0.2079, -0.1200, -0.1048,  0.1022, -0.1816,\n",
      "          0.2038,  0.0586,  0.1859, -0.1357],\n",
      "        [ 0.1652,  0.0580,  0.1588,  0.1229,  0.1644, -0.1746,  0.0521, -0.1309,\n",
      "         -0.1667,  0.1405,  0.1319,  0.0537, -0.0173,  0.0158, -0.1492,  0.0535,\n",
      "         -0.1218, -0.0451,  0.1761, -0.0948],\n",
      "        [-0.0856,  0.1808,  0.1573,  0.0696,  0.0144,  0.2185,  0.2079, -0.1597,\n",
      "          0.1036,  0.1113,  0.1327, -0.1364, -0.1811, -0.1835,  0.1656, -0.2001,\n",
      "          0.0317, -0.0285,  0.0480,  0.1277]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1825,  0.0483, -0.0240, -0.0669, -0.0470, -0.1810,  0.0807, -0.1131,\n",
      "        -0.1536, -0.1475,  0.0537, -0.0037, -0.1765, -0.0059,  0.0137,  0.0741,\n",
      "         0.1156,  0.1705,  0.1833,  0.1267], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.3924e-01,  1.7027e-01,  1.2743e-01,  8.9008e-02, -8.3939e-02,\n",
      "         -5.1222e-02, -1.1570e-01,  5.3714e-02, -1.4137e-01,  8.6188e-02,\n",
      "          8.1986e-02, -2.0686e-01, -5.8879e-02, -1.9797e-01,  8.8109e-02,\n",
      "         -9.8167e-02, -1.0014e-01,  6.3846e-02,  3.9942e-03, -3.9367e-02],\n",
      "        [-1.0035e-01,  2.1527e-01, -1.8753e-01,  1.8019e-02,  7.1443e-02,\n",
      "         -6.4945e-02, -9.9981e-02,  1.4632e-01,  1.5371e-01, -1.4686e-01,\n",
      "          1.2396e-01, -8.0720e-02, -3.7353e-02, -7.0949e-02, -1.5269e-01,\n",
      "          1.4360e-01,  5.7670e-02, -1.2182e-01, -1.0673e-02, -8.6981e-02],\n",
      "        [ 2.1412e-01, -1.2085e-01, -1.7039e-01,  1.7578e-01,  6.8848e-03,\n",
      "         -2.6485e-02, -1.0488e-01, -8.8491e-02,  1.2238e-01, -1.2232e-01,\n",
      "         -3.0812e-02, -3.8066e-02,  2.0181e-01,  2.0734e-01, -1.3086e-01,\n",
      "         -1.6637e-01, -3.7070e-02, -1.4060e-01, -4.1974e-02, -9.1264e-02],\n",
      "        [ 2.0679e-01, -1.9702e-01,  2.1189e-01, -4.7521e-02,  1.4034e-01,\n",
      "          1.9905e-01,  7.6770e-02,  1.0799e-02, -1.7980e-01, -1.5513e-01,\n",
      "         -1.3952e-01, -6.6874e-02,  6.4385e-02,  3.4289e-02, -3.1257e-02,\n",
      "          1.4809e-02,  1.2964e-01,  1.0937e-01, -4.4176e-02, -1.0970e-01],\n",
      "        [ 1.4759e-01,  4.7123e-02, -1.6196e-01, -9.5808e-03,  1.4509e-01,\n",
      "         -5.6455e-02,  1.7772e-01,  1.8842e-01,  1.9409e-01,  9.0144e-02,\n",
      "         -5.6498e-02,  9.5580e-02,  5.2992e-02,  5.9468e-03,  8.6295e-02,\n",
      "         -1.2358e-01, -8.7799e-02, -8.6508e-02,  6.0370e-02, -2.1579e-01],\n",
      "        [ 2.1998e-01, -7.8060e-02, -1.0938e-01, -6.1176e-02,  6.7636e-02,\n",
      "          1.5390e-01,  3.4579e-02,  5.6927e-02, -8.5670e-02, -1.4282e-01,\n",
      "         -4.6985e-02, -4.6617e-02,  1.7929e-01,  1.6505e-01, -8.0011e-02,\n",
      "         -2.1343e-01,  1.1365e-01, -2.8433e-02, -1.2114e-01, -1.6626e-01],\n",
      "        [-3.7631e-02,  1.7770e-01, -2.8248e-02, -7.5458e-02, -4.8073e-02,\n",
      "         -1.5667e-01, -1.6167e-01, -1.6346e-01, -1.4315e-01,  7.4655e-02,\n",
      "         -6.9267e-02,  1.7862e-01,  1.3861e-01,  1.8262e-01,  1.8982e-01,\n",
      "          1.4500e-01,  1.4381e-01,  1.8792e-01, -1.8555e-01, -7.9024e-02],\n",
      "        [ 1.2706e-01,  1.2613e-01,  7.8830e-02,  8.3709e-02,  1.2512e-01,\n",
      "          8.1654e-02, -1.2268e-01,  2.9672e-02, -2.1008e-01,  3.6509e-02,\n",
      "          1.1448e-01, -9.7695e-02, -6.5306e-02,  2.1224e-01,  1.9191e-02,\n",
      "          1.2142e-01, -1.7258e-03,  4.0862e-02,  9.1726e-02, -2.2349e-01],\n",
      "        [ 4.0140e-02, -1.2566e-02, -2.0245e-01, -1.0741e-01, -1.7575e-01,\n",
      "          8.1207e-02, -7.0727e-02,  1.3982e-01, -1.3158e-01,  2.0300e-01,\n",
      "          1.0015e-01, -5.3451e-02,  7.7034e-02,  1.1222e-01, -2.1521e-01,\n",
      "         -1.1556e-01,  1.5070e-01,  2.2068e-01,  5.7528e-02, -1.2018e-01],\n",
      "        [ 1.5930e-01, -5.4747e-02,  2.5981e-02,  1.0650e-01,  3.3455e-02,\n",
      "         -4.9420e-02, -1.9398e-01, -1.3033e-01, -1.3202e-01, -2.9842e-02,\n",
      "          5.1437e-02, -7.9450e-03,  1.6194e-01,  3.0182e-02,  1.1244e-01,\n",
      "         -1.5427e-01,  1.1417e-01, -1.1041e-01, -5.0671e-02, -1.5611e-01],\n",
      "        [ 1.4454e-01, -1.5031e-01,  1.9176e-01, -2.0997e-01, -9.2464e-02,\n",
      "         -2.0896e-01,  1.2941e-01,  4.0380e-02,  3.7698e-02,  9.2616e-02,\n",
      "          6.3297e-02,  1.7151e-01,  2.0949e-01,  1.0627e-01, -1.7658e-01,\n",
      "          1.9975e-01, -8.0948e-03,  6.3332e-02,  4.6247e-02, -3.9933e-02],\n",
      "        [-9.6868e-02,  5.8091e-02, -8.7062e-02, -1.5170e-01,  1.6489e-01,\n",
      "         -1.7214e-01, -6.6134e-02, -8.3538e-02, -1.8154e-01, -1.2487e-01,\n",
      "         -1.2977e-01,  8.0493e-02,  8.8028e-03, -1.2437e-01,  8.4242e-02,\n",
      "         -7.6662e-02, -9.1785e-02, -2.4482e-02, -1.9671e-01,  9.6463e-02],\n",
      "        [ 1.2002e-01, -2.0765e-01, -2.0160e-01, -9.0565e-02,  1.9983e-01,\n",
      "          1.8943e-01, -2.1117e-01,  2.6064e-02, -2.8890e-02, -8.4187e-02,\n",
      "         -1.9722e-01, -3.8873e-02, -2.0130e-01,  1.8260e-01, -4.1119e-02,\n",
      "         -1.7865e-01,  2.0853e-01,  1.7323e-02, -8.0864e-04, -1.6965e-01],\n",
      "        [ 8.8659e-02,  1.5642e-01,  9.4396e-02,  1.7176e-01, -2.1329e-01,\n",
      "         -3.8980e-02,  2.0772e-01,  2.5784e-02,  1.5953e-01,  1.2558e-01,\n",
      "         -1.3049e-02, -5.7516e-03, -1.4636e-01,  1.1658e-01, -1.2200e-01,\n",
      "         -1.6812e-02,  2.1199e-01, -1.3128e-01, -5.5046e-02,  3.0115e-02],\n",
      "        [ 1.5318e-01, -2.0914e-01, -3.7227e-02, -1.5656e-01,  1.4198e-01,\n",
      "          8.7672e-02, -2.0679e-01,  1.1893e-01,  9.8411e-02,  1.6879e-01,\n",
      "         -5.6524e-02,  1.5792e-02, -1.6172e-01,  1.0298e-01, -2.1916e-01,\n",
      "         -2.4831e-02, -5.1426e-03,  2.4332e-02, -1.8953e-01, -1.9775e-02],\n",
      "        [ 9.9349e-02,  4.5625e-02,  1.2922e-01, -3.7820e-02,  1.1384e-01,\n",
      "          4.0294e-03, -2.5636e-02,  8.2304e-02, -1.8251e-04, -1.6510e-02,\n",
      "          9.4899e-02, -2.1051e-01, -9.3597e-02,  7.3188e-02,  6.4021e-02,\n",
      "         -1.5277e-01,  1.5101e-01, -4.5526e-02,  1.8789e-01, -1.4909e-01],\n",
      "        [ 1.8927e-01, -1.6264e-01, -1.0719e-01, -1.2639e-01, -3.5937e-02,\n",
      "          1.1208e-01,  1.6358e-02, -4.4535e-03,  1.6520e-01,  7.4438e-02,\n",
      "         -5.9844e-02, -1.5380e-01, -9.6847e-02, -1.1864e-01, -1.7287e-01,\n",
      "          1.7875e-01, -1.3651e-01, -2.2321e-01,  1.2507e-02, -1.3869e-01],\n",
      "        [ 4.9647e-02,  1.1783e-01,  5.0120e-02,  1.0910e-02, -1.3895e-01,\n",
      "         -8.7736e-02,  1.0818e-01,  1.0387e-02,  1.4178e-02,  1.8576e-01,\n",
      "         -1.3210e-01,  2.1447e-01,  2.2560e-02,  5.8044e-03, -7.6364e-02,\n",
      "         -1.8419e-01,  1.5984e-01, -1.4901e-01, -5.7701e-02,  9.5899e-02],\n",
      "        [ 1.1902e-01, -1.8406e-01, -5.5437e-02, -1.5246e-01, -2.8509e-02,\n",
      "         -4.1713e-02, -1.1402e-01, -8.1569e-02, -3.1926e-02,  1.8529e-01,\n",
      "          8.4932e-02, -9.2776e-02,  3.2359e-02,  7.7992e-02, -2.2231e-01,\n",
      "          1.7215e-01, -9.0471e-02,  1.7698e-01,  4.0556e-03, -1.3738e-01],\n",
      "        [ 1.5203e-01, -2.0953e-01,  1.9575e-01,  6.0292e-02, -1.7065e-01,\n",
      "         -7.8145e-02,  1.7146e-01, -1.6213e-01, -7.2796e-02, -8.5961e-02,\n",
      "         -1.2884e-01, -1.5643e-01,  1.0563e-01, -1.6298e-01, -1.1146e-01,\n",
      "          6.3958e-02, -1.5909e-01,  2.0511e-01,  1.8239e-01,  4.5849e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0750, -0.0813,  0.0354, -0.0026, -0.0718, -0.1910, -0.0058,  0.1210,\n",
      "         0.2174,  0.1881, -0.0729,  0.1927, -0.1287, -0.1996,  0.1707,  0.0205,\n",
      "         0.1856, -0.0393,  0.0854,  0.0430], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.8400e-01, -1.9383e-01, -1.6397e-01,  1.9152e-01, -8.0622e-02,\n",
      "          3.3224e-02, -4.2830e-02, -1.9424e-01,  2.1272e-02, -6.3012e-02,\n",
      "         -8.7724e-03, -1.4670e-01, -1.0260e-01, -1.6851e-01,  3.0072e-02,\n",
      "         -4.0633e-02,  2.0888e-01,  1.3254e-01,  1.9957e-01, -1.3440e-01],\n",
      "        [-8.1663e-02,  6.4762e-02,  2.0890e-01,  1.4447e-01, -5.4454e-02,\n",
      "          3.2423e-02, -4.8739e-02,  2.3407e-02,  4.2748e-03, -2.0815e-01,\n",
      "          5.9553e-02, -1.9660e-01,  1.4981e-02,  2.0595e-01,  1.8255e-01,\n",
      "         -2.0988e-01, -1.2358e-01, -2.6242e-02,  1.8250e-01, -1.7654e-01],\n",
      "        [-2.0558e-01, -2.0486e-01, -3.8490e-03,  9.1117e-02, -2.0887e-01,\n",
      "         -8.5241e-02,  8.0319e-03, -1.4000e-01,  3.5169e-03, -1.2314e-01,\n",
      "          1.5102e-01, -4.9987e-02, -2.0432e-01, -1.3171e-01,  1.5402e-01,\n",
      "         -1.2476e-01,  1.7373e-01, -1.0791e-01,  1.4437e-01,  9.2174e-02],\n",
      "        [ 1.7875e-01,  6.0987e-03, -2.0310e-01, -1.6607e-02, -9.0045e-03,\n",
      "         -7.7709e-02, -1.3012e-01,  1.9347e-01,  7.9247e-02,  2.1024e-01,\n",
      "          6.2804e-02, -3.5796e-02, -1.3304e-01,  1.6332e-01, -1.4710e-01,\n",
      "         -1.2693e-01, -1.9717e-01, -1.1521e-01, -1.8198e-02,  5.1544e-02],\n",
      "        [-1.3685e-01, -8.0127e-02, -1.3290e-01,  1.4278e-01, -4.5971e-02,\n",
      "         -2.2091e-01,  1.3650e-01, -2.9666e-02,  8.6198e-03, -1.2089e-01,\n",
      "          1.8011e-01, -4.7487e-02,  2.3527e-02,  2.1826e-01,  6.7656e-02,\n",
      "          2.1672e-01,  2.1333e-01, -6.0475e-02,  7.5830e-03, -4.5287e-02],\n",
      "        [ 1.9358e-01,  2.2238e-01, -9.3385e-02,  2.5326e-02, -1.0521e-02,\n",
      "         -1.4746e-01, -4.7718e-02, -3.5658e-02, -1.1159e-01,  1.5879e-01,\n",
      "         -2.1762e-01, -3.2150e-02, -1.4690e-01,  2.0577e-01, -1.9511e-01,\n",
      "          8.2086e-02, -1.2155e-01,  1.7136e-01,  4.1119e-02, -1.5126e-01],\n",
      "        [ 1.0521e-01,  2.1175e-01, -1.2983e-01, -7.8898e-02, -1.6476e-01,\n",
      "          1.4602e-01,  9.7517e-02,  1.4575e-02,  9.1247e-02, -2.0815e-01,\n",
      "         -9.9199e-02, -1.1005e-02, -1.1367e-04, -1.1247e-01, -3.6185e-03,\n",
      "         -2.1444e-01,  8.1856e-03, -1.4008e-02, -2.0938e-01,  1.0773e-01],\n",
      "        [ 5.3710e-02, -9.1001e-02, -5.4495e-02, -4.3681e-02, -5.9024e-02,\n",
      "          1.0487e-01, -1.3758e-01,  1.1465e-01,  1.0352e-01, -6.9628e-02,\n",
      "          1.3055e-01, -1.6228e-03, -1.8682e-01, -8.4961e-02,  6.2936e-02,\n",
      "          1.4046e-01,  1.6387e-01, -7.3736e-02,  1.6444e-01,  8.2283e-02],\n",
      "        [-1.2973e-01, -1.7959e-01,  1.4156e-01, -1.5254e-01,  6.0776e-02,\n",
      "          1.5937e-01, -2.0793e-01,  1.6588e-02,  1.0224e-01,  7.4281e-02,\n",
      "         -2.0382e-01,  8.0070e-02, -1.9887e-01,  3.8680e-02,  1.9517e-01,\n",
      "         -2.6273e-02,  8.2903e-02, -1.8679e-01, -1.2797e-01, -9.9102e-02],\n",
      "        [-5.8451e-02, -1.3225e-01, -8.8313e-02,  2.1234e-01, -1.8665e-01,\n",
      "         -1.2458e-01, -8.5414e-02, -1.2245e-01, -1.1079e-01,  1.2268e-01,\n",
      "          1.5472e-01,  1.1573e-01, -1.6140e-01, -9.0014e-02,  2.0577e-01,\n",
      "         -1.7653e-01,  1.2433e-01,  2.0949e-01,  9.6047e-02, -5.7431e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2222,  0.1003, -0.0514,  0.0867,  0.1529,  0.1868, -0.1790, -0.1299,\n",
      "         0.0224,  0.0130], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0845,  0.0610,  0.2606, -0.2464,  0.1393,  0.3116, -0.3125, -0.0217,\n",
      "          0.1963, -0.0502],\n",
      "        [ 0.0694, -0.3136, -0.0761,  0.2178,  0.1995, -0.0441, -0.0894, -0.0144,\n",
      "          0.2956, -0.0483],\n",
      "        [-0.2955,  0.1431,  0.0354, -0.2833, -0.0445,  0.0197,  0.2707,  0.2381,\n",
      "          0.0374,  0.1524],\n",
      "        [-0.2400,  0.2082,  0.0521, -0.0864, -0.2271, -0.1002,  0.1100, -0.3121,\n",
      "          0.1553,  0.0305]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0096,  0.0811, -0.1726, -0.2281], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3561, -0.2795],\n",
      "        [-0.3798, -0.3373],\n",
      "        [ 0.4313, -0.0330],\n",
      "        [-0.1225, -0.5462],\n",
      "        [-0.1074, -0.5682],\n",
      "        [ 0.5460,  0.6218],\n",
      "        [ 0.3868, -0.5042],\n",
      "        [-0.3717,  0.3662],\n",
      "        [ 0.1505,  0.1306],\n",
      "        [-0.3085,  0.0205]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4328,  0.1223,  0.4497, -0.5204,  0.1456,  0.0743,  0.4771,  0.0875,\n",
      "         0.3406,  0.2764], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2881, -0.2387,  0.2534, -0.2700, -0.0725, -0.1281,  0.2605, -0.2023,\n",
      "         -0.2481, -0.0674],\n",
      "        [ 0.1851, -0.0558, -0.1614, -0.2329, -0.2196,  0.1950, -0.0015,  0.0432,\n",
      "         -0.1310, -0.0184],\n",
      "        [ 0.0790, -0.1646,  0.0828,  0.0817, -0.0286, -0.2219,  0.1494,  0.0321,\n",
      "         -0.2189, -0.0539],\n",
      "        [ 0.0823,  0.2469,  0.0668,  0.1941, -0.2774, -0.1028, -0.2590, -0.1304,\n",
      "          0.1251,  0.1164],\n",
      "        [-0.2658,  0.0621, -0.1008,  0.2333,  0.2389,  0.2932,  0.2484,  0.0895,\n",
      "          0.0837, -0.2558],\n",
      "        [-0.3144,  0.2979, -0.1342,  0.1055, -0.0825,  0.2219, -0.0578,  0.0771,\n",
      "          0.2349, -0.2375],\n",
      "        [-0.0725, -0.0692, -0.2339,  0.2505,  0.2341, -0.1262, -0.0342, -0.0292,\n",
      "          0.2713,  0.2751],\n",
      "        [ 0.1038,  0.2812,  0.2279, -0.2490,  0.1518,  0.1893, -0.1211, -0.0620,\n",
      "         -0.1030,  0.1479],\n",
      "        [-0.1702, -0.1247,  0.1172, -0.2229,  0.2261,  0.1175,  0.2754,  0.0671,\n",
      "          0.0415, -0.0840],\n",
      "        [-0.2879,  0.2061, -0.0157,  0.0333,  0.2347, -0.2612,  0.1570,  0.2786,\n",
      "          0.2717,  0.2763],\n",
      "        [-0.1892, -0.2588,  0.0711, -0.0745,  0.1691,  0.0577, -0.0766, -0.2022,\n",
      "          0.2887, -0.2712],\n",
      "        [ 0.1498, -0.2350, -0.1411,  0.1827, -0.2693,  0.1240, -0.2721, -0.1533,\n",
      "          0.0188,  0.0651],\n",
      "        [ 0.3018, -0.0374,  0.0899,  0.0844,  0.0826,  0.3067, -0.0046, -0.1084,\n",
      "          0.0865,  0.1252],\n",
      "        [ 0.2766,  0.0447, -0.0048, -0.3086, -0.2050, -0.2654, -0.3025,  0.2901,\n",
      "         -0.2586,  0.0265],\n",
      "        [ 0.1242,  0.1518,  0.2823,  0.1185,  0.1749,  0.2832,  0.1104, -0.0574,\n",
      "          0.1514,  0.2854],\n",
      "        [ 0.1592,  0.2652,  0.2729, -0.2733,  0.0921,  0.0697, -0.2472, -0.1663,\n",
      "          0.0251, -0.2001],\n",
      "        [-0.0330, -0.2510, -0.0828, -0.2615,  0.0855, -0.0720, -0.1691,  0.0254,\n",
      "         -0.2256,  0.0214],\n",
      "        [ 0.1189, -0.3020, -0.1838,  0.0174, -0.1879, -0.0179, -0.2730,  0.0019,\n",
      "         -0.0830,  0.2775],\n",
      "        [ 0.1056,  0.2342, -0.3017, -0.3132, -0.0519, -0.2735, -0.1535,  0.2961,\n",
      "         -0.0407,  0.2832],\n",
      "        [-0.2548,  0.2045, -0.1769, -0.2239,  0.1395,  0.1753, -0.1444,  0.0282,\n",
      "          0.0598,  0.0790]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1720, -0.0856,  0.0928, -0.1129, -0.1219,  0.2890,  0.2870, -0.1960,\n",
      "        -0.0828, -0.0521,  0.1887,  0.0595, -0.1089, -0.1173,  0.3092, -0.1490,\n",
      "         0.1372, -0.0669, -0.1682,  0.0174], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1090, -0.0115,  0.1783,  0.0308,  0.0371,  0.0003, -0.2043,  0.1049,\n",
      "          0.1928, -0.2181,  0.1592, -0.1202, -0.0233,  0.1765, -0.0369, -0.0375,\n",
      "          0.0620, -0.1031,  0.0956, -0.1772],\n",
      "        [-0.1992, -0.1929,  0.0398,  0.0166, -0.2235, -0.1095,  0.0780, -0.1240,\n",
      "          0.0696, -0.1145,  0.0434, -0.2025,  0.1200,  0.2030, -0.1512,  0.1912,\n",
      "          0.0714, -0.1651,  0.1777, -0.0269],\n",
      "        [ 0.0264,  0.0342,  0.2232,  0.1988,  0.2048, -0.0966, -0.0687,  0.1802,\n",
      "          0.0952, -0.0136, -0.0147, -0.0627, -0.1836, -0.2192, -0.0366, -0.1946,\n",
      "          0.1967,  0.0434,  0.2028, -0.1328],\n",
      "        [ 0.0176,  0.1085,  0.0840,  0.1976, -0.0104, -0.0783,  0.0871,  0.2182,\n",
      "          0.0008,  0.2184,  0.1320,  0.1850,  0.0735,  0.0867,  0.2093,  0.1657,\n",
      "         -0.0220,  0.1968, -0.0316, -0.0424],\n",
      "        [-0.1913, -0.1189,  0.1799, -0.1074,  0.1248, -0.0350,  0.1241,  0.2092,\n",
      "          0.0317,  0.2018, -0.1028, -0.1965,  0.1028,  0.0047, -0.1448, -0.1696,\n",
      "         -0.0233,  0.2113, -0.1157,  0.0390],\n",
      "        [-0.1911,  0.0233, -0.2126, -0.0443, -0.1930, -0.1706, -0.1901,  0.0451,\n",
      "         -0.1596,  0.1787, -0.1139, -0.1257, -0.1389,  0.1495,  0.0750, -0.0803,\n",
      "          0.2214,  0.1932, -0.0376, -0.2004],\n",
      "        [ 0.1452,  0.1370, -0.0658,  0.1203,  0.0310,  0.0981, -0.0255, -0.1938,\n",
      "         -0.2088, -0.0141,  0.0640,  0.0818,  0.2072,  0.1559,  0.0827, -0.0816,\n",
      "          0.1664,  0.0502,  0.0409,  0.0029],\n",
      "        [ 0.1867,  0.0162,  0.0125,  0.2105, -0.2084, -0.0892, -0.1808,  0.1829,\n",
      "         -0.0368, -0.0459, -0.0411,  0.0961,  0.2107,  0.2203,  0.1766,  0.1031,\n",
      "         -0.2030, -0.1077,  0.1351, -0.1915],\n",
      "        [ 0.0556,  0.1472, -0.1553, -0.2021,  0.0280, -0.1158,  0.0709, -0.1739,\n",
      "          0.1867,  0.2102,  0.1390,  0.1976, -0.0733, -0.1267,  0.1321, -0.1931,\n",
      "         -0.1461, -0.1171,  0.1983, -0.1178],\n",
      "        [-0.1564,  0.2157, -0.0017, -0.0817,  0.0553,  0.0999,  0.1962, -0.1200,\n",
      "          0.1749,  0.0875, -0.1729, -0.0165,  0.0760, -0.0714, -0.0073,  0.1104,\n",
      "         -0.0430,  0.1888, -0.1953,  0.0198],\n",
      "        [ 0.0735,  0.1307,  0.0631, -0.0450, -0.0493, -0.1568,  0.0266, -0.0322,\n",
      "         -0.0680, -0.0142,  0.2110,  0.1145, -0.1297,  0.1758,  0.1382,  0.0967,\n",
      "         -0.2047,  0.1819,  0.0477,  0.0401],\n",
      "        [ 0.1562,  0.1102, -0.2095,  0.0598, -0.1196, -0.2047,  0.1727,  0.0233,\n",
      "         -0.0836, -0.2127, -0.0441, -0.0616,  0.1204, -0.0843,  0.0022,  0.2115,\n",
      "         -0.1863,  0.1510,  0.1472, -0.1213],\n",
      "        [-0.0025,  0.1614,  0.0133,  0.1520,  0.0642, -0.0185,  0.1195,  0.0826,\n",
      "         -0.1323,  0.1836, -0.1237, -0.0724,  0.1610, -0.2117, -0.0063,  0.0984,\n",
      "          0.1405, -0.2158,  0.2009, -0.0884],\n",
      "        [ 0.0892,  0.1382, -0.1154,  0.0671, -0.1152,  0.0127,  0.1213,  0.1481,\n",
      "         -0.0331,  0.1883, -0.0574,  0.0554, -0.1250,  0.1663, -0.0542,  0.1598,\n",
      "          0.0829,  0.1708, -0.1826,  0.0385],\n",
      "        [-0.1861, -0.1548, -0.1550,  0.1620, -0.2002,  0.0235, -0.1842,  0.0650,\n",
      "         -0.0381, -0.1086, -0.1746, -0.1527,  0.2028, -0.0249, -0.2193,  0.1723,\n",
      "         -0.1647,  0.0751,  0.1220,  0.0422],\n",
      "        [-0.2037,  0.1415, -0.1448,  0.1062, -0.0968,  0.0213,  0.0482,  0.1543,\n",
      "         -0.0568,  0.1806,  0.0260,  0.0490,  0.1776,  0.1237,  0.0449,  0.0172,\n",
      "          0.1752,  0.1356,  0.0417,  0.1741],\n",
      "        [-0.0605,  0.1638,  0.0107, -0.0349,  0.1559,  0.1130, -0.0995, -0.1502,\n",
      "          0.1268,  0.0739,  0.2057, -0.0736, -0.0730,  0.1750, -0.0624, -0.1286,\n",
      "          0.0398, -0.1624, -0.2007,  0.1219],\n",
      "        [-0.1696,  0.1337,  0.0397, -0.1780,  0.0909,  0.1904, -0.0375, -0.1149,\n",
      "         -0.0855, -0.0228,  0.2053, -0.1341,  0.1595, -0.2076,  0.0744, -0.1434,\n",
      "         -0.2186,  0.1366,  0.1269,  0.1230],\n",
      "        [-0.0728, -0.0626, -0.0196, -0.1286,  0.0337, -0.0645, -0.1381,  0.0551,\n",
      "          0.0045,  0.2066, -0.1539,  0.0765, -0.0853, -0.1684,  0.0512, -0.0883,\n",
      "         -0.1278,  0.0593, -0.1128, -0.1628],\n",
      "        [ 0.0164,  0.0348, -0.0312, -0.1911, -0.0498, -0.0496, -0.1290,  0.1686,\n",
      "          0.1128, -0.1128, -0.1958, -0.0795,  0.1341,  0.1613,  0.2064,  0.0219,\n",
      "         -0.0784, -0.1167, -0.1540,  0.0441]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1859,  0.1775, -0.0061,  0.1274,  0.1800, -0.0942,  0.0052, -0.0204,\n",
      "         0.1261, -0.1114,  0.1790,  0.1080, -0.1920, -0.1661,  0.0271, -0.0418,\n",
      "         0.0837,  0.0936, -0.1156, -0.2076], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1707,  0.2218, -0.2001,  0.0585,  0.0142, -0.0160,  0.2008, -0.1187,\n",
      "          0.0762, -0.0311, -0.1453, -0.1493, -0.1195,  0.0446, -0.0789, -0.0659,\n",
      "          0.1898,  0.1819,  0.0023,  0.0119],\n",
      "        [ 0.1383, -0.0728,  0.1581,  0.1435, -0.1500, -0.0161, -0.1774, -0.1578,\n",
      "         -0.1987,  0.1788,  0.0369,  0.1497,  0.1285,  0.0647, -0.1045,  0.1131,\n",
      "         -0.1962,  0.1639,  0.0821, -0.1767],\n",
      "        [ 0.1701, -0.0228, -0.0427, -0.1213, -0.0038, -0.1715,  0.1011,  0.0393,\n",
      "          0.1552, -0.1830, -0.1521,  0.1998,  0.0338,  0.1073, -0.0677, -0.0811,\n",
      "          0.1423,  0.1340,  0.0488,  0.0700],\n",
      "        [ 0.2219,  0.1572, -0.2210, -0.0163, -0.1880,  0.0301, -0.0639,  0.1796,\n",
      "         -0.2000, -0.1937, -0.1761,  0.1833,  0.1779,  0.0655,  0.0587,  0.0959,\n",
      "         -0.0517, -0.0211,  0.2158, -0.1948],\n",
      "        [ 0.0800, -0.0805, -0.1282,  0.0528, -0.2022, -0.0559, -0.1465, -0.1289,\n",
      "         -0.0219, -0.0900,  0.1481, -0.0643, -0.0846,  0.1045, -0.0014, -0.1785,\n",
      "          0.0531, -0.0785, -0.0329,  0.0496],\n",
      "        [-0.1261,  0.0646,  0.0300, -0.1641,  0.0247, -0.0251,  0.0389,  0.0697,\n",
      "          0.0636, -0.2083, -0.0986,  0.1780, -0.2222,  0.1184, -0.1137, -0.0245,\n",
      "          0.1163, -0.1880,  0.0700, -0.0363],\n",
      "        [-0.0291, -0.2140,  0.0571,  0.0606, -0.2133,  0.1578, -0.0752,  0.1793,\n",
      "          0.0771, -0.0680, -0.0699, -0.1012,  0.1079,  0.0104,  0.1294,  0.1711,\n",
      "         -0.0289, -0.0586, -0.0919,  0.1650],\n",
      "        [-0.2097, -0.2206, -0.0390,  0.0171, -0.1730, -0.1799, -0.1942, -0.1248,\n",
      "         -0.1586, -0.0793,  0.1100, -0.1214, -0.0300,  0.0156,  0.0497,  0.2057,\n",
      "         -0.0312, -0.1078, -0.1290,  0.0807],\n",
      "        [ 0.1877,  0.0727, -0.0098, -0.0334,  0.1837,  0.0706,  0.0008, -0.1931,\n",
      "         -0.2135,  0.1783,  0.0903,  0.1280,  0.0721,  0.1355, -0.0233, -0.2079,\n",
      "          0.1035,  0.1677,  0.2231,  0.2010],\n",
      "        [-0.0794, -0.2053,  0.1780,  0.1192,  0.0927, -0.1426,  0.0564,  0.0382,\n",
      "          0.1342, -0.2168,  0.1458,  0.1837, -0.0524,  0.2004, -0.1592, -0.0660,\n",
      "         -0.1766,  0.0234,  0.2114, -0.0955],\n",
      "        [-0.0743,  0.1383, -0.0653, -0.1519, -0.1496, -0.1033, -0.1317, -0.1898,\n",
      "         -0.1527, -0.1721,  0.2120,  0.1488,  0.1547, -0.0712, -0.0461, -0.0916,\n",
      "         -0.1028, -0.2078,  0.0579, -0.0242],\n",
      "        [-0.2231,  0.1108,  0.1180, -0.0390,  0.1052,  0.1738,  0.1642,  0.1347,\n",
      "         -0.1151,  0.0137, -0.2089,  0.1036, -0.1400,  0.0628,  0.0995,  0.1497,\n",
      "          0.1892,  0.2083,  0.1854,  0.0489],\n",
      "        [ 0.1176,  0.1926,  0.0836,  0.2169,  0.0978,  0.2026, -0.1833, -0.0321,\n",
      "          0.1856,  0.0436, -0.1960,  0.1966, -0.1106, -0.1493, -0.0763, -0.0703,\n",
      "         -0.1087, -0.0812,  0.0499,  0.1358],\n",
      "        [-0.0293, -0.1885,  0.1550, -0.1654, -0.0536,  0.1646, -0.2174,  0.1240,\n",
      "          0.1003, -0.0562, -0.1551,  0.1980,  0.0936,  0.0704, -0.0170, -0.0256,\n",
      "          0.2220, -0.0419, -0.0069,  0.2223],\n",
      "        [-0.0295, -0.2090, -0.0914, -0.2028,  0.0067,  0.0595,  0.1000, -0.0507,\n",
      "         -0.1832,  0.1854,  0.0679, -0.1427, -0.1093, -0.0044,  0.1521,  0.1140,\n",
      "         -0.1679,  0.2074, -0.1081, -0.1728],\n",
      "        [ 0.0758, -0.1436, -0.0899,  0.2089, -0.0663, -0.0544,  0.1538,  0.0158,\n",
      "          0.0430, -0.1367, -0.2173, -0.0467, -0.1939,  0.0602,  0.0739,  0.0034,\n",
      "          0.1849,  0.1525,  0.1132, -0.0651],\n",
      "        [-0.1704, -0.0527,  0.0980, -0.1055,  0.2002, -0.1559,  0.1109, -0.1410,\n",
      "          0.0942, -0.0793, -0.1830,  0.0172, -0.0276,  0.1306, -0.0461,  0.0400,\n",
      "          0.2216,  0.0837,  0.0387, -0.1880],\n",
      "        [ 0.0348, -0.1927, -0.0435, -0.0854, -0.0818, -0.1900,  0.1229, -0.0516,\n",
      "         -0.1782, -0.0420, -0.0539,  0.0603, -0.1915,  0.1909, -0.0160,  0.1085,\n",
      "         -0.0538, -0.2198,  0.0017,  0.0408],\n",
      "        [ 0.0823,  0.1585,  0.1259, -0.0640, -0.0455,  0.1987,  0.1411, -0.0575,\n",
      "         -0.2024, -0.0173, -0.1881, -0.0499, -0.1200,  0.0829, -0.1450,  0.0049,\n",
      "          0.1312,  0.0437,  0.0100,  0.0493],\n",
      "        [-0.2236,  0.1323, -0.0170, -0.0738, -0.0044,  0.2125, -0.0074,  0.1720,\n",
      "         -0.0634, -0.0364, -0.1246,  0.0372, -0.2056,  0.1661, -0.0469,  0.1191,\n",
      "          0.0402, -0.0606, -0.1525,  0.1801]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1484,  0.1285,  0.1266,  0.0245,  0.1884, -0.1183,  0.0952, -0.0630,\n",
      "         0.0811, -0.1820, -0.0484,  0.1929,  0.1456,  0.0288,  0.1233,  0.0127,\n",
      "         0.2050, -0.0148, -0.2141, -0.2140], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1023, -0.0940, -0.1982,  0.1497, -0.0094,  0.0652,  0.0222,  0.0916,\n",
      "          0.0092, -0.1583,  0.1779,  0.0005,  0.1800,  0.1316,  0.0830, -0.0416,\n",
      "         -0.1698,  0.2142, -0.0661, -0.2179],\n",
      "        [-0.0832, -0.1628, -0.0295, -0.0968, -0.1118,  0.1576, -0.2067,  0.1814,\n",
      "          0.1630,  0.2213,  0.0264, -0.1405, -0.1277, -0.1588,  0.1554, -0.1046,\n",
      "         -0.1229,  0.0819, -0.2145, -0.1653],\n",
      "        [ 0.1908, -0.0405, -0.1756,  0.0564,  0.1845, -0.1079, -0.1383, -0.0967,\n",
      "          0.1303,  0.1636, -0.0757, -0.0064,  0.1628, -0.1700, -0.0065, -0.2107,\n",
      "          0.0660,  0.0195, -0.2140,  0.0555],\n",
      "        [ 0.0065, -0.0971,  0.2132, -0.1616, -0.0154, -0.0549,  0.0507,  0.1740,\n",
      "         -0.2120,  0.1707, -0.2036, -0.1051,  0.2163,  0.2047, -0.0311,  0.0519,\n",
      "         -0.1695,  0.0233,  0.0510, -0.0417],\n",
      "        [-0.1563,  0.0938, -0.1619,  0.0718,  0.0852, -0.0109,  0.0599,  0.1454,\n",
      "         -0.1794,  0.1386,  0.0721, -0.1971, -0.2022,  0.0617,  0.0013,  0.0578,\n",
      "         -0.0976,  0.0955, -0.1548,  0.0550],\n",
      "        [-0.2165,  0.1068,  0.1516, -0.1412,  0.1491, -0.1463, -0.2212, -0.0608,\n",
      "          0.1621, -0.1115,  0.0996, -0.0946,  0.1925,  0.0553,  0.1232, -0.0329,\n",
      "         -0.1987, -0.1634,  0.0573,  0.0191],\n",
      "        [ 0.0507,  0.2144,  0.2122,  0.0423, -0.1898, -0.0173,  0.1130,  0.0706,\n",
      "          0.1471,  0.2215,  0.0703, -0.0197, -0.1434, -0.1803, -0.0135, -0.1164,\n",
      "         -0.0238, -0.0882,  0.0374, -0.1927],\n",
      "        [ 0.1479, -0.0030, -0.1230, -0.1944, -0.1558, -0.1707,  0.0478, -0.1006,\n",
      "         -0.0779, -0.2203, -0.0498, -0.2128,  0.1335, -0.0702, -0.1858, -0.1252,\n",
      "         -0.1299,  0.2119, -0.1128, -0.0874],\n",
      "        [ 0.0082,  0.1340,  0.0069, -0.0801, -0.2175,  0.1083, -0.2092,  0.1319,\n",
      "          0.0457, -0.0426,  0.0876, -0.0863,  0.2054, -0.0548,  0.2015,  0.0107,\n",
      "          0.1293, -0.1716,  0.1883,  0.1817],\n",
      "        [-0.0374, -0.1013,  0.2000,  0.1770, -0.0852,  0.1841, -0.0138, -0.1097,\n",
      "          0.0397, -0.0005,  0.0319, -0.1363, -0.1686, -0.0852, -0.0854,  0.0558,\n",
      "          0.0808, -0.1228,  0.1470,  0.1229],\n",
      "        [-0.1525,  0.0945, -0.0580,  0.1579, -0.0849, -0.1854,  0.1667, -0.1161,\n",
      "          0.1738,  0.1249,  0.1779,  0.1552, -0.2146, -0.0533, -0.1777,  0.1285,\n",
      "          0.2081,  0.0202, -0.0567,  0.1515],\n",
      "        [ 0.1404,  0.0866,  0.1708,  0.1103,  0.0837,  0.1445, -0.0993, -0.0393,\n",
      "          0.1558, -0.1235, -0.1425,  0.1374,  0.0026, -0.1570,  0.1933, -0.0511,\n",
      "          0.1674,  0.0180, -0.0644, -0.1510],\n",
      "        [ 0.2122, -0.1874, -0.0776, -0.1968,  0.0760,  0.1408,  0.0323, -0.2141,\n",
      "          0.1315,  0.0653,  0.0735,  0.1982,  0.1986, -0.2226, -0.2220,  0.1800,\n",
      "         -0.0463,  0.1690,  0.0628,  0.1837],\n",
      "        [ 0.1395,  0.1840, -0.0879, -0.1735, -0.1823, -0.2092,  0.0736, -0.0488,\n",
      "          0.0162,  0.0169,  0.1750,  0.0724,  0.0260,  0.1046,  0.2206, -0.0273,\n",
      "          0.0022,  0.0062, -0.2049,  0.1983],\n",
      "        [ 0.1806,  0.0771, -0.1641, -0.0216,  0.0774, -0.1390, -0.1960, -0.1055,\n",
      "          0.0854,  0.1634,  0.0191,  0.1143,  0.1580, -0.1540,  0.1180, -0.0275,\n",
      "         -0.0031, -0.0878,  0.0882,  0.2032],\n",
      "        [-0.0453, -0.1351, -0.0780,  0.0302,  0.0557, -0.2102, -0.1085,  0.2165,\n",
      "         -0.1566,  0.0783,  0.0636, -0.1521,  0.0355, -0.1049, -0.0774,  0.1879,\n",
      "          0.1259,  0.1787,  0.2092,  0.0954],\n",
      "        [-0.0834, -0.0782,  0.1154, -0.0449,  0.1223,  0.0320, -0.0854,  0.1557,\n",
      "          0.0672, -0.0984,  0.0951,  0.0300, -0.0808,  0.0293, -0.0069, -0.0057,\n",
      "          0.1220, -0.1321,  0.1268, -0.2214],\n",
      "        [-0.0031, -0.0088,  0.1981, -0.0151, -0.0619, -0.0170,  0.0706,  0.1460,\n",
      "         -0.0482,  0.1498,  0.0393,  0.1947,  0.0397,  0.0120,  0.1025,  0.0243,\n",
      "          0.1910, -0.0219,  0.0347, -0.0986],\n",
      "        [ 0.1189, -0.0769, -0.2220, -0.1448,  0.0107, -0.1316,  0.0896,  0.1311,\n",
      "         -0.1074,  0.2078,  0.1619, -0.0019, -0.0522, -0.0570,  0.0185,  0.0682,\n",
      "         -0.0241, -0.2010,  0.0212, -0.1924],\n",
      "        [-0.2223,  0.0548, -0.1758, -0.1425,  0.1309, -0.2221, -0.1106, -0.0328,\n",
      "         -0.0686, -0.0427, -0.0697, -0.1093, -0.1962, -0.0837,  0.0360,  0.0924,\n",
      "         -0.0534, -0.1939, -0.0431,  0.1146]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1876, -0.0962, -0.0178,  0.1360,  0.1388,  0.0740,  0.2069,  0.1106,\n",
      "        -0.1706, -0.2233,  0.0429, -0.1371, -0.2222, -0.1374, -0.0188, -0.0086,\n",
      "         0.0971, -0.0933,  0.1454, -0.0964], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.4199e-01,  1.8983e-01,  2.1404e-01,  2.9468e-02, -2.0238e-01,\n",
      "          1.0411e-01,  8.9612e-02,  3.5735e-02, -8.1970e-02,  2.8716e-03,\n",
      "          1.1100e-01,  4.2738e-02,  6.6664e-02,  5.2564e-03, -1.8813e-01,\n",
      "         -9.4179e-02, -1.7846e-01,  7.5938e-02, -5.2611e-02,  1.8153e-01],\n",
      "        [ 5.0336e-02,  7.2161e-03, -9.9383e-02,  2.0003e-02,  4.3560e-02,\n",
      "         -2.2271e-01, -1.7750e-01, -1.6040e-01,  3.4517e-02,  3.4293e-02,\n",
      "         -1.5165e-01,  9.1769e-02, -2.1709e-01, -1.0187e-02, -1.0299e-01,\n",
      "          2.1533e-01,  2.0108e-01,  1.7492e-01,  9.1777e-02,  4.4016e-02],\n",
      "        [ 7.1059e-02,  1.4880e-01, -1.7467e-01, -5.2980e-02,  1.3083e-01,\n",
      "          7.0795e-02,  1.9516e-01,  7.1560e-02,  1.3516e-01, -7.2844e-02,\n",
      "          1.3354e-01, -1.9480e-01, -1.9146e-01, -2.1765e-01,  1.9354e-01,\n",
      "         -1.9240e-01,  1.4783e-01,  1.5279e-01,  1.9733e-02, -8.3578e-03],\n",
      "        [ 1.8646e-01,  1.6369e-01, -5.3728e-02,  2.1856e-02,  6.5025e-02,\n",
      "          7.0084e-02,  1.8877e-01,  1.2114e-01, -1.4512e-01, -2.8912e-02,\n",
      "         -6.7659e-02,  1.1048e-02, -1.6710e-01,  1.7286e-01, -2.2054e-01,\n",
      "          6.0091e-02,  2.0393e-01, -1.8127e-01,  2.0230e-01, -4.7429e-02],\n",
      "        [-1.8564e-01, -1.7921e-01,  2.2398e-02, -6.7922e-02,  1.9507e-01,\n",
      "         -2.5114e-02, -6.6366e-02, -4.8604e-02, -1.7677e-01,  2.2247e-01,\n",
      "          2.0875e-02,  2.2016e-01, -1.6295e-02,  6.7224e-02,  2.2298e-01,\n",
      "          1.1996e-01, -3.4869e-02,  1.3262e-01,  1.5958e-01, -6.6988e-02],\n",
      "        [-1.0710e-01, -9.7562e-02, -2.1856e-01, -3.2767e-02, -1.4780e-01,\n",
      "         -8.9442e-02, -2.0098e-01,  1.1971e-01,  1.3376e-02, -1.0718e-01,\n",
      "         -1.4542e-01,  1.9768e-01,  9.7019e-02, -4.1793e-02, -1.5024e-01,\n",
      "         -1.5693e-01,  2.0980e-01, -1.7086e-01,  7.5938e-02,  1.6917e-01],\n",
      "        [-9.8088e-02,  2.5965e-02, -8.5695e-02, -5.6828e-02,  1.2790e-01,\n",
      "          3.4940e-02,  3.6596e-02,  1.5209e-02, -1.7211e-01, -1.8259e-01,\n",
      "         -1.4433e-01,  4.1818e-02, -1.7919e-01, -8.0211e-02,  1.2550e-01,\n",
      "          8.7058e-02,  6.4093e-02, -5.9976e-02, -7.1961e-02,  1.4891e-01],\n",
      "        [ 1.9353e-01,  1.7981e-01, -1.9835e-01, -2.4299e-02, -1.6580e-01,\n",
      "         -1.1349e-01,  1.1786e-01,  6.8863e-02, -1.0969e-01, -1.4976e-01,\n",
      "         -7.1958e-02,  2.0219e-01,  1.0494e-01, -8.1153e-02, -8.6823e-02,\n",
      "          2.1850e-01,  5.2239e-02, -2.7456e-02, -1.4203e-01,  1.2242e-02],\n",
      "        [ 1.4593e-01,  1.5882e-01,  2.0570e-01, -3.5663e-02,  1.8404e-01,\n",
      "         -1.7695e-01, -6.4389e-02, -8.4908e-02,  5.0081e-02,  1.0242e-01,\n",
      "          3.7322e-02,  1.6557e-01, -7.0109e-02, -1.9704e-01, -7.9771e-02,\n",
      "         -7.9209e-02, -1.1967e-01,  1.4282e-01, -6.5237e-02, -6.2890e-03],\n",
      "        [ 7.4393e-02, -1.1931e-01,  1.6147e-01,  1.2286e-01,  2.0076e-01,\n",
      "         -1.5423e-01,  2.1659e-01, -1.6245e-01,  1.4906e-01, -2.0587e-01,\n",
      "         -9.7050e-02,  2.2137e-01,  2.1081e-01,  2.1048e-01,  2.3456e-02,\n",
      "          1.5844e-01, -1.9364e-01, -8.1785e-02,  1.9729e-01, -1.3041e-01],\n",
      "        [-1.5783e-01, -1.5966e-01,  1.1133e-02,  1.6076e-01, -1.0948e-01,\n",
      "         -6.8871e-02, -2.0279e-01,  1.2102e-02,  1.7184e-01,  2.7965e-02,\n",
      "         -5.9776e-02, -7.0894e-02, -2.1967e-01, -2.1136e-01,  1.0275e-01,\n",
      "          1.7724e-01, -4.5363e-02,  4.3076e-02,  2.0264e-02,  1.7129e-01],\n",
      "        [-1.6799e-01,  5.4576e-02, -2.0333e-01, -8.7636e-02,  1.1191e-01,\n",
      "         -2.0718e-01,  1.9373e-01, -1.8448e-01,  8.9323e-02, -2.2360e-01,\n",
      "          1.7739e-01, -8.4417e-02, -1.2190e-01, -5.7597e-02, -5.4135e-02,\n",
      "         -1.0241e-01, -1.2282e-01,  3.5050e-02,  1.9820e-01,  4.1701e-02],\n",
      "        [ 1.1953e-01, -1.2007e-02,  7.7553e-03,  7.8864e-02, -5.3472e-02,\n",
      "          2.5147e-02,  2.1328e-02, -1.7950e-01,  6.2462e-02,  7.7889e-02,\n",
      "         -6.1908e-02, -1.6278e-02,  7.6043e-02, -6.8706e-02, -1.0503e-01,\n",
      "          1.1655e-01,  1.6186e-01,  1.0391e-01,  3.0014e-02,  2.1202e-03],\n",
      "        [-1.0878e-01, -4.6338e-02, -7.8109e-03,  1.3665e-01, -2.1581e-01,\n",
      "         -7.8257e-02,  1.3606e-01, -7.6587e-02, -1.9076e-01, -1.4066e-01,\n",
      "          6.3065e-02, -1.9678e-01,  2.1787e-01,  2.0439e-01,  1.8886e-01,\n",
      "          2.2038e-01,  1.2517e-01, -1.7012e-01, -6.1810e-04, -1.0965e-01],\n",
      "        [ 7.1812e-02, -3.3073e-02, -8.1087e-02, -5.7151e-02,  3.9332e-02,\n",
      "          1.4580e-01,  1.1245e-01,  1.3301e-01, -1.4529e-02, -1.1741e-01,\n",
      "         -1.6110e-01,  7.7956e-03, -1.3823e-01,  1.2846e-01, -2.7040e-02,\n",
      "         -2.2083e-01,  5.6161e-02, -2.8528e-02,  1.0260e-01,  3.4356e-02],\n",
      "        [ 2.2094e-01,  1.6538e-01,  1.8237e-01,  4.8277e-02,  1.5484e-01,\n",
      "         -1.8016e-01, -1.7305e-01, -2.8616e-02, -2.0546e-01,  1.3417e-01,\n",
      "          6.0763e-02, -1.9844e-01,  2.1693e-01,  1.6637e-01, -7.0335e-02,\n",
      "          1.8944e-01, -8.2140e-02, -1.6700e-01,  7.0620e-02,  6.3812e-02],\n",
      "        [-1.5752e-01,  1.5368e-01, -1.4879e-01,  3.6108e-02,  4.0427e-02,\n",
      "          2.0561e-01, -2.0768e-01,  1.9051e-01, -1.3765e-01,  2.1202e-01,\n",
      "         -1.8584e-01,  2.0277e-01, -1.4782e-01,  1.7927e-01, -1.7316e-01,\n",
      "         -1.8524e-01, -2.9687e-02,  1.8196e-01, -2.1306e-01, -2.1634e-01],\n",
      "        [ 2.1658e-01,  1.8645e-01,  1.4543e-01, -4.5598e-02,  2.2340e-04,\n",
      "          2.0461e-01,  9.0416e-02, -7.2336e-02,  2.0852e-02,  8.7075e-02,\n",
      "          4.2787e-02, -1.5523e-01, -1.1371e-01, -1.2476e-01, -6.5730e-02,\n",
      "          1.4793e-02, -1.8010e-01,  1.8649e-01,  1.4597e-01, -1.5293e-01],\n",
      "        [ 1.6800e-01, -1.1102e-01, -6.1912e-02, -1.9463e-01, -5.6267e-03,\n",
      "         -7.7729e-02, -6.1616e-02,  1.3504e-01,  1.5685e-01,  6.8330e-02,\n",
      "         -6.4917e-02, -4.6335e-02,  1.5039e-01,  6.0527e-02,  4.8542e-02,\n",
      "          1.4082e-01,  1.5419e-02, -9.1918e-02, -1.6860e-01,  6.9562e-02],\n",
      "        [ 1.3402e-01, -6.2842e-02, -4.0215e-02,  1.5251e-01,  2.1166e-01,\n",
      "         -3.1648e-02,  1.3960e-01, -1.2453e-01, -2.9437e-02,  1.4393e-01,\n",
      "         -1.2900e-01, -6.8383e-02, -2.1437e-02, -1.8423e-01, -1.1777e-01,\n",
      "          1.8497e-01,  3.0846e-02, -2.9948e-02, -1.4640e-01,  1.0576e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1492, -0.1421,  0.0776, -0.1088, -0.2141,  0.0936, -0.1086,  0.0361,\n",
      "        -0.2032, -0.1103, -0.0102, -0.1936,  0.0507, -0.1536,  0.1785,  0.0729,\n",
      "         0.1521, -0.0563,  0.1946, -0.0998], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-8.1203e-02, -7.9069e-02,  1.8525e-01,  1.2184e-01,  1.2710e-01,\n",
      "         -3.4295e-02, -1.6142e-01,  1.8512e-01, -1.3177e-01, -1.0788e-02,\n",
      "          1.5439e-01,  1.7417e-01,  1.2587e-02,  2.0041e-01, -1.9025e-01,\n",
      "          5.9849e-02, -1.6644e-01, -8.4825e-02,  1.5760e-01,  1.9651e-01],\n",
      "        [-1.7390e-02,  3.0467e-02, -1.8024e-01,  1.7173e-01,  2.2280e-01,\n",
      "         -1.0006e-01,  1.4118e-01,  5.1301e-02, -3.1172e-02, -2.1346e-01,\n",
      "          9.9017e-02,  1.4656e-01, -9.5229e-02, -1.3288e-01, -1.3664e-01,\n",
      "          1.2035e-01,  3.6345e-02,  2.1063e-01, -1.1521e-01, -2.6620e-03],\n",
      "        [ 1.9154e-01, -1.5657e-01, -3.8162e-02,  9.5189e-02, -1.8826e-01,\n",
      "         -1.6017e-01, -2.4985e-02, -8.6458e-02,  4.4990e-02, -1.7494e-01,\n",
      "          1.4995e-02,  1.0111e-01, -2.3825e-02,  5.3890e-02, -2.0191e-01,\n",
      "         -1.8117e-02,  1.9955e-01, -1.5151e-01,  1.3328e-01, -1.7283e-01],\n",
      "        [-4.8029e-02, -1.8571e-01,  3.7111e-02, -4.7407e-02, -4.4444e-02,\n",
      "          1.8657e-01, -1.6816e-01, -1.1114e-01,  3.3613e-02,  1.1852e-01,\n",
      "         -1.3210e-01,  1.8816e-01, -1.3283e-01,  1.9535e-01,  1.9873e-01,\n",
      "         -3.1129e-05, -1.5773e-01,  1.2828e-01, -1.5446e-01,  1.2196e-01],\n",
      "        [ 4.5632e-03,  1.0124e-01,  1.1059e-01, -7.0217e-03,  2.1777e-01,\n",
      "          4.7198e-03,  1.1854e-01, -6.8029e-02,  9.6491e-02, -3.1790e-04,\n",
      "          4.5776e-02,  1.8629e-01,  1.0675e-01,  7.2697e-02,  2.0214e-02,\n",
      "          5.1393e-02, -1.5200e-01,  1.4996e-01, -1.2784e-01, -1.4664e-02],\n",
      "        [-9.1953e-02,  1.0467e-01, -8.9579e-02, -9.8857e-02,  7.3112e-03,\n",
      "         -5.3714e-02, -1.6434e-01, -5.9830e-02, -1.9260e-01,  1.4859e-02,\n",
      "         -1.6355e-01,  3.9594e-02, -1.5872e-02,  1.7296e-02, -2.0601e-01,\n",
      "          2.1924e-01,  1.5743e-01, -6.7837e-02,  1.8134e-01,  1.9264e-01],\n",
      "        [ 7.2925e-02, -1.3861e-01,  4.9806e-02, -2.5819e-02, -1.0086e-01,\n",
      "          2.0623e-01,  8.7547e-02, -1.7534e-01,  5.8995e-02, -4.1927e-02,\n",
      "          5.3069e-02,  3.3404e-02,  1.0370e-01, -5.9220e-02, -7.1428e-03,\n",
      "          5.6663e-02,  1.7147e-01,  1.7085e-01, -4.0840e-03,  1.4569e-01],\n",
      "        [-2.1589e-01,  2.3105e-02, -1.5965e-02, -1.5741e-01,  2.0984e-01,\n",
      "          1.1735e-02, -7.6758e-02, -6.6614e-02, -2.1850e-01,  8.8857e-02,\n",
      "          8.0951e-02,  1.5351e-01,  9.1364e-02, -1.0155e-01, -2.0611e-01,\n",
      "          1.6191e-01,  9.4166e-02,  8.6443e-02, -2.1876e-02, -5.6074e-02],\n",
      "        [ 2.1350e-01,  9.2709e-02,  1.5719e-01,  1.2782e-01,  8.4124e-02,\n",
      "          1.3659e-01,  1.2225e-01, -1.7606e-01, -1.2767e-01,  2.0331e-01,\n",
      "          8.3697e-02,  1.5350e-01, -1.7280e-01,  6.8973e-02,  1.7128e-01,\n",
      "          5.6152e-02, -2.0207e-01, -2.2317e-01,  4.2197e-02, -2.0045e-01],\n",
      "        [ 1.9574e-01,  1.1890e-01,  7.7239e-02,  2.1513e-01, -2.2207e-01,\n",
      "         -3.0963e-02, -8.4336e-02, -9.4184e-02, -1.4273e-01,  7.8377e-02,\n",
      "          1.0339e-01, -7.1769e-03, -5.2942e-02,  6.9922e-03, -1.1753e-01,\n",
      "          2.0371e-01,  1.6156e-01, -2.1120e-01, -2.2251e-01, -4.8622e-02],\n",
      "        [ 7.9178e-02, -4.0136e-02, -3.3277e-02, -6.1236e-02,  4.9023e-02,\n",
      "         -4.8569e-02,  2.8126e-02, -2.0765e-01,  1.7475e-01, -8.9647e-02,\n",
      "          1.0416e-01, -2.1872e-02, -1.9316e-01,  2.7276e-02, -2.1388e-02,\n",
      "          2.1570e-01, -3.0221e-02, -3.4145e-02,  1.3274e-01,  3.6615e-02],\n",
      "        [ 6.4341e-02, -6.2544e-02,  1.4309e-01, -1.2292e-01,  1.3496e-01,\n",
      "         -9.2799e-02,  1.5949e-01, -5.5541e-04,  1.8013e-01, -9.6980e-02,\n",
      "          1.1783e-01, -8.5345e-02, -4.5735e-02, -1.3826e-01, -1.5655e-01,\n",
      "          8.6238e-02, -8.0117e-02,  1.9383e-01,  1.4096e-01, -1.0510e-01],\n",
      "        [ 2.2269e-01, -1.9148e-01,  2.7396e-02,  1.3484e-01, -8.4204e-02,\n",
      "          2.1700e-01, -6.3999e-02, -6.8931e-02,  2.0093e-01, -1.1175e-01,\n",
      "         -2.2080e-01, -1.7248e-01,  8.4723e-02, -1.3469e-01,  9.1876e-02,\n",
      "          7.7404e-02,  6.8967e-02, -8.5987e-02, -1.2531e-01,  3.0681e-02],\n",
      "        [-6.8034e-02, -3.4898e-02,  8.7913e-02, -6.2013e-02,  1.4434e-01,\n",
      "         -2.1571e-01, -2.0850e-01, -2.4606e-02, -8.8985e-02, -1.9421e-01,\n",
      "          1.2504e-01, -1.2261e-01,  1.2455e-01, -8.4865e-02,  4.5679e-02,\n",
      "          6.9378e-04,  7.6344e-02,  1.3676e-01,  1.2756e-01,  1.0949e-01],\n",
      "        [ 1.4031e-01,  4.0965e-02, -2.4156e-02,  2.5999e-02, -1.7325e-01,\n",
      "         -1.9014e-01, -2.7460e-02, -1.6773e-01, -1.1435e-01, -1.2443e-01,\n",
      "          8.3167e-02,  6.5650e-02, -1.7006e-01, -1.5917e-01,  2.1580e-01,\n",
      "         -9.5096e-02, -1.2522e-01,  2.0498e-01, -2.0883e-01,  1.9304e-01],\n",
      "        [-3.6524e-02,  9.5950e-02,  4.2720e-03, -6.0215e-02, -3.6490e-02,\n",
      "         -1.5683e-01,  1.9641e-01,  2.2275e-01, -1.0871e-01, -5.1401e-02,\n",
      "          1.6985e-02, -1.0155e-02, -1.0345e-01, -1.4284e-01,  6.2972e-02,\n",
      "         -6.2914e-02, -7.2382e-02, -3.7889e-02, -1.3134e-01,  2.0254e-01],\n",
      "        [ 1.7548e-01,  1.2433e-01, -1.7740e-02, -2.0857e-01, -3.4206e-02,\n",
      "          9.4252e-02,  9.7611e-02,  2.0214e-01, -1.9348e-02, -2.1982e-01,\n",
      "         -6.3215e-02,  1.9216e-01,  1.7582e-01,  1.4901e-02,  9.8006e-02,\n",
      "         -8.0661e-02,  1.6652e-01, -1.2350e-01,  3.8724e-02,  1.6042e-01],\n",
      "        [ 5.4966e-02,  1.9068e-01, -3.4738e-02,  4.3308e-02,  3.2340e-02,\n",
      "          1.7404e-01, -1.8631e-01, -4.9473e-03, -1.3348e-01, -1.8428e-01,\n",
      "         -1.2753e-01, -2.0288e-02, -1.1595e-01,  2.7550e-03, -1.5604e-01,\n",
      "         -7.2596e-03,  1.3893e-01, -1.0360e-01, -1.7346e-01, -2.1548e-02],\n",
      "        [ 1.8596e-01, -8.6770e-02,  6.9996e-02,  1.3637e-01, -5.8152e-02,\n",
      "         -5.8229e-02,  1.9608e-02,  2.1077e-02,  2.2244e-01,  3.5185e-02,\n",
      "         -1.6467e-01, -1.4459e-01, -4.9895e-02, -1.9011e-01,  4.3836e-02,\n",
      "         -1.6271e-01,  3.0571e-02,  4.1840e-02,  1.6337e-01,  6.4941e-02],\n",
      "        [ 7.4698e-02,  1.3262e-01,  8.1349e-02,  1.1164e-01,  6.6457e-02,\n",
      "          1.6038e-01, -1.1130e-01, -1.7028e-01,  6.2806e-02, -1.6283e-01,\n",
      "          9.5039e-02,  2.0304e-01, -6.9816e-02,  1.1114e-01,  5.8524e-02,\n",
      "          6.6766e-02,  1.8647e-01, -2.1357e-01, -1.5168e-01,  8.3837e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0793,  0.0216, -0.0496, -0.1508, -0.0352,  0.1546,  0.0717,  0.1187,\n",
      "         0.1629, -0.0861, -0.2074,  0.0135,  0.2149, -0.0715,  0.1629, -0.0038,\n",
      "        -0.1208,  0.1894, -0.0374,  0.1751], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0592, -0.0388, -0.1466, -0.0987,  0.0559, -0.1220,  0.0116,  0.2117,\n",
      "         -0.0398, -0.1863,  0.2177, -0.1681, -0.0198, -0.0273,  0.0551, -0.1378,\n",
      "          0.1453,  0.1754,  0.1441, -0.2017],\n",
      "        [-0.0870,  0.1365, -0.0158, -0.0068,  0.1852, -0.1903, -0.0133, -0.1037,\n",
      "          0.0043,  0.0640, -0.0816,  0.1093, -0.1958,  0.0277, -0.0110,  0.0571,\n",
      "         -0.1321,  0.1816, -0.0488,  0.1449],\n",
      "        [ 0.1628,  0.1598, -0.0858,  0.1890, -0.1232,  0.2144,  0.2065,  0.0277,\n",
      "         -0.1217,  0.1255, -0.1719, -0.2200, -0.0188, -0.2184,  0.1402, -0.0504,\n",
      "          0.1163,  0.0457, -0.1542,  0.2044],\n",
      "        [ 0.0775,  0.1207, -0.0043, -0.1346, -0.1899, -0.0381, -0.1092, -0.0489,\n",
      "         -0.1991,  0.0611, -0.1096, -0.0306,  0.2180, -0.0728,  0.2062,  0.0521,\n",
      "          0.0654,  0.1745, -0.0187, -0.1709],\n",
      "        [ 0.0175, -0.0022, -0.0362, -0.1762, -0.1682, -0.1568,  0.1105,  0.1214,\n",
      "          0.1359,  0.1393, -0.1172,  0.2167,  0.0806, -0.1933,  0.0130, -0.0932,\n",
      "         -0.1880,  0.0683, -0.1985, -0.2179],\n",
      "        [-0.1728,  0.0053, -0.1725, -0.0943,  0.0099, -0.0934, -0.0455, -0.0995,\n",
      "         -0.1104, -0.2096,  0.1645, -0.1011, -0.0431,  0.0596, -0.1804,  0.2013,\n",
      "         -0.0311,  0.0574,  0.1149,  0.1093],\n",
      "        [-0.0190, -0.1255, -0.1625,  0.1513,  0.1855, -0.2195, -0.1001, -0.1197,\n",
      "         -0.0228,  0.1774,  0.0576,  0.1504,  0.0616,  0.0760, -0.1128, -0.1083,\n",
      "          0.0071,  0.1022, -0.1416,  0.1750],\n",
      "        [-0.1629,  0.1790, -0.1980,  0.0165, -0.0837,  0.1814,  0.0737,  0.1821,\n",
      "          0.1282, -0.0467,  0.2013,  0.1810, -0.1108,  0.0421,  0.1862,  0.0708,\n",
      "         -0.0973,  0.1615, -0.0196, -0.0697],\n",
      "        [ 0.0560,  0.1356, -0.0118, -0.1345,  0.0467, -0.0790, -0.1684,  0.0924,\n",
      "          0.2017,  0.0215, -0.0759, -0.0763,  0.1737, -0.1408,  0.0048,  0.1341,\n",
      "         -0.2049, -0.1060,  0.0815,  0.1950],\n",
      "        [-0.1424, -0.0849,  0.1579, -0.2085,  0.1964, -0.1726,  0.2016,  0.0931,\n",
      "          0.2035,  0.0736, -0.1975,  0.2132,  0.1917,  0.1755,  0.0260,  0.0317,\n",
      "         -0.0652,  0.1675, -0.1550, -0.2162],\n",
      "        [ 0.0250, -0.0979, -0.0856, -0.1159,  0.1581, -0.0827,  0.0570, -0.1335,\n",
      "          0.0262, -0.0888,  0.1474,  0.1302,  0.1851, -0.0811, -0.1747,  0.1900,\n",
      "          0.1120,  0.1625, -0.0590, -0.1421],\n",
      "        [-0.0902,  0.0506,  0.1654, -0.2098,  0.0480, -0.0475, -0.0625,  0.1937,\n",
      "         -0.1481,  0.1092,  0.1788,  0.0114, -0.1014,  0.0884,  0.0336, -0.1552,\n",
      "          0.0143, -0.1824,  0.1414, -0.0508],\n",
      "        [ 0.2103,  0.0325,  0.0244,  0.1665, -0.0306,  0.1165,  0.1375,  0.0337,\n",
      "          0.0777, -0.1613,  0.1674,  0.1541, -0.0057,  0.0324, -0.0372, -0.0301,\n",
      "         -0.1841, -0.2017,  0.2027, -0.1363],\n",
      "        [-0.1864, -0.0691,  0.0778,  0.1894,  0.0254,  0.0567, -0.2038, -0.0438,\n",
      "         -0.0789,  0.0652,  0.1569,  0.0517, -0.1653,  0.0985,  0.0180, -0.2012,\n",
      "          0.1557, -0.0412,  0.2155,  0.0381],\n",
      "        [-0.2043,  0.0835, -0.1091,  0.0922,  0.1224, -0.0597,  0.0233, -0.2058,\n",
      "         -0.2159, -0.1083,  0.1517,  0.0341,  0.1170, -0.2213, -0.1716,  0.1124,\n",
      "          0.1083,  0.1857,  0.1394, -0.1805],\n",
      "        [ 0.1503, -0.0774,  0.0463,  0.1670,  0.1135,  0.1097, -0.0466,  0.1063,\n",
      "          0.0537,  0.0620, -0.1804,  0.0243,  0.0241, -0.2020, -0.1507,  0.0357,\n",
      "          0.0093,  0.1576,  0.1576,  0.2198],\n",
      "        [-0.2204,  0.2152, -0.0440, -0.0385,  0.2058, -0.1371, -0.0263, -0.0997,\n",
      "         -0.2142,  0.0838,  0.1087, -0.1530, -0.0529, -0.1120,  0.1750,  0.0900,\n",
      "         -0.1361, -0.0182, -0.1753,  0.0060],\n",
      "        [ 0.1560,  0.1634,  0.1674, -0.0431,  0.1270, -0.1214, -0.1745, -0.1668,\n",
      "         -0.1975, -0.1154, -0.0098,  0.0855, -0.1138,  0.0569, -0.1603, -0.0265,\n",
      "         -0.1276, -0.1566,  0.1577, -0.2182],\n",
      "        [-0.1452, -0.0690,  0.1784,  0.0891, -0.0804, -0.1238,  0.0951,  0.1957,\n",
      "          0.0198, -0.0887,  0.0326, -0.0930,  0.0473, -0.2040,  0.0943,  0.0966,\n",
      "         -0.1807,  0.0365,  0.1066, -0.1016],\n",
      "        [ 0.1713,  0.1265, -0.0283, -0.1459,  0.1072,  0.1182,  0.1036,  0.1054,\n",
      "         -0.1624, -0.0178, -0.0832, -0.1801,  0.0940, -0.0621,  0.0090, -0.0233,\n",
      "          0.0465,  0.0545,  0.0921, -0.1230]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0742, -0.2022,  0.1581, -0.2149, -0.1241,  0.0502,  0.0741,  0.1002,\n",
      "         0.1258,  0.1794, -0.0098,  0.1092, -0.0525,  0.1436,  0.1600,  0.0281,\n",
      "         0.1720,  0.0936,  0.1932, -0.0491], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.1228e-01,  2.2285e-01,  2.1232e-01,  1.3574e-02, -1.4581e-01,\n",
      "         -4.2746e-02, -1.0024e-01,  5.8222e-03, -9.2772e-02,  1.7590e-01,\n",
      "         -1.0468e-02, -5.3388e-02,  5.1095e-03,  1.2363e-01,  1.5979e-02,\n",
      "          1.0417e-01, -3.9850e-03, -3.7124e-02,  8.6612e-03, -1.6073e-01],\n",
      "        [-2.1009e-01, -1.7628e-01,  1.8617e-01, -9.8927e-02, -6.0865e-03,\n",
      "          2.0643e-01, -1.3197e-01, -6.4116e-02, -9.4139e-02,  6.2785e-02,\n",
      "          6.6855e-02, -8.6498e-02,  8.6855e-02, -1.7116e-01, -1.6213e-01,\n",
      "          5.4042e-02,  1.4504e-01, -5.4841e-02,  8.8651e-02,  1.9381e-02],\n",
      "        [-1.1516e-01,  2.1758e-01,  1.3235e-01,  2.1811e-01,  1.6658e-01,\n",
      "          2.0850e-01,  1.6449e-01,  3.8753e-02, -1.1459e-01,  1.4914e-01,\n",
      "         -4.2712e-02, -1.6548e-01,  5.3578e-02,  1.7790e-01, -8.2044e-02,\n",
      "         -3.6236e-03, -1.1084e-03,  4.2327e-02,  1.8189e-01,  8.7828e-02],\n",
      "        [ 1.5677e-01, -8.8392e-02,  6.7582e-02,  1.8264e-01,  2.0093e-01,\n",
      "          5.5666e-02,  9.6283e-02,  2.1842e-01,  1.6173e-01,  9.4710e-02,\n",
      "          2.0935e-01, -4.8631e-02,  1.5682e-01, -2.2501e-02, -1.2372e-01,\n",
      "         -1.5989e-01, -1.8377e-01,  2.2357e-01, -8.8732e-02,  1.3511e-01],\n",
      "        [ 1.1644e-01, -2.4881e-02,  2.1124e-02,  1.4771e-01,  1.2910e-01,\n",
      "          4.2715e-02, -1.9381e-01,  1.7028e-01, -1.2692e-01,  3.6656e-02,\n",
      "          1.0335e-01, -2.2179e-02,  2.0091e-01, -1.7005e-01, -1.9998e-01,\n",
      "         -8.1294e-02,  6.6134e-02, -2.3213e-02,  4.5049e-03, -2.2143e-01],\n",
      "        [ 1.8579e-01,  1.2292e-01,  1.4273e-01, -6.1999e-02, -1.9239e-01,\n",
      "         -5.1972e-02,  1.2097e-01,  1.5654e-01,  2.3986e-02, -9.0602e-02,\n",
      "          1.6457e-01,  1.4425e-01, -1.6227e-01, -7.7050e-02,  1.1572e-01,\n",
      "         -1.1509e-01, -1.6538e-01, -7.7449e-02, -4.1122e-02, -6.9580e-02],\n",
      "        [ 1.2182e-01,  2.0832e-01,  1.4322e-01, -6.5353e-02,  1.3717e-01,\n",
      "          5.3348e-02, -1.7405e-01, -1.9370e-01,  2.2141e-01, -2.2083e-01,\n",
      "          1.9157e-01, -9.3300e-02,  1.2230e-01, -1.3914e-01,  1.8392e-01,\n",
      "         -2.2200e-01,  1.7308e-01, -1.4675e-01, -1.5080e-02, -3.5019e-02],\n",
      "        [ 4.2640e-02,  1.1274e-01, -2.1795e-01,  1.9621e-01,  9.0285e-02,\n",
      "          1.8386e-01,  1.3659e-01, -9.6939e-02, -1.2681e-01,  1.4307e-01,\n",
      "          1.3349e-01, -3.0016e-03, -1.0298e-01, -1.1252e-01, -3.7252e-02,\n",
      "          2.9097e-02, -1.2128e-01, -1.1873e-01,  1.6855e-01, -1.4809e-01],\n",
      "        [ 4.5196e-03, -1.5869e-02, -1.2346e-01,  1.9979e-02,  1.0860e-01,\n",
      "         -1.1124e-01, -4.7101e-03,  1.1298e-01, -1.8232e-01, -1.2354e-01,\n",
      "         -7.0643e-02, -8.9790e-02,  4.0091e-03, -1.5468e-01,  1.8845e-01,\n",
      "          2.1777e-01, -1.1930e-01,  1.2280e-01, -5.3765e-02, -1.5451e-01],\n",
      "        [-2.1570e-01,  3.7961e-02,  1.3495e-02, -2.2991e-02, -8.3944e-02,\n",
      "          1.3464e-01, -1.2090e-01, -2.1930e-01,  8.9483e-02,  2.1752e-01,\n",
      "          2.7055e-02,  5.1104e-02, -4.8710e-02, -3.7112e-02,  2.0964e-01,\n",
      "         -1.4582e-03, -4.3507e-02, -1.8590e-01, -1.0877e-01,  7.2326e-02],\n",
      "        [ 7.5794e-02, -1.7095e-01, -2.6675e-02, -8.1371e-02,  5.1901e-05,\n",
      "          6.8031e-02,  5.3469e-02, -2.0933e-01, -1.0880e-01,  4.4258e-02,\n",
      "          4.9563e-02,  5.9735e-02, -3.6018e-02,  1.4805e-01, -4.4781e-02,\n",
      "         -1.7019e-01, -8.9738e-02, -1.8566e-01,  7.9146e-02, -8.9704e-03],\n",
      "        [ 9.9275e-02, -1.1088e-01, -1.8982e-01,  1.3578e-01, -1.2182e-01,\n",
      "         -1.7640e-01, -1.7410e-01, -1.0681e-01, -1.7119e-01,  1.2248e-01,\n",
      "         -7.1121e-02, -1.0234e-01, -2.5116e-02, -2.2078e-01, -2.1555e-01,\n",
      "          4.4379e-02,  7.1691e-02, -6.7744e-02,  2.0489e-01,  2.0927e-01],\n",
      "        [ 1.7577e-01,  9.3326e-02,  1.7977e-02, -1.5545e-01, -1.9355e-01,\n",
      "         -1.0673e-01, -6.7247e-03, -2.7165e-02, -1.8772e-01,  1.7634e-01,\n",
      "          5.2905e-02,  1.6794e-01, -1.9274e-01,  9.1376e-02, -1.8545e-01,\n",
      "         -1.6884e-01,  1.9034e-01, -2.0111e-01, -8.2601e-02, -5.4234e-04],\n",
      "        [-1.1767e-01,  1.8416e-01,  1.4902e-01,  1.2975e-01,  2.0832e-01,\n",
      "         -2.1104e-01,  1.6736e-01,  1.6625e-01, -1.9497e-02, -1.1195e-01,\n",
      "          1.7333e-02, -1.9528e-01, -1.1541e-01,  1.6354e-01,  1.5802e-01,\n",
      "         -6.8767e-02, -2.6387e-02, -1.7000e-01, -1.3462e-02, -3.7984e-02],\n",
      "        [ 1.2052e-01, -2.1450e-01, -1.5784e-01, -2.0522e-01, -2.0006e-01,\n",
      "         -5.5158e-02, -1.7979e-01, -1.2280e-01, -6.6288e-02,  1.3554e-01,\n",
      "         -1.0111e-01,  2.1297e-01,  2.3445e-02, -2.1062e-02,  2.1269e-01,\n",
      "         -8.1004e-02, -1.5873e-01,  7.8636e-02,  1.1170e-01,  2.0108e-01],\n",
      "        [ 1.2920e-01, -1.1668e-01, -6.1894e-02,  4.1314e-02,  1.0885e-01,\n",
      "         -6.5787e-02,  1.2300e-01, -2.4262e-02,  3.6823e-02,  8.6368e-02,\n",
      "         -9.3849e-02, -9.2154e-02, -5.2581e-02,  3.6792e-02,  6.4748e-02,\n",
      "         -2.0406e-01,  7.5407e-02, -1.9474e-01,  2.2056e-01,  1.4203e-01],\n",
      "        [ 1.4902e-01,  1.0899e-01, -8.7572e-02,  5.7056e-02, -8.0392e-02,\n",
      "         -1.0266e-01, -1.4619e-01, -1.6433e-01, -8.5827e-02, -1.4165e-01,\n",
      "         -2.1094e-01, -1.0098e-01,  1.0642e-02, -1.9476e-01, -1.9215e-01,\n",
      "          1.7270e-01, -1.6871e-01,  1.1201e-01,  1.3733e-01,  1.6495e-01],\n",
      "        [-2.2321e-01, -2.0294e-01, -6.9545e-03, -1.3796e-01,  1.6684e-02,\n",
      "         -3.8216e-02,  6.5741e-02,  1.8226e-01, -1.4079e-01, -1.9680e-01,\n",
      "          1.1447e-01, -1.2194e-02,  4.0804e-02, -1.2129e-01, -1.5499e-01,\n",
      "          2.1325e-01,  9.7841e-02, -1.0091e-01, -2.1354e-01, -1.9938e-01],\n",
      "        [ 1.6213e-01, -2.2327e-01, -2.0321e-01, -1.8489e-01, -1.9994e-01,\n",
      "         -3.9075e-02,  2.0115e-01,  1.7312e-01,  1.9014e-01, -1.5643e-01,\n",
      "         -1.3398e-01,  9.7930e-02, -2.1428e-01,  6.6299e-02, -1.9749e-01,\n",
      "          1.3492e-01, -2.9469e-03,  6.7340e-03, -9.3956e-02,  1.3752e-01],\n",
      "        [-5.8705e-02,  3.9247e-02,  1.3362e-01,  1.4685e-01, -2.0140e-01,\n",
      "          1.0757e-01, -2.2029e-01,  1.5817e-01, -2.1989e-01, -1.8155e-01,\n",
      "         -9.6719e-02, -5.9139e-02, -8.5242e-02, -1.9385e-01,  8.0575e-02,\n",
      "          1.5113e-01,  2.1905e-01, -1.3780e-01,  2.1024e-01, -2.0191e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1612,  0.0622,  0.0039,  0.2028,  0.2114,  0.0011, -0.1033, -0.1906,\n",
      "         0.1160,  0.1056, -0.0821,  0.1280,  0.1897,  0.1565, -0.0298, -0.1918,\n",
      "        -0.0602, -0.0693, -0.1324, -0.1990], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0474,  0.0266,  0.1740,  0.0967, -0.1800,  0.0469,  0.1093,  0.1286,\n",
      "         -0.1707, -0.0909, -0.1817, -0.1808, -0.0627,  0.0246, -0.1952, -0.0116,\n",
      "         -0.1053,  0.0341,  0.1206, -0.1414],\n",
      "        [-0.0034,  0.0304,  0.2205, -0.1248,  0.1973,  0.1450, -0.1329, -0.0725,\n",
      "          0.1972,  0.0964, -0.0743, -0.2013,  0.1504, -0.0092, -0.1572, -0.0640,\n",
      "         -0.0939,  0.1747,  0.1912, -0.0335],\n",
      "        [ 0.1678, -0.1831,  0.0636,  0.0496,  0.1904, -0.1971,  0.1803, -0.0482,\n",
      "          0.0955, -0.0941,  0.2120, -0.0391,  0.1335, -0.1899,  0.1099,  0.1615,\n",
      "         -0.0746, -0.0095, -0.1294, -0.1531],\n",
      "        [-0.0129,  0.1971, -0.1768, -0.0758, -0.1169, -0.0041,  0.1210, -0.0488,\n",
      "         -0.1827,  0.0473, -0.1166, -0.2117,  0.0316,  0.1105, -0.2087,  0.1137,\n",
      "         -0.1456,  0.0316,  0.2122,  0.1752],\n",
      "        [-0.0560, -0.2052, -0.0151,  0.1122,  0.1153,  0.0649,  0.1576,  0.0483,\n",
      "         -0.1254, -0.1296,  0.0901, -0.2072, -0.1773,  0.1776,  0.2105, -0.1593,\n",
      "          0.1222,  0.0836,  0.0114, -0.2134]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1118,  0.0742,  0.0083,  0.0204, -0.0003], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.4343,  0.1931],\n",
      "        [-0.3933, -0.5824]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3651, -0.0707], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in obj1.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59iLYeT-NRsG"
   },
   "source": [
    "# RUN for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ekz8oACnb3tA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:372: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n",
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 total_train_loss: 1.027460 Total_test_loss: 0.866746 Total_BCE_test_loss: 0.789441 Total_KLD_test_loss: 0.000007 Total_CEP_test_loss: 0.077297\n",
      "====> Epoch: 2 total_train_loss: 0.946405 Total_test_loss: 0.866250 Total_BCE_test_loss: 0.789049 Total_KLD_test_loss: 0.000009 Total_CEP_test_loss: 0.077192\n",
      "====> Epoch: 3 total_train_loss: 0.914498 Total_test_loss: 0.865015 Total_BCE_test_loss: 0.789062 Total_KLD_test_loss: 0.000012 Total_CEP_test_loss: 0.075941\n",
      "====> Epoch: 4 total_train_loss: 0.904479 Total_test_loss: 0.865420 Total_BCE_test_loss: 0.790327 Total_KLD_test_loss: 0.000018 Total_CEP_test_loss: 0.075076\n",
      "====> Epoch: 5 total_train_loss: 0.895540 Total_test_loss: 0.866335 Total_BCE_test_loss: 0.791058 Total_KLD_test_loss: 0.000025 Total_CEP_test_loss: 0.075251\n",
      "====> Epoch: 6 total_train_loss: 0.888908 Total_test_loss: 0.866321 Total_BCE_test_loss: 0.792112 Total_KLD_test_loss: 0.000033 Total_CEP_test_loss: 0.074176\n",
      "====> Epoch: 7 total_train_loss: 0.882132 Total_test_loss: 0.862787 Total_BCE_test_loss: 0.790590 Total_KLD_test_loss: 0.000041 Total_CEP_test_loss: 0.072156\n",
      "====> Epoch: 8 total_train_loss: 0.879108 Total_test_loss: 0.863566 Total_BCE_test_loss: 0.791208 Total_KLD_test_loss: 0.000050 Total_CEP_test_loss: 0.072308\n",
      "====> Epoch: 9 total_train_loss: 0.877869 Total_test_loss: 0.863871 Total_BCE_test_loss: 0.792197 Total_KLD_test_loss: 0.000059 Total_CEP_test_loss: 0.071615\n",
      "====> Epoch: 10 total_train_loss: 0.871280 Total_test_loss: 0.861425 Total_BCE_test_loss: 0.789980 Total_KLD_test_loss: 0.000067 Total_CEP_test_loss: 0.071378\n",
      "====> Epoch: 11 total_train_loss: 0.874624 Total_test_loss: 0.859459 Total_BCE_test_loss: 0.788507 Total_KLD_test_loss: 0.000072 Total_CEP_test_loss: 0.070880\n",
      "====> Epoch: 12 total_train_loss: 0.870364 Total_test_loss: 0.856148 Total_BCE_test_loss: 0.786584 Total_KLD_test_loss: 0.000076 Total_CEP_test_loss: 0.069487\n",
      "====> Epoch: 13 total_train_loss: 0.866500 Total_test_loss: 0.856421 Total_BCE_test_loss: 0.785957 Total_KLD_test_loss: 0.000077 Total_CEP_test_loss: 0.070388\n",
      "====> Epoch: 14 total_train_loss: 0.869296 Total_test_loss: 0.857204 Total_BCE_test_loss: 0.786549 Total_KLD_test_loss: 0.000078 Total_CEP_test_loss: 0.070577\n",
      "====> Epoch: 15 total_train_loss: 0.863938 Total_test_loss: 0.855756 Total_BCE_test_loss: 0.785394 Total_KLD_test_loss: 0.000081 Total_CEP_test_loss: 0.070280\n",
      "====> Epoch: 16 total_train_loss: 0.864803 Total_test_loss: 0.854691 Total_BCE_test_loss: 0.784771 Total_KLD_test_loss: 0.000086 Total_CEP_test_loss: 0.069834\n",
      "====> Epoch: 17 total_train_loss: 0.860971 Total_test_loss: 0.855482 Total_BCE_test_loss: 0.785730 Total_KLD_test_loss: 0.000089 Total_CEP_test_loss: 0.069663\n",
      "====> Epoch: 18 total_train_loss: 0.862275 Total_test_loss: 0.854846 Total_BCE_test_loss: 0.785142 Total_KLD_test_loss: 0.000093 Total_CEP_test_loss: 0.069611\n",
      "====> Epoch: 19 total_train_loss: 0.863060 Total_test_loss: 0.854610 Total_BCE_test_loss: 0.784841 Total_KLD_test_loss: 0.000094 Total_CEP_test_loss: 0.069675\n",
      "====> Epoch: 20 total_train_loss: 0.859021 Total_test_loss: 0.852410 Total_BCE_test_loss: 0.782771 Total_KLD_test_loss: 0.000095 Total_CEP_test_loss: 0.069544\n",
      "====> Epoch: 21 total_train_loss: 0.855244 Total_test_loss: 0.848415 Total_BCE_test_loss: 0.778957 Total_KLD_test_loss: 0.000094 Total_CEP_test_loss: 0.069363\n",
      "====> Epoch: 22 total_train_loss: 0.854907 Total_test_loss: 0.850405 Total_BCE_test_loss: 0.780695 Total_KLD_test_loss: 0.000096 Total_CEP_test_loss: 0.069614\n",
      "====> Epoch: 23 total_train_loss: 0.856207 Total_test_loss: 0.847619 Total_BCE_test_loss: 0.778132 Total_KLD_test_loss: 0.000098 Total_CEP_test_loss: 0.069389\n",
      "====> Epoch: 24 total_train_loss: 0.857935 Total_test_loss: 0.845959 Total_BCE_test_loss: 0.776407 Total_KLD_test_loss: 0.000102 Total_CEP_test_loss: 0.069450\n",
      "====> Epoch: 25 total_train_loss: 0.850428 Total_test_loss: 0.843404 Total_BCE_test_loss: 0.774069 Total_KLD_test_loss: 0.000107 Total_CEP_test_loss: 0.069228\n",
      "====> Epoch: 26 total_train_loss: 0.850921 Total_test_loss: 0.841817 Total_BCE_test_loss: 0.772436 Total_KLD_test_loss: 0.000115 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 27 total_train_loss: 0.850889 Total_test_loss: 0.837152 Total_BCE_test_loss: 0.767720 Total_KLD_test_loss: 0.000123 Total_CEP_test_loss: 0.069309\n",
      "====> Epoch: 28 total_train_loss: 0.848913 Total_test_loss: 0.831822 Total_BCE_test_loss: 0.762332 Total_KLD_test_loss: 0.000136 Total_CEP_test_loss: 0.069354\n",
      "====> Epoch: 29 total_train_loss: 0.844889 Total_test_loss: 0.833167 Total_BCE_test_loss: 0.763681 Total_KLD_test_loss: 0.000149 Total_CEP_test_loss: 0.069336\n",
      "====> Epoch: 30 total_train_loss: 0.840223 Total_test_loss: 0.826692 Total_BCE_test_loss: 0.757227 Total_KLD_test_loss: 0.000169 Total_CEP_test_loss: 0.069295\n",
      "====> Epoch: 31 total_train_loss: 0.838858 Total_test_loss: 0.816170 Total_BCE_test_loss: 0.746649 Total_KLD_test_loss: 0.000190 Total_CEP_test_loss: 0.069330\n",
      "====> Epoch: 32 total_train_loss: 0.827224 Total_test_loss: 0.808268 Total_BCE_test_loss: 0.738730 Total_KLD_test_loss: 0.000221 Total_CEP_test_loss: 0.069316\n",
      "====> Epoch: 33 total_train_loss: 0.825870 Total_test_loss: 0.800803 Total_BCE_test_loss: 0.731234 Total_KLD_test_loss: 0.000253 Total_CEP_test_loss: 0.069316\n",
      "====> Epoch: 34 total_train_loss: 0.821891 Total_test_loss: 0.790108 Total_BCE_test_loss: 0.720483 Total_KLD_test_loss: 0.000299 Total_CEP_test_loss: 0.069325\n",
      "====> Epoch: 35 total_train_loss: 0.810187 Total_test_loss: 0.786333 Total_BCE_test_loss: 0.716675 Total_KLD_test_loss: 0.000346 Total_CEP_test_loss: 0.069313\n",
      "====> Epoch: 36 total_train_loss: 0.807432 Total_test_loss: 0.776517 Total_BCE_test_loss: 0.706797 Total_KLD_test_loss: 0.000410 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 37 total_train_loss: 0.799315 Total_test_loss: 0.776101 Total_BCE_test_loss: 0.706343 Total_KLD_test_loss: 0.000463 Total_CEP_test_loss: 0.069296\n",
      "====> Epoch: 38 total_train_loss: 0.799315 Total_test_loss: 0.770790 Total_BCE_test_loss: 0.701033 Total_KLD_test_loss: 0.000501 Total_CEP_test_loss: 0.069256\n",
      "====> Epoch: 39 total_train_loss: 0.786717 Total_test_loss: 0.764863 Total_BCE_test_loss: 0.695067 Total_KLD_test_loss: 0.000547 Total_CEP_test_loss: 0.069250\n",
      "====> Epoch: 40 total_train_loss: 0.783170 Total_test_loss: 0.761699 Total_BCE_test_loss: 0.691873 Total_KLD_test_loss: 0.000601 Total_CEP_test_loss: 0.069225\n",
      "====> Epoch: 41 total_train_loss: 0.780636 Total_test_loss: 0.745960 Total_BCE_test_loss: 0.676067 Total_KLD_test_loss: 0.000662 Total_CEP_test_loss: 0.069231\n",
      "====> Epoch: 42 total_train_loss: 0.769807 Total_test_loss: 0.740014 Total_BCE_test_loss: 0.670117 Total_KLD_test_loss: 0.000713 Total_CEP_test_loss: 0.069184\n",
      "====> Epoch: 43 total_train_loss: 0.764695 Total_test_loss: 0.733227 Total_BCE_test_loss: 0.663197 Total_KLD_test_loss: 0.000760 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 44 total_train_loss: 0.766126 Total_test_loss: 0.724380 Total_BCE_test_loss: 0.654268 Total_KLD_test_loss: 0.000822 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 45 total_train_loss: 0.756220 Total_test_loss: 0.715977 Total_BCE_test_loss: 0.645773 Total_KLD_test_loss: 0.000908 Total_CEP_test_loss: 0.069296\n",
      "====> Epoch: 46 total_train_loss: 0.745269 Total_test_loss: 0.713699 Total_BCE_test_loss: 0.643420 Total_KLD_test_loss: 0.000978 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 47 total_train_loss: 0.741337 Total_test_loss: 0.712618 Total_BCE_test_loss: 0.642292 Total_KLD_test_loss: 0.001032 Total_CEP_test_loss: 0.069294\n",
      "====> Epoch: 48 total_train_loss: 0.735263 Total_test_loss: 0.701465 Total_BCE_test_loss: 0.631130 Total_KLD_test_loss: 0.001100 Total_CEP_test_loss: 0.069236\n",
      "====> Epoch: 49 total_train_loss: 0.727846 Total_test_loss: 0.696085 Total_BCE_test_loss: 0.625717 Total_KLD_test_loss: 0.001131 Total_CEP_test_loss: 0.069238\n",
      "====> Epoch: 50 total_train_loss: 0.730900 Total_test_loss: 0.692520 Total_BCE_test_loss: 0.622093 Total_KLD_test_loss: 0.001194 Total_CEP_test_loss: 0.069233\n",
      "====> Epoch: 51 total_train_loss: 0.724659 Total_test_loss: 0.685965 Total_BCE_test_loss: 0.615461 Total_KLD_test_loss: 0.001284 Total_CEP_test_loss: 0.069220\n",
      "====> Epoch: 52 total_train_loss: 0.721371 Total_test_loss: 0.684467 Total_BCE_test_loss: 0.613895 Total_KLD_test_loss: 0.001350 Total_CEP_test_loss: 0.069223\n",
      "====> Epoch: 53 total_train_loss: 0.718102 Total_test_loss: 0.682685 Total_BCE_test_loss: 0.612013 Total_KLD_test_loss: 0.001423 Total_CEP_test_loss: 0.069249\n",
      "====> Epoch: 54 total_train_loss: 0.715626 Total_test_loss: 0.680544 Total_BCE_test_loss: 0.609808 Total_KLD_test_loss: 0.001468 Total_CEP_test_loss: 0.069268\n",
      "====> Epoch: 55 total_train_loss: 0.709314 Total_test_loss: 0.680593 Total_BCE_test_loss: 0.609793 Total_KLD_test_loss: 0.001515 Total_CEP_test_loss: 0.069285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 56 total_train_loss: 0.706591 Total_test_loss: 0.682936 Total_BCE_test_loss: 0.612052 Total_KLD_test_loss: 0.001566 Total_CEP_test_loss: 0.069318\n",
      "====> Epoch: 57 total_train_loss: 0.703663 Total_test_loss: 0.679614 Total_BCE_test_loss: 0.608684 Total_KLD_test_loss: 0.001614 Total_CEP_test_loss: 0.069316\n",
      "====> Epoch: 58 total_train_loss: 0.705050 Total_test_loss: 0.675095 Total_BCE_test_loss: 0.604094 Total_KLD_test_loss: 0.001686 Total_CEP_test_loss: 0.069315\n",
      "====> Epoch: 59 total_train_loss: 0.700464 Total_test_loss: 0.672194 Total_BCE_test_loss: 0.601132 Total_KLD_test_loss: 0.001753 Total_CEP_test_loss: 0.069309\n",
      "====> Epoch: 60 total_train_loss: 0.698936 Total_test_loss: 0.671108 Total_BCE_test_loss: 0.600026 Total_KLD_test_loss: 0.001795 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 61 total_train_loss: 0.697718 Total_test_loss: 0.670977 Total_BCE_test_loss: 0.599886 Total_KLD_test_loss: 0.001822 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 62 total_train_loss: 0.695276 Total_test_loss: 0.668510 Total_BCE_test_loss: 0.597424 Total_KLD_test_loss: 0.001845 Total_CEP_test_loss: 0.069241\n",
      "====> Epoch: 63 total_train_loss: 0.700378 Total_test_loss: 0.664024 Total_BCE_test_loss: 0.592919 Total_KLD_test_loss: 0.001859 Total_CEP_test_loss: 0.069246\n",
      "====> Epoch: 64 total_train_loss: 0.697274 Total_test_loss: 0.661426 Total_BCE_test_loss: 0.590278 Total_KLD_test_loss: 0.001884 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 65 total_train_loss: 0.693755 Total_test_loss: 0.660995 Total_BCE_test_loss: 0.589810 Total_KLD_test_loss: 0.001896 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 66 total_train_loss: 0.692402 Total_test_loss: 0.660387 Total_BCE_test_loss: 0.589143 Total_KLD_test_loss: 0.001934 Total_CEP_test_loss: 0.069309\n",
      "====> Epoch: 67 total_train_loss: 0.687896 Total_test_loss: 0.659856 Total_BCE_test_loss: 0.588589 Total_KLD_test_loss: 0.001968 Total_CEP_test_loss: 0.069298\n",
      "====> Epoch: 68 total_train_loss: 0.692682 Total_test_loss: 0.657655 Total_BCE_test_loss: 0.586388 Total_KLD_test_loss: 0.001985 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 69 total_train_loss: 0.685847 Total_test_loss: 0.655633 Total_BCE_test_loss: 0.584387 Total_KLD_test_loss: 0.001992 Total_CEP_test_loss: 0.069254\n",
      "====> Epoch: 70 total_train_loss: 0.689364 Total_test_loss: 0.652609 Total_BCE_test_loss: 0.581379 Total_KLD_test_loss: 0.001998 Total_CEP_test_loss: 0.069232\n",
      "====> Epoch: 71 total_train_loss: 0.685484 Total_test_loss: 0.650721 Total_BCE_test_loss: 0.579500 Total_KLD_test_loss: 0.002035 Total_CEP_test_loss: 0.069186\n",
      "====> Epoch: 72 total_train_loss: 0.683644 Total_test_loss: 0.650425 Total_BCE_test_loss: 0.579197 Total_KLD_test_loss: 0.002063 Total_CEP_test_loss: 0.069165\n",
      "====> Epoch: 73 total_train_loss: 0.684390 Total_test_loss: 0.645842 Total_BCE_test_loss: 0.574557 Total_KLD_test_loss: 0.002079 Total_CEP_test_loss: 0.069206\n",
      "====> Epoch: 74 total_train_loss: 0.678147 Total_test_loss: 0.646183 Total_BCE_test_loss: 0.574821 Total_KLD_test_loss: 0.002072 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 75 total_train_loss: 0.683942 Total_test_loss: 0.647118 Total_BCE_test_loss: 0.575743 Total_KLD_test_loss: 0.002070 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 76 total_train_loss: 0.680529 Total_test_loss: 0.646889 Total_BCE_test_loss: 0.575576 Total_KLD_test_loss: 0.002065 Total_CEP_test_loss: 0.069248\n",
      "====> Epoch: 77 total_train_loss: 0.676290 Total_test_loss: 0.643390 Total_BCE_test_loss: 0.572102 Total_KLD_test_loss: 0.002085 Total_CEP_test_loss: 0.069204\n",
      "====> Epoch: 78 total_train_loss: 0.676490 Total_test_loss: 0.643155 Total_BCE_test_loss: 0.571850 Total_KLD_test_loss: 0.002081 Total_CEP_test_loss: 0.069224\n",
      "====> Epoch: 79 total_train_loss: 0.672019 Total_test_loss: 0.640959 Total_BCE_test_loss: 0.569643 Total_KLD_test_loss: 0.002084 Total_CEP_test_loss: 0.069233\n",
      "====> Epoch: 80 total_train_loss: 0.677136 Total_test_loss: 0.640162 Total_BCE_test_loss: 0.568806 Total_KLD_test_loss: 0.002098 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 81 total_train_loss: 0.671961 Total_test_loss: 0.638660 Total_BCE_test_loss: 0.567230 Total_KLD_test_loss: 0.002104 Total_CEP_test_loss: 0.069326\n",
      "====> Epoch: 82 total_train_loss: 0.673037 Total_test_loss: 0.635332 Total_BCE_test_loss: 0.563851 Total_KLD_test_loss: 0.002126 Total_CEP_test_loss: 0.069355\n",
      "====> Epoch: 83 total_train_loss: 0.674992 Total_test_loss: 0.640328 Total_BCE_test_loss: 0.568886 Total_KLD_test_loss: 0.002113 Total_CEP_test_loss: 0.069329\n",
      "====> Epoch: 84 total_train_loss: 0.674546 Total_test_loss: 0.637768 Total_BCE_test_loss: 0.566337 Total_KLD_test_loss: 0.002116 Total_CEP_test_loss: 0.069315\n",
      "====> Epoch: 85 total_train_loss: 0.671669 Total_test_loss: 0.634644 Total_BCE_test_loss: 0.563274 Total_KLD_test_loss: 0.002071 Total_CEP_test_loss: 0.069299\n",
      "====> Epoch: 86 total_train_loss: 0.678316 Total_test_loss: 0.635147 Total_BCE_test_loss: 0.563776 Total_KLD_test_loss: 0.002125 Total_CEP_test_loss: 0.069246\n",
      "====> Epoch: 87 total_train_loss: 0.669737 Total_test_loss: 0.637202 Total_BCE_test_loss: 0.565791 Total_KLD_test_loss: 0.002174 Total_CEP_test_loss: 0.069237\n",
      "====> Epoch: 88 total_train_loss: 0.674588 Total_test_loss: 0.637296 Total_BCE_test_loss: 0.565879 Total_KLD_test_loss: 0.002219 Total_CEP_test_loss: 0.069198\n",
      "====> Epoch: 89 total_train_loss: 0.673775 Total_test_loss: 0.635466 Total_BCE_test_loss: 0.563965 Total_KLD_test_loss: 0.002223 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 90 total_train_loss: 0.672123 Total_test_loss: 0.636378 Total_BCE_test_loss: 0.564865 Total_KLD_test_loss: 0.002210 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 91 total_train_loss: 0.667745 Total_test_loss: 0.634942 Total_BCE_test_loss: 0.563384 Total_KLD_test_loss: 0.002247 Total_CEP_test_loss: 0.069311\n",
      "====> Epoch: 92 total_train_loss: 0.665431 Total_test_loss: 0.638080 Total_BCE_test_loss: 0.566498 Total_KLD_test_loss: 0.002273 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 93 total_train_loss: 0.670754 Total_test_loss: 0.634328 Total_BCE_test_loss: 0.562724 Total_KLD_test_loss: 0.002285 Total_CEP_test_loss: 0.069319\n",
      "====> Epoch: 94 total_train_loss: 0.672018 Total_test_loss: 0.630699 Total_BCE_test_loss: 0.559087 Total_KLD_test_loss: 0.002274 Total_CEP_test_loss: 0.069338\n",
      "====> Epoch: 95 total_train_loss: 0.665270 Total_test_loss: 0.627649 Total_BCE_test_loss: 0.556005 Total_KLD_test_loss: 0.002303 Total_CEP_test_loss: 0.069340\n",
      "====> Epoch: 96 total_train_loss: 0.665692 Total_test_loss: 0.632640 Total_BCE_test_loss: 0.560956 Total_KLD_test_loss: 0.002365 Total_CEP_test_loss: 0.069318\n",
      "====> Epoch: 97 total_train_loss: 0.665316 Total_test_loss: 0.628668 Total_BCE_test_loss: 0.556985 Total_KLD_test_loss: 0.002395 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 98 total_train_loss: 0.664947 Total_test_loss: 0.625508 Total_BCE_test_loss: 0.553860 Total_KLD_test_loss: 0.002408 Total_CEP_test_loss: 0.069240\n",
      "====> Epoch: 99 total_train_loss: 0.660599 Total_test_loss: 0.623379 Total_BCE_test_loss: 0.551704 Total_KLD_test_loss: 0.002411 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 100 total_train_loss: 0.663055 Total_test_loss: 0.621185 Total_BCE_test_loss: 0.549394 Total_KLD_test_loss: 0.002463 Total_CEP_test_loss: 0.069328\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.0005\n",
      "====> Epoch: 1 total_train_loss: 0.662284 Total_test_loss: 0.619207 Total_BCE_test_loss: 0.547441 Total_KLD_test_loss: 0.002446 Total_CEP_test_loss: 0.069320\n",
      "====> Epoch: 2 total_train_loss: 0.658923 Total_test_loss: 0.619219 Total_BCE_test_loss: 0.547505 Total_KLD_test_loss: 0.002401 Total_CEP_test_loss: 0.069313\n",
      "====> Epoch: 3 total_train_loss: 0.660447 Total_test_loss: 0.619263 Total_BCE_test_loss: 0.547582 Total_KLD_test_loss: 0.002381 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 4 total_train_loss: 0.657123 Total_test_loss: 0.618925 Total_BCE_test_loss: 0.547246 Total_KLD_test_loss: 0.002346 Total_CEP_test_loss: 0.069333\n",
      "====> Epoch: 5 total_train_loss: 0.655731 Total_test_loss: 0.618772 Total_BCE_test_loss: 0.547153 Total_KLD_test_loss: 0.002348 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 6 total_train_loss: 0.657897 Total_test_loss: 0.617627 Total_BCE_test_loss: 0.545998 Total_KLD_test_loss: 0.002348 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 7 total_train_loss: 0.656657 Total_test_loss: 0.616679 Total_BCE_test_loss: 0.545049 Total_KLD_test_loss: 0.002356 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 8 total_train_loss: 0.656395 Total_test_loss: 0.617110 Total_BCE_test_loss: 0.545503 Total_KLD_test_loss: 0.002333 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 9 total_train_loss: 0.655992 Total_test_loss: 0.617121 Total_BCE_test_loss: 0.545487 Total_KLD_test_loss: 0.002335 Total_CEP_test_loss: 0.069299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 total_train_loss: 0.653674 Total_test_loss: 0.617592 Total_BCE_test_loss: 0.545988 Total_KLD_test_loss: 0.002329 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 11 total_train_loss: 0.659796 Total_test_loss: 0.616605 Total_BCE_test_loss: 0.544978 Total_KLD_test_loss: 0.002330 Total_CEP_test_loss: 0.069298\n",
      "====> Epoch: 12 total_train_loss: 0.658288 Total_test_loss: 0.617324 Total_BCE_test_loss: 0.545741 Total_KLD_test_loss: 0.002306 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 13 total_train_loss: 0.654680 Total_test_loss: 0.616153 Total_BCE_test_loss: 0.544562 Total_KLD_test_loss: 0.002304 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 14 total_train_loss: 0.652106 Total_test_loss: 0.617154 Total_BCE_test_loss: 0.545583 Total_KLD_test_loss: 0.002306 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 15 total_train_loss: 0.653277 Total_test_loss: 0.616196 Total_BCE_test_loss: 0.544667 Total_KLD_test_loss: 0.002302 Total_CEP_test_loss: 0.069227\n",
      "====> Epoch: 16 total_train_loss: 0.654791 Total_test_loss: 0.616655 Total_BCE_test_loss: 0.545106 Total_KLD_test_loss: 0.002296 Total_CEP_test_loss: 0.069253\n",
      "====> Epoch: 17 total_train_loss: 0.658048 Total_test_loss: 0.616355 Total_BCE_test_loss: 0.544786 Total_KLD_test_loss: 0.002278 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 18 total_train_loss: 0.652647 Total_test_loss: 0.616520 Total_BCE_test_loss: 0.544943 Total_KLD_test_loss: 0.002309 Total_CEP_test_loss: 0.069268\n",
      "====> Epoch: 19 total_train_loss: 0.655448 Total_test_loss: 0.615393 Total_BCE_test_loss: 0.543800 Total_KLD_test_loss: 0.002319 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 20 total_train_loss: 0.651517 Total_test_loss: 0.615438 Total_BCE_test_loss: 0.543842 Total_KLD_test_loss: 0.002324 Total_CEP_test_loss: 0.069272\n",
      "====> Epoch: 21 total_train_loss: 0.652763 Total_test_loss: 0.615452 Total_BCE_test_loss: 0.543834 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 22 total_train_loss: 0.652672 Total_test_loss: 0.614901 Total_BCE_test_loss: 0.543237 Total_KLD_test_loss: 0.002373 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 23 total_train_loss: 0.650874 Total_test_loss: 0.615046 Total_BCE_test_loss: 0.543440 Total_KLD_test_loss: 0.002326 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 24 total_train_loss: 0.655968 Total_test_loss: 0.615633 Total_BCE_test_loss: 0.544045 Total_KLD_test_loss: 0.002308 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 25 total_train_loss: 0.656652 Total_test_loss: 0.615655 Total_BCE_test_loss: 0.544027 Total_KLD_test_loss: 0.002330 Total_CEP_test_loss: 0.069299\n",
      "====> Epoch: 26 total_train_loss: 0.653341 Total_test_loss: 0.614190 Total_BCE_test_loss: 0.542592 Total_KLD_test_loss: 0.002318 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 27 total_train_loss: 0.657497 Total_test_loss: 0.613896 Total_BCE_test_loss: 0.542260 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 28 total_train_loss: 0.659011 Total_test_loss: 0.613692 Total_BCE_test_loss: 0.542033 Total_KLD_test_loss: 0.002368 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 29 total_train_loss: 0.652555 Total_test_loss: 0.611929 Total_BCE_test_loss: 0.540284 Total_KLD_test_loss: 0.002361 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 30 total_train_loss: 0.651875 Total_test_loss: 0.612102 Total_BCE_test_loss: 0.540460 Total_KLD_test_loss: 0.002354 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 31 total_train_loss: 0.652821 Total_test_loss: 0.612409 Total_BCE_test_loss: 0.540768 Total_KLD_test_loss: 0.002361 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 32 total_train_loss: 0.652302 Total_test_loss: 0.611494 Total_BCE_test_loss: 0.539844 Total_KLD_test_loss: 0.002356 Total_CEP_test_loss: 0.069293\n",
      "====> Epoch: 33 total_train_loss: 0.654205 Total_test_loss: 0.610845 Total_BCE_test_loss: 0.539196 Total_KLD_test_loss: 0.002360 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 34 total_train_loss: 0.659328 Total_test_loss: 0.610195 Total_BCE_test_loss: 0.538537 Total_KLD_test_loss: 0.002357 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 35 total_train_loss: 0.653497 Total_test_loss: 0.610221 Total_BCE_test_loss: 0.538572 Total_KLD_test_loss: 0.002352 Total_CEP_test_loss: 0.069297\n",
      "====> Epoch: 36 total_train_loss: 0.653649 Total_test_loss: 0.611866 Total_BCE_test_loss: 0.540238 Total_KLD_test_loss: 0.002342 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 37 total_train_loss: 0.651636 Total_test_loss: 0.612428 Total_BCE_test_loss: 0.540802 Total_KLD_test_loss: 0.002331 Total_CEP_test_loss: 0.069295\n",
      "====> Epoch: 38 total_train_loss: 0.657701 Total_test_loss: 0.611768 Total_BCE_test_loss: 0.540150 Total_KLD_test_loss: 0.002334 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 39 total_train_loss: 0.654012 Total_test_loss: 0.611647 Total_BCE_test_loss: 0.540009 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 40 total_train_loss: 0.651126 Total_test_loss: 0.610698 Total_BCE_test_loss: 0.539076 Total_KLD_test_loss: 0.002335 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 41 total_train_loss: 0.650763 Total_test_loss: 0.610232 Total_BCE_test_loss: 0.538601 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 42 total_train_loss: 0.649952 Total_test_loss: 0.609690 Total_BCE_test_loss: 0.538046 Total_KLD_test_loss: 0.002358 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 43 total_train_loss: 0.649503 Total_test_loss: 0.609913 Total_BCE_test_loss: 0.538275 Total_KLD_test_loss: 0.002364 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 44 total_train_loss: 0.649443 Total_test_loss: 0.608589 Total_BCE_test_loss: 0.536970 Total_KLD_test_loss: 0.002342 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 45 total_train_loss: 0.655640 Total_test_loss: 0.607862 Total_BCE_test_loss: 0.536223 Total_KLD_test_loss: 0.002370 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 46 total_train_loss: 0.648955 Total_test_loss: 0.607369 Total_BCE_test_loss: 0.535720 Total_KLD_test_loss: 0.002374 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 47 total_train_loss: 0.646130 Total_test_loss: 0.607762 Total_BCE_test_loss: 0.536140 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 48 total_train_loss: 0.647737 Total_test_loss: 0.607873 Total_BCE_test_loss: 0.536235 Total_KLD_test_loss: 0.002360 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 49 total_train_loss: 0.648592 Total_test_loss: 0.608617 Total_BCE_test_loss: 0.536990 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 50 total_train_loss: 0.649415 Total_test_loss: 0.609052 Total_BCE_test_loss: 0.537442 Total_KLD_test_loss: 0.002341 Total_CEP_test_loss: 0.069269\n",
      "1e-05\n",
      "====> Epoch: 1 total_train_loss: 0.648920 Total_test_loss: 0.607641 Total_BCE_test_loss: 0.536001 Total_KLD_test_loss: 0.002365 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 2 total_train_loss: 0.649828 Total_test_loss: 0.608380 Total_BCE_test_loss: 0.536747 Total_KLD_test_loss: 0.002355 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 3 total_train_loss: 0.655111 Total_test_loss: 0.608751 Total_BCE_test_loss: 0.537124 Total_KLD_test_loss: 0.002354 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 4 total_train_loss: 0.651220 Total_test_loss: 0.607474 Total_BCE_test_loss: 0.535792 Total_KLD_test_loss: 0.002396 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 5 total_train_loss: 0.652264 Total_test_loss: 0.607064 Total_BCE_test_loss: 0.535405 Total_KLD_test_loss: 0.002383 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 6 total_train_loss: 0.653108 Total_test_loss: 0.608163 Total_BCE_test_loss: 0.536496 Total_KLD_test_loss: 0.002387 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 7 total_train_loss: 0.643058 Total_test_loss: 0.608597 Total_BCE_test_loss: 0.536976 Total_KLD_test_loss: 0.002352 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 8 total_train_loss: 0.653842 Total_test_loss: 0.608393 Total_BCE_test_loss: 0.536772 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 9 total_train_loss: 0.650997 Total_test_loss: 0.608113 Total_BCE_test_loss: 0.536467 Total_KLD_test_loss: 0.002372 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 10 total_train_loss: 0.649576 Total_test_loss: 0.608274 Total_BCE_test_loss: 0.536639 Total_KLD_test_loss: 0.002359 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 11 total_train_loss: 0.646971 Total_test_loss: 0.608057 Total_BCE_test_loss: 0.536425 Total_KLD_test_loss: 0.002353 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 12 total_train_loss: 0.650355 Total_test_loss: 0.608339 Total_BCE_test_loss: 0.536719 Total_KLD_test_loss: 0.002342 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 13 total_train_loss: 0.648395 Total_test_loss: 0.607814 Total_BCE_test_loss: 0.536187 Total_KLD_test_loss: 0.002348 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 14 total_train_loss: 0.649804 Total_test_loss: 0.608484 Total_BCE_test_loss: 0.536857 Total_KLD_test_loss: 0.002342 Total_CEP_test_loss: 0.069285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 15 total_train_loss: 0.648383 Total_test_loss: 0.607736 Total_BCE_test_loss: 0.536116 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 16 total_train_loss: 0.649860 Total_test_loss: 0.608563 Total_BCE_test_loss: 0.536931 Total_KLD_test_loss: 0.002350 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 17 total_train_loss: 0.650107 Total_test_loss: 0.608546 Total_BCE_test_loss: 0.536921 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 18 total_train_loss: 0.648802 Total_test_loss: 0.608038 Total_BCE_test_loss: 0.536404 Total_KLD_test_loss: 0.002366 Total_CEP_test_loss: 0.069268\n",
      "====> Epoch: 19 total_train_loss: 0.649215 Total_test_loss: 0.607555 Total_BCE_test_loss: 0.535923 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 20 total_train_loss: 0.654112 Total_test_loss: 0.608220 Total_BCE_test_loss: 0.536573 Total_KLD_test_loss: 0.002363 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 21 total_train_loss: 0.650008 Total_test_loss: 0.607466 Total_BCE_test_loss: 0.535839 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 22 total_train_loss: 0.649063 Total_test_loss: 0.607762 Total_BCE_test_loss: 0.536118 Total_KLD_test_loss: 0.002356 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 23 total_train_loss: 0.653885 Total_test_loss: 0.607629 Total_BCE_test_loss: 0.535977 Total_KLD_test_loss: 0.002375 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 24 total_train_loss: 0.651330 Total_test_loss: 0.607624 Total_BCE_test_loss: 0.536000 Total_KLD_test_loss: 0.002350 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 25 total_train_loss: 0.649280 Total_test_loss: 0.608033 Total_BCE_test_loss: 0.536417 Total_KLD_test_loss: 0.002343 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 26 total_train_loss: 0.649182 Total_test_loss: 0.607040 Total_BCE_test_loss: 0.535417 Total_KLD_test_loss: 0.002348 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 27 total_train_loss: 0.651882 Total_test_loss: 0.608017 Total_BCE_test_loss: 0.536397 Total_KLD_test_loss: 0.002338 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 28 total_train_loss: 0.653898 Total_test_loss: 0.608595 Total_BCE_test_loss: 0.536966 Total_KLD_test_loss: 0.002352 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 29 total_train_loss: 0.650483 Total_test_loss: 0.608041 Total_BCE_test_loss: 0.536424 Total_KLD_test_loss: 0.002334 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 30 total_train_loss: 0.655899 Total_test_loss: 0.607436 Total_BCE_test_loss: 0.535810 Total_KLD_test_loss: 0.002344 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 31 total_train_loss: 0.654534 Total_test_loss: 0.607472 Total_BCE_test_loss: 0.535833 Total_KLD_test_loss: 0.002355 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 32 total_train_loss: 0.646537 Total_test_loss: 0.607568 Total_BCE_test_loss: 0.535933 Total_KLD_test_loss: 0.002354 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 33 total_train_loss: 0.652325 Total_test_loss: 0.607058 Total_BCE_test_loss: 0.535428 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 34 total_train_loss: 0.651201 Total_test_loss: 0.607490 Total_BCE_test_loss: 0.535843 Total_KLD_test_loss: 0.002372 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 35 total_train_loss: 0.647872 Total_test_loss: 0.607167 Total_BCE_test_loss: 0.535513 Total_KLD_test_loss: 0.002377 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 36 total_train_loss: 0.652736 Total_test_loss: 0.608388 Total_BCE_test_loss: 0.536736 Total_KLD_test_loss: 0.002362 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 37 total_train_loss: 0.652294 Total_test_loss: 0.609093 Total_BCE_test_loss: 0.537465 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 38 total_train_loss: 0.653547 Total_test_loss: 0.608374 Total_BCE_test_loss: 0.536755 Total_KLD_test_loss: 0.002337 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 39 total_train_loss: 0.648229 Total_test_loss: 0.608276 Total_BCE_test_loss: 0.536666 Total_KLD_test_loss: 0.002331 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 40 total_train_loss: 0.652438 Total_test_loss: 0.608548 Total_BCE_test_loss: 0.536921 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 41 total_train_loss: 0.648654 Total_test_loss: 0.607112 Total_BCE_test_loss: 0.535484 Total_KLD_test_loss: 0.002355 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 42 total_train_loss: 0.649341 Total_test_loss: 0.608136 Total_BCE_test_loss: 0.536490 Total_KLD_test_loss: 0.002356 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 43 total_train_loss: 0.653302 Total_test_loss: 0.606812 Total_BCE_test_loss: 0.535188 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 44 total_train_loss: 0.652682 Total_test_loss: 0.608623 Total_BCE_test_loss: 0.537007 Total_KLD_test_loss: 0.002338 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 45 total_train_loss: 0.650203 Total_test_loss: 0.608330 Total_BCE_test_loss: 0.536710 Total_KLD_test_loss: 0.002330 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 46 total_train_loss: 0.650392 Total_test_loss: 0.607033 Total_BCE_test_loss: 0.535398 Total_KLD_test_loss: 0.002355 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 47 total_train_loss: 0.656710 Total_test_loss: 0.607025 Total_BCE_test_loss: 0.535367 Total_KLD_test_loss: 0.002387 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 48 total_train_loss: 0.651469 Total_test_loss: 0.606736 Total_BCE_test_loss: 0.535084 Total_KLD_test_loss: 0.002370 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 49 total_train_loss: 0.651524 Total_test_loss: 0.607513 Total_BCE_test_loss: 0.535858 Total_KLD_test_loss: 0.002368 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 50 total_train_loss: 0.655566 Total_test_loss: 0.607527 Total_BCE_test_loss: 0.535870 Total_KLD_test_loss: 0.002372 Total_CEP_test_loss: 0.069284\n",
      "5e-06\n",
      "====> Epoch: 1 total_train_loss: 0.647843 Total_test_loss: 0.608225 Total_BCE_test_loss: 0.536595 Total_KLD_test_loss: 0.002344 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 2 total_train_loss: 0.652418 Total_test_loss: 0.607375 Total_BCE_test_loss: 0.535745 Total_KLD_test_loss: 0.002354 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 3 total_train_loss: 0.648890 Total_test_loss: 0.607377 Total_BCE_test_loss: 0.535756 Total_KLD_test_loss: 0.002347 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 4 total_train_loss: 0.644327 Total_test_loss: 0.607319 Total_BCE_test_loss: 0.535684 Total_KLD_test_loss: 0.002354 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 5 total_train_loss: 0.650937 Total_test_loss: 0.608204 Total_BCE_test_loss: 0.536559 Total_KLD_test_loss: 0.002368 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 6 total_train_loss: 0.649874 Total_test_loss: 0.607380 Total_BCE_test_loss: 0.535730 Total_KLD_test_loss: 0.002372 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 7 total_train_loss: 0.646502 Total_test_loss: 0.607030 Total_BCE_test_loss: 0.535411 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 8 total_train_loss: 0.648835 Total_test_loss: 0.607114 Total_BCE_test_loss: 0.535483 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 9 total_train_loss: 0.651775 Total_test_loss: 0.607110 Total_BCE_test_loss: 0.535476 Total_KLD_test_loss: 0.002349 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 10 total_train_loss: 0.654713 Total_test_loss: 0.607386 Total_BCE_test_loss: 0.535772 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 11 total_train_loss: 0.648184 Total_test_loss: 0.607627 Total_BCE_test_loss: 0.536006 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 12 total_train_loss: 0.647916 Total_test_loss: 0.607196 Total_BCE_test_loss: 0.535589 Total_KLD_test_loss: 0.002335 Total_CEP_test_loss: 0.069272\n",
      "====> Epoch: 13 total_train_loss: 0.651590 Total_test_loss: 0.607488 Total_BCE_test_loss: 0.535873 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 14 total_train_loss: 0.651867 Total_test_loss: 0.606923 Total_BCE_test_loss: 0.535267 Total_KLD_test_loss: 0.002371 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 15 total_train_loss: 0.653595 Total_test_loss: 0.606907 Total_BCE_test_loss: 0.535257 Total_KLD_test_loss: 0.002379 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 16 total_train_loss: 0.650791 Total_test_loss: 0.607325 Total_BCE_test_loss: 0.535688 Total_KLD_test_loss: 0.002361 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 17 total_train_loss: 0.652512 Total_test_loss: 0.607469 Total_BCE_test_loss: 0.535837 Total_KLD_test_loss: 0.002360 Total_CEP_test_loss: 0.069272\n",
      "====> Epoch: 18 total_train_loss: 0.649280 Total_test_loss: 0.608664 Total_BCE_test_loss: 0.537026 Total_KLD_test_loss: 0.002357 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 19 total_train_loss: 0.649623 Total_test_loss: 0.606496 Total_BCE_test_loss: 0.534852 Total_KLD_test_loss: 0.002366 Total_CEP_test_loss: 0.069278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 total_train_loss: 0.649577 Total_test_loss: 0.607696 Total_BCE_test_loss: 0.536051 Total_KLD_test_loss: 0.002365 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 21 total_train_loss: 0.650578 Total_test_loss: 0.606819 Total_BCE_test_loss: 0.535183 Total_KLD_test_loss: 0.002362 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 22 total_train_loss: 0.651342 Total_test_loss: 0.607002 Total_BCE_test_loss: 0.535328 Total_KLD_test_loss: 0.002393 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 23 total_train_loss: 0.646897 Total_test_loss: 0.607285 Total_BCE_test_loss: 0.535634 Total_KLD_test_loss: 0.002360 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 24 total_train_loss: 0.656645 Total_test_loss: 0.606954 Total_BCE_test_loss: 0.535276 Total_KLD_test_loss: 0.002402 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 25 total_train_loss: 0.651018 Total_test_loss: 0.607275 Total_BCE_test_loss: 0.535617 Total_KLD_test_loss: 0.002369 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 26 total_train_loss: 0.655692 Total_test_loss: 0.606123 Total_BCE_test_loss: 0.534448 Total_KLD_test_loss: 0.002392 Total_CEP_test_loss: 0.069283\n",
      "====> Epoch: 27 total_train_loss: 0.648999 Total_test_loss: 0.607420 Total_BCE_test_loss: 0.535789 Total_KLD_test_loss: 0.002359 Total_CEP_test_loss: 0.069272\n",
      "====> Epoch: 28 total_train_loss: 0.648100 Total_test_loss: 0.606631 Total_BCE_test_loss: 0.534987 Total_KLD_test_loss: 0.002357 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 29 total_train_loss: 0.652202 Total_test_loss: 0.607319 Total_BCE_test_loss: 0.535677 Total_KLD_test_loss: 0.002365 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 30 total_train_loss: 0.654429 Total_test_loss: 0.607696 Total_BCE_test_loss: 0.536049 Total_KLD_test_loss: 0.002375 Total_CEP_test_loss: 0.069272\n",
      "====> Epoch: 31 total_train_loss: 0.648854 Total_test_loss: 0.607556 Total_BCE_test_loss: 0.535904 Total_KLD_test_loss: 0.002367 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 32 total_train_loss: 0.651354 Total_test_loss: 0.606778 Total_BCE_test_loss: 0.535129 Total_KLD_test_loss: 0.002371 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 33 total_train_loss: 0.653726 Total_test_loss: 0.606291 Total_BCE_test_loss: 0.534632 Total_KLD_test_loss: 0.002374 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 34 total_train_loss: 0.651276 Total_test_loss: 0.606818 Total_BCE_test_loss: 0.535160 Total_KLD_test_loss: 0.002387 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 35 total_train_loss: 0.649258 Total_test_loss: 0.607097 Total_BCE_test_loss: 0.535426 Total_KLD_test_loss: 0.002387 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 36 total_train_loss: 0.654239 Total_test_loss: 0.606798 Total_BCE_test_loss: 0.535129 Total_KLD_test_loss: 0.002382 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 37 total_train_loss: 0.650152 Total_test_loss: 0.607157 Total_BCE_test_loss: 0.535513 Total_KLD_test_loss: 0.002368 Total_CEP_test_loss: 0.069277\n",
      "====> Epoch: 38 total_train_loss: 0.651426 Total_test_loss: 0.608295 Total_BCE_test_loss: 0.536675 Total_KLD_test_loss: 0.002352 Total_CEP_test_loss: 0.069268\n",
      "====> Epoch: 39 total_train_loss: 0.652050 Total_test_loss: 0.607523 Total_BCE_test_loss: 0.535906 Total_KLD_test_loss: 0.002345 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 40 total_train_loss: 0.651187 Total_test_loss: 0.606931 Total_BCE_test_loss: 0.535276 Total_KLD_test_loss: 0.002374 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 41 total_train_loss: 0.653240 Total_test_loss: 0.607099 Total_BCE_test_loss: 0.535432 Total_KLD_test_loss: 0.002377 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 42 total_train_loss: 0.652554 Total_test_loss: 0.607533 Total_BCE_test_loss: 0.535854 Total_KLD_test_loss: 0.002393 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 43 total_train_loss: 0.648323 Total_test_loss: 0.607214 Total_BCE_test_loss: 0.535540 Total_KLD_test_loss: 0.002391 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 44 total_train_loss: 0.652708 Total_test_loss: 0.607381 Total_BCE_test_loss: 0.535719 Total_KLD_test_loss: 0.002393 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 45 total_train_loss: 0.650189 Total_test_loss: 0.606578 Total_BCE_test_loss: 0.534899 Total_KLD_test_loss: 0.002403 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 46 total_train_loss: 0.647983 Total_test_loss: 0.607605 Total_BCE_test_loss: 0.535948 Total_KLD_test_loss: 0.002379 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 47 total_train_loss: 0.652735 Total_test_loss: 0.607480 Total_BCE_test_loss: 0.535824 Total_KLD_test_loss: 0.002380 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 48 total_train_loss: 0.646811 Total_test_loss: 0.607781 Total_BCE_test_loss: 0.536115 Total_KLD_test_loss: 0.002393 Total_CEP_test_loss: 0.069273\n",
      "====> Epoch: 49 total_train_loss: 0.652938 Total_test_loss: 0.607468 Total_BCE_test_loss: 0.535837 Total_KLD_test_loss: 0.002360 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 50 total_train_loss: 0.649148 Total_test_loss: 0.607349 Total_BCE_test_loss: 0.535703 Total_KLD_test_loss: 0.002366 Total_CEP_test_loss: 0.069280\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "if model_tobe_trained:\n",
    "    lr=1e-2\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=100,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=200,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=5e-4\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "\n",
    "    lr=1e-5\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    lr=5e-6\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5tsLzrBNa7E"
   },
   "source": [
    "# Save The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42YFEvnqbE9U",
    "outputId": "555826a8-f613-4331-b6a5-1acc5a1b82a8"
   },
   "outputs": [],
   "source": [
    "##########\n",
    "print(\"running the neural network\")\n",
    "#run(obj1,save_address)\n",
    "obj1.model_save(address=save_address+\".pt\")\n",
    "obj1.save_residuals(address=save_address+'_residuals.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUnPPyZ6NfDr"
   },
   "source": [
    "# Visualize Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DStavVSXYRs5",
    "outputId": "047b7d45-2f98-4854-9ea2-17f3781d9842"
   },
   "outputs": [],
   "source": [
    "obj1.plot_residuals(init_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkjkH6AjN5pq"
   },
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBW6HgBINu9v"
   },
   "outputs": [],
   "source": [
    "from ci_vae import ivae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import umap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqNZPHbPOkoh",
    "outputId": "35686f32-9d60-4ae3-b7b6-a634658bb1b3"
   },
   "outputs": [],
   "source": [
    "print(\"start of the code\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##############################################################   \n",
    "##############################################################\n",
    "model_init=True\n",
    "model_tobe_trained=False\n",
    "\n",
    "model_init=True\n",
    "model_file_address='./bb.pt'\n",
    "save_address1=\"./\"\n",
    "\n",
    "df_XY=pd.read_csv('df_XY.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oDqQbsnOO6d"
   },
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHFjOqpnigK_"
   },
   "outputs": [],
   "source": [
    "obj1 = ivae.IVAE(df_XY = df_XY,\n",
    "               reconst_coef = reconst_coef,\n",
    "               latent_size = 10,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "obj1.model_initialiaze()\n",
    "\n",
    "obj1.model_load(address=\"bb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOUIzmGTOTyG"
   },
   "source": [
    "## Print the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-H3bybRp484",
    "outputId": "fce40859-ea30-4f75-b4b5-08284301f7cf"
   },
   "outputs": [],
   "source": [
    "for param in obj1.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi3yqTDIOoui"
   },
   "source": [
    "# Make Prediction of All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHcer1BPikHd",
    "outputId": "f72b5c62-5fd7-438d-818f-47f5dddaf3a6"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obj1.model.eval()\n",
    "\n",
    "    #obj1.load_residuals(address='bb_residuals.pkl')\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    obj1.generate_test_results()\n",
    "    print(\"test data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaKllEltPf16"
   },
   "source": [
    "# Comprehensive Checking of The Prediction Values vs. True Values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK8l95VcvpJt",
    "outputId": "470c82e6-2b9a-4903-eaa1-732e46bd84c1"
   },
   "outputs": [],
   "source": [
    "print(obj1.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9ZitDT26ZXW",
    "outputId": "de963c21-10fd-4515-b6b5-b9a06eb3bd46"
   },
   "outputs": [],
   "source": [
    "print(obj1.x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ENQ8rtRHInw",
    "outputId": "959af6be-4ab0-4150-e843-b0d2297efc30"
   },
   "outputs": [],
   "source": [
    "(np.abs(obj1.x_pred - obj1.x_last)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJJfodLhQmrD",
    "outputId": "1bc5cb45-60ba-4bc1-c812-fd51c64c41ef"
   },
   "outputs": [],
   "source": [
    "(obj1.x_pred-obj1.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3q3XVFEvsEQ",
    "outputId": "4add0237-b89d-4803-983f-2d35cad6599d"
   },
   "outputs": [],
   "source": [
    "print(obj1.y_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opj3sBJX6cQH",
    "outputId": "daedae9e-1edb-4243-efa3-e54976aaee6c"
   },
   "outputs": [],
   "source": [
    "print(obj1.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "N-n9_o8nQxBa",
    "outputId": "08c51321-bbdc-4302-8c5d-c97c8700b590"
   },
   "outputs": [],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2Yzu9Eiis50",
    "outputId": "d909e107-e05b-41a0-f075-c48541287153"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obj1.model.eval()\n",
    "    for x, y in obj1.testloader:\n",
    "      x = x.to(device)\n",
    "      print(x.size())\n",
    "      print(x)\n",
    "      # forward\n",
    "      x_hat,y_hat, mu, logvar,z = obj1.model(x)\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(x_hat.cpu().detach().numpy(), columns=obj1.df_XY.drop(columns=['Y']).columns)\n",
    "    print(df_reconstructed.shape)\n",
    "    df_latent=pd.DataFrame(z.cpu().detach().numpy())\n",
    "    \n",
    "    obj1.model.eval()\n",
    "    \n",
    "    df_reconstructed_decoder=pd.DataFrame(obj1.model.decoder(z).cpu().detach().numpy(), columns=obj1.df_XY.drop(columns=['Y']).columns)\n",
    "\n",
    "    df_reconstructed.to_csv('df_reconstructed.csv')\n",
    "    df_latent.to_csv('df_latent.csv')\n",
    "    df_reconstructed_decoder.to_csv('df_reconstructed_decoder.csv')\n",
    "    print(\"Full_data_reconstructed...\")\n",
    "    \n",
    "    print(\"========df_reconstructed========\")\n",
    "    print(df_reconstructed)\n",
    "    print(\"========df_reconstructed_decoder========\")\n",
    "    print(df_reconstructed_decoder)\n",
    "    print(\"========df_Original========\")\n",
    "    print(df_XY)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CciNZOW_Rc0n"
   },
   "source": [
    "# Checking Linear Separability of Data on Lower Dimensioanl Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHqfONTOlqSr",
    "outputId": "6a2fa0ee-b52d-49dd-f55a-f6cdae3ee20e"
   },
   "outputs": [],
   "source": [
    "print(\"regression analysis\")\n",
    "obj1.regression_analysis(obj1.zs,df_XY['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBQlR5KERlL-"
   },
   "source": [
    "# Visualize Data on Lower Dimensional Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SXZtQfoj93-"
   },
   "outputs": [],
   "source": [
    "print(\"calculate tsne_umap_pca\")\n",
    "tsne_mat,umap_mat,pca_mat,Y=obj1.calculate_lower_dimensions(obj1.zs,obj1.y_last,N=100)\n",
    "obj1.plot_lower_dimension(tsne_mat,Y,projection='3d',save_str='tsne3d.pdf')\n",
    "obj1.plot_lower_dimension(tsne_mat,Y,projection='2d',save_str='tsne2d.pdf')\n",
    "obj1.plot_lower_dimension(umap_mat,Y,projection='3d',save_str='umap3d.pdf')\n",
    "obj1.plot_lower_dimension(umap_mat,Y,projection='2d',save_str='umap2d.pdf')\n",
    "obj1.plot_lower_dimension(pca_mat,Y,projection='3d',save_str='pca3d.pdf')\n",
    "obj1.plot_lower_dimension(pca_mat,Y,projection='2d',save_str='pca2d.pdf')\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ULT6UtRxU6"
   },
   "source": [
    "# Perform Interpolation across all groups (Y) and all features from YY=0 to YY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dXtgbd1iJ0s",
    "outputId": "4710be7c-960f-4184-bdb5-931a5699dd5a"
   },
   "outputs": [],
   "source": [
    "ff = obj1.traversal_all_groups(traversal_step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyAzr8KSAgH"
   },
   "source": [
    "# See the interpolation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWzpnwK0g-8g"
   },
   "outputs": [],
   "source": [
    "with open('results_dict.pkl', 'rb') as f:\n",
    "    ff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "p23aMsKEknL4",
    "outputId": "29141142-bd3d-4f66-c936-9479312bcba9"
   },
   "outputs": [],
   "source": [
    "ff['med']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hMMGIGAslEoz",
    "outputId": "52e9d86d-dcb8-46c1-dfbf-1866da50861b"
   },
   "outputs": [],
   "source": [
    "ff['mean']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "T7sydB3ISVvE",
    "outputId": "36818219-d221-43b4-aa92-fb45eb31fb7a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qHpnnAoxoy_r",
    "outputId": "cc5504d0-001b-4b5d-afc1-d01f408db930"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['mean']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "kR4GHdxyqCxn",
    "outputId": "f9387ed5-758f-41db-bd4a-9043c31ccb4a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['0']['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "l92UJyGZSMYB",
    "outputId": "5632470a-2575-41d5-9877-1335947b8d45"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ff['mean']['0']['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHBvdSESFRV"
   },
   "source": [
    "# Generate Synthetic Data for a Given Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ_3NAgi2-6L",
    "outputId": "a11c8fd5-41de-40d3-dee2-91580cb322a9"
   },
   "outputs": [],
   "source": [
    "bb = obj1.synthetic_single_group(group_id=0,nr_of_synthetic=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E330szfA6BD7",
    "outputId": "a8687ace-e5b6-46d2-904d-e854e532d0d8"
   },
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey_yS1WZ3Os2",
    "outputId": "dbeb2f28-2746-4a69-beeb-a934f19eb8a1"
   },
   "outputs": [],
   "source": [
    "bb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
