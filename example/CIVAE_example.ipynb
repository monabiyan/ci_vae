{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhV9PPiQKgSg"
   },
   "source": [
    "# Generate Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "9HIiKcw_PCm5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "np.random.seed(0)\n",
    "n_samples=1000\n",
    "n_features = 5\n",
    "df_XY=pd.DataFrame(data = np.random.normal(0,1, size=(n_samples, n_features)), columns = ['A','B','C','D','E'])\n",
    "df_XY['Y']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY['YY']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY\n",
    "\n",
    "##############################################################   \n",
    "df_XY.shape\n",
    "df_XY.head()\n",
    "df_XY.to_csv('df_XY.csv',index=False)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "31zCNsOv0BoE",
    "outputId": "b7af009e-8ad8-463e-805d-07a8d5e7e082"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>Y</th>\n",
       "      <th>YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.764052</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>2.240893</td>\n",
       "      <td>1.867558</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.977278</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.103219</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144044</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333674</td>\n",
       "      <td>1.494079</td>\n",
       "      <td>-0.205158</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>-0.854096</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.552990</td>\n",
       "      <td>0.653619</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>-0.742165</td>\n",
       "      <td>2.269755</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.711489</td>\n",
       "      <td>-1.820816</td>\n",
       "      <td>0.163495</td>\n",
       "      <td>-0.813117</td>\n",
       "      <td>-0.605355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.327524</td>\n",
       "      <td>-0.644172</td>\n",
       "      <td>1.908883</td>\n",
       "      <td>-0.563545</td>\n",
       "      <td>1.082473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-1.951911</td>\n",
       "      <td>2.441216</td>\n",
       "      <td>-0.017285</td>\n",
       "      <td>0.912282</td>\n",
       "      <td>1.239658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.573367</td>\n",
       "      <td>0.424889</td>\n",
       "      <td>-0.271260</td>\n",
       "      <td>-0.683568</td>\n",
       "      <td>-1.537438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.101374</td>\n",
       "      <td>0.746666</td>\n",
       "      <td>0.929182</td>\n",
       "      <td>0.229418</td>\n",
       "      <td>0.414406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         B         C         D         E  Y  YY\n",
       "0    1.764052  0.400157  0.978738  2.240893  1.867558  1   1\n",
       "1   -0.977278  0.950088 -0.151357 -0.103219  0.410599  0   0\n",
       "2    0.144044  1.454274  0.761038  0.121675  0.443863  0   0\n",
       "3    0.333674  1.494079 -0.205158  0.313068 -0.854096  1   0\n",
       "4   -2.552990  0.653619  0.864436 -0.742165  2.269755  0   1\n",
       "..        ...       ...       ...       ...       ... ..  ..\n",
       "995  1.711489 -1.820816  0.163495 -0.813117 -0.605355  0   0\n",
       "996 -1.327524 -0.644172  1.908883 -0.563545  1.082473  1   0\n",
       "997 -1.951911  2.441216 -0.017285  0.912282  1.239658  1   1\n",
       "998 -0.573367  0.424889 -0.271260 -0.683568 -1.537438  1   1\n",
       "999 -0.101374  0.746666  0.929182  0.229418  0.414406  0   1\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77NKQyeKwIj"
   },
   "source": [
    "# Download CI-VAE, other necessary packages and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "cAuVLcUNETr7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: df_reconstructed.csv: No such file or directory\n",
      "rm: df_reconstructed_decoder.csv: No such file or directory\n",
      "rm: results_dict.pkl: No such file or directory\n",
      "rm: df_latent.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ci_vae\n",
    "! rm bb.pt\n",
    "! rm bb_residuals.pkl\n",
    "! rm df_reconstructed.csv\n",
    "! rm df_reconstructed_decoder.csv\n",
    "! rm residuals.pdf\n",
    "! rm results_dict.pkl\n",
    "! rm df_latent.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "18S0saDPLp0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ci_vae'...\n",
      "remote: Enumerating objects: 334, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
      "remote: Total 334 (delta 48), reused 65 (delta 24), pack-reused 245\u001b[K\n",
      "Receiving objects: 100% (334/334), 46.88 MiB | 5.06 MiB/s, done.\n",
      "Resolving deltas: 100% (204/204), done.\n",
      "Requirement already satisfied: umap-learn in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.51.2)\n",
      "Requirement already satisfied: tqdm in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (4.50.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.23.2)\n",
      "Requirement already satisfied: setuptools in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (50.3.1.post20201107)\n",
      "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/monabiyan/ci_vae.git\n",
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kzwk1I17VAQx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ci_vae import ivae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPVcV9thL8SP"
   },
   "source": [
    "# Set Necessary Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KAVb-irtbIpc"
   },
   "outputs": [],
   "source": [
    "model_init=True\n",
    "model_tobe_trained=True\n",
    "save_address=\"bb\"\n",
    "\n",
    "kl_coef = 0.0001\n",
    "reconst_coef = 1\n",
    "classifier_coef = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRIfHjpSMKF5"
   },
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ll0w2DunMJei"
   },
   "outputs": [],
   "source": [
    "obj1 = ivae.IVAE(df_XY = df_XY,\n",
    "               latent_size = 2,\n",
    "               reconst_coef = reconst_coef,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "if model_init:\n",
    "    obj1.model_initialiaze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT-AB_8-M-tD"
   },
   "source": [
    "## See The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJXlsM8Uk2Ry",
    "outputId": "129babee-fc0e-48c5-9262-08404a9b89c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVAE_ARCH(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=20, out_features=5, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Dropout(p=0.8, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSj9WT_mNHNl"
   },
   "source": [
    "## See the Initialized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUFyVcu4ldFH",
    "outputId": "5a470060-626d-4602-dcf4-78d601febd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.2200, -0.0311, -0.3187,  0.0512,  0.0037],\n",
      "        [-0.2980, -0.0701, -0.2644, -0.3377,  0.0365],\n",
      "        [-0.1359,  0.0343, -0.2789,  0.3185,  0.2952],\n",
      "        [ 0.4085,  0.0436,  0.2929,  0.2545, -0.0765],\n",
      "        [-0.2045, -0.0104, -0.2330, -0.3084,  0.3366],\n",
      "        [-0.3150,  0.1827, -0.2409, -0.3672, -0.3048],\n",
      "        [-0.0883,  0.1794,  0.3068, -0.4053,  0.2801],\n",
      "        [ 0.0562,  0.0938,  0.0022, -0.2357,  0.3789],\n",
      "        [-0.0689,  0.1914,  0.4211, -0.1635,  0.0329],\n",
      "        [-0.0394,  0.1990, -0.3799,  0.3463,  0.3213],\n",
      "        [-0.0227, -0.3543, -0.3431,  0.0570,  0.4155],\n",
      "        [-0.3112, -0.2825,  0.3977,  0.0452, -0.3098],\n",
      "        [ 0.1274, -0.2553, -0.4094,  0.1679, -0.0977],\n",
      "        [-0.0963, -0.3973, -0.1100, -0.1181,  0.0633],\n",
      "        [-0.2314,  0.2761,  0.1616,  0.4420,  0.4101],\n",
      "        [-0.2054,  0.3315,  0.3340,  0.3047, -0.1031],\n",
      "        [ 0.1801, -0.3202, -0.0540, -0.3175,  0.4462],\n",
      "        [ 0.0466,  0.0127,  0.1794,  0.0762, -0.3760],\n",
      "        [ 0.0084,  0.1458,  0.1671,  0.3885,  0.3414],\n",
      "        [ 0.3529, -0.1453, -0.0220, -0.3753,  0.3313]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0562,  0.3362,  0.4400,  0.4165,  0.0849,  0.0259,  0.2030,  0.1403,\n",
      "        -0.0800,  0.0451,  0.1053, -0.2984,  0.1778,  0.3729, -0.3577, -0.2115,\n",
      "        -0.2326,  0.3363,  0.2531,  0.2502], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1699,  0.2197, -0.1934,  0.2182,  0.0451, -0.1267, -0.1864,  0.0387,\n",
      "         -0.0777, -0.2079,  0.1729, -0.0103,  0.1152,  0.1749, -0.0509,  0.1632,\n",
      "         -0.1401,  0.1165,  0.0341,  0.1364],\n",
      "        [-0.0552, -0.0765,  0.1574,  0.0848, -0.0563, -0.0977, -0.0811, -0.1487,\n",
      "         -0.1054, -0.2128,  0.0067,  0.2086, -0.2232,  0.1715,  0.0702, -0.1785,\n",
      "          0.0430,  0.0998,  0.0031,  0.1168],\n",
      "        [-0.2002, -0.0252,  0.2232, -0.1958,  0.2218, -0.2222,  0.2129, -0.2190,\n",
      "          0.2168, -0.0227, -0.0241, -0.2083, -0.0935, -0.1767, -0.0160,  0.1524,\n",
      "          0.1451, -0.0447,  0.0116,  0.0438],\n",
      "        [ 0.1923,  0.1865,  0.2164, -0.0179,  0.1992, -0.0005, -0.2113,  0.0803,\n",
      "         -0.0108,  0.0340,  0.0636,  0.2087,  0.1819, -0.0651, -0.1980, -0.1989,\n",
      "         -0.2060,  0.0111, -0.0615,  0.1519],\n",
      "        [-0.1369, -0.0126,  0.0730, -0.0540, -0.1943, -0.1332,  0.2135, -0.2160,\n",
      "         -0.1961, -0.1337,  0.1394, -0.1371,  0.2222,  0.2030, -0.2041, -0.0567,\n",
      "          0.0913,  0.0233,  0.0998,  0.2021],\n",
      "        [ 0.0047,  0.2199,  0.2105, -0.1199,  0.0448,  0.0330,  0.2115, -0.1150,\n",
      "          0.2200, -0.0341, -0.0752,  0.1524, -0.1070, -0.0777, -0.1150, -0.1592,\n",
      "          0.2132, -0.0187,  0.1303,  0.0153],\n",
      "        [-0.1861,  0.1632, -0.2113, -0.2113, -0.1169,  0.1594, -0.1517, -0.1974,\n",
      "         -0.0657,  0.1011, -0.0404, -0.0171,  0.1780, -0.0402, -0.0126,  0.0213,\n",
      "          0.0297,  0.1293, -0.0774, -0.1328],\n",
      "        [-0.1198,  0.0339,  0.0455,  0.1257, -0.0656, -0.1982,  0.0974, -0.2153,\n",
      "          0.1671, -0.1714, -0.0839, -0.0016, -0.1676, -0.1247, -0.2128, -0.1357,\n",
      "          0.1205,  0.0967,  0.1095, -0.0914],\n",
      "        [-0.1849,  0.1867,  0.2084, -0.0738, -0.0739, -0.0362,  0.1178,  0.1969,\n",
      "          0.0200, -0.1686,  0.0504, -0.0404,  0.0954,  0.0013,  0.2108,  0.1077,\n",
      "          0.1644,  0.1346,  0.1309, -0.1700],\n",
      "        [ 0.0703, -0.0366, -0.1756, -0.0716, -0.1040,  0.1669,  0.1779, -0.1840,\n",
      "          0.2149,  0.1368,  0.1475,  0.0780,  0.1516,  0.2038,  0.0326, -0.1818,\n",
      "         -0.1581, -0.0918, -0.1458,  0.0693],\n",
      "        [ 0.0180, -0.1741,  0.0289, -0.2154, -0.0767,  0.0137, -0.0394, -0.2181,\n",
      "          0.0407,  0.0775, -0.0349,  0.0020,  0.0382,  0.1477,  0.1438, -0.1313,\n",
      "          0.1014, -0.0316,  0.0141, -0.2158],\n",
      "        [ 0.1809, -0.2036,  0.0253,  0.0648,  0.1000,  0.2102, -0.1341, -0.0169,\n",
      "         -0.1319, -0.0392,  0.1593,  0.1747,  0.0721, -0.1561, -0.1469, -0.0114,\n",
      "          0.0117, -0.0628, -0.1981, -0.1618],\n",
      "        [ 0.1111,  0.1253,  0.0718, -0.0104,  0.1303,  0.0065,  0.0574,  0.2195,\n",
      "         -0.0210, -0.2236, -0.1183, -0.0256,  0.1866, -0.0621,  0.1647,  0.0442,\n",
      "         -0.0668, -0.1259, -0.2173,  0.2031],\n",
      "        [-0.1864,  0.0600,  0.1195,  0.0256, -0.0525, -0.1631, -0.1146,  0.0257,\n",
      "          0.1714, -0.1971,  0.0103, -0.0332, -0.2024, -0.1564, -0.1620,  0.1426,\n",
      "          0.1159,  0.0891,  0.0076,  0.0135],\n",
      "        [-0.2004,  0.0464,  0.0651,  0.1131, -0.1955, -0.0483, -0.0305,  0.0348,\n",
      "          0.0012, -0.1316,  0.0478, -0.0350,  0.0632, -0.0201,  0.1904,  0.0302,\n",
      "          0.1032,  0.0783,  0.1075, -0.0261],\n",
      "        [ 0.0102,  0.0813,  0.0563,  0.0458,  0.0625, -0.1754,  0.0883,  0.1340,\n",
      "         -0.1200,  0.0781,  0.1133, -0.1161,  0.1727, -0.1745,  0.0199,  0.0148,\n",
      "         -0.0199,  0.0376,  0.1887,  0.1788],\n",
      "        [ 0.1663, -0.2217, -0.0217,  0.0601,  0.1849,  0.1999, -0.0898,  0.0282,\n",
      "         -0.1901,  0.0665, -0.1537, -0.0862,  0.0096,  0.1976, -0.0329, -0.1309,\n",
      "          0.1689,  0.2188, -0.0933, -0.1893],\n",
      "        [ 0.1371,  0.0147,  0.1474,  0.0775,  0.1932, -0.2185,  0.0632, -0.1981,\n",
      "          0.0763,  0.0161, -0.0791,  0.1454, -0.1667,  0.1790, -0.1093, -0.2110,\n",
      "          0.0611,  0.1772,  0.1858,  0.1040],\n",
      "        [ 0.0262,  0.1150, -0.2206, -0.0705, -0.0411, -0.0823, -0.1981, -0.1509,\n",
      "          0.1588, -0.0508, -0.0996,  0.2193, -0.1421, -0.0654, -0.1578,  0.1428,\n",
      "         -0.1024, -0.1832, -0.1521,  0.0071],\n",
      "        [-0.2229,  0.0293, -0.2193, -0.1259,  0.1996,  0.0311, -0.1024, -0.2110,\n",
      "         -0.1344,  0.1123, -0.1150, -0.0964, -0.0772, -0.1786,  0.2201,  0.1257,\n",
      "         -0.1821, -0.1580, -0.1830, -0.1950]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1234,  0.1519, -0.1950,  0.1354, -0.0443, -0.0453,  0.1916,  0.1053,\n",
      "         0.1340, -0.2026, -0.0783,  0.1425, -0.2204,  0.1700, -0.0720, -0.1283,\n",
      "        -0.0728, -0.0334,  0.1229, -0.2060], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.0964e-01, -9.9704e-02,  1.8335e-01,  6.2093e-02,  1.2057e-01,\n",
      "         -6.0770e-02, -5.2804e-02, -1.1301e-01, -1.2368e-01, -1.2627e-02,\n",
      "          2.2272e-01,  1.4460e-01, -1.6681e-01,  2.0505e-01, -1.2293e-01,\n",
      "         -2.1723e-01,  8.5144e-02,  7.6102e-02,  5.8118e-02, -1.0132e-01],\n",
      "        [ 1.8663e-01, -1.1777e-01, -1.2917e-01, -2.3515e-02, -1.2085e-01,\n",
      "         -1.3597e-01,  7.1962e-02, -7.6843e-02,  2.1119e-01, -2.6901e-02,\n",
      "         -1.2389e-01,  2.1034e-01, -7.5187e-02, -2.1744e-01,  1.2690e-01,\n",
      "         -1.8455e-01, -2.2080e-01,  2.2854e-02, -7.7864e-02, -1.3418e-01],\n",
      "        [-1.0152e-02,  5.6730e-02, -9.8822e-04,  1.6125e-01,  6.7444e-03,\n",
      "          2.0630e-01,  1.7167e-01, -5.9765e-02, -5.2365e-02,  1.2894e-01,\n",
      "          1.2712e-01, -1.1184e-02, -3.9376e-02,  6.4076e-02, -1.8125e-01,\n",
      "          7.8854e-03,  2.0610e-04, -4.0561e-02, -1.3407e-01, -5.9741e-02],\n",
      "        [ 1.6028e-01, -1.5565e-01, -1.3537e-01, -1.2592e-01,  3.3821e-02,\n",
      "          8.9058e-02,  1.9910e-01, -2.0483e-01, -7.0040e-02,  8.7301e-03,\n",
      "          1.4167e-01,  5.4615e-02, -1.7091e-01,  1.8823e-02, -1.6925e-01,\n",
      "          3.4433e-02,  1.2524e-01, -8.9404e-02,  1.9670e-01, -2.1994e-01],\n",
      "        [ 2.0445e-01,  1.1365e-02,  7.9496e-02, -1.5661e-01,  1.5484e-01,\n",
      "          4.9733e-02,  2.1850e-01,  3.6733e-02, -6.0214e-02,  2.3570e-02,\n",
      "         -1.5379e-01,  2.0658e-01, -3.8211e-02,  1.9048e-02, -1.9066e-01,\n",
      "          6.9347e-02, -1.7627e-01, -9.2883e-02,  1.7227e-01,  1.7539e-01],\n",
      "        [ 7.0713e-02,  1.2806e-01,  1.9649e-01,  5.2887e-02, -7.9122e-02,\n",
      "          3.3848e-02,  1.7405e-01, -1.7578e-01,  2.1523e-01, -1.9919e-01,\n",
      "          1.8159e-01,  6.5150e-02,  1.4692e-01, -1.4175e-01,  1.4055e-02,\n",
      "         -9.7378e-02,  1.8499e-01,  1.7982e-01,  1.5254e-01,  1.9618e-01],\n",
      "        [ 4.8474e-02, -1.7546e-01,  1.4711e-02, -9.2745e-02, -1.6933e-01,\n",
      "          1.1952e-02,  1.0287e-01,  1.4213e-02,  1.6040e-01,  4.9569e-02,\n",
      "          9.5995e-02,  1.2791e-01,  1.2857e-01,  7.5314e-02, -3.3240e-02,\n",
      "         -2.0556e-02, -1.3025e-02,  1.5808e-01,  6.6196e-02,  1.6452e-01],\n",
      "        [ 1.5382e-01, -1.3680e-01, -6.3732e-02,  2.3218e-02,  1.4967e-01,\n",
      "         -1.5789e-01,  2.2240e-01, -1.4995e-01, -2.0518e-01,  5.7290e-02,\n",
      "          7.4438e-03, -8.3991e-02, -1.2474e-01,  1.6647e-01, -1.7656e-01,\n",
      "         -3.0463e-02, -1.3267e-01,  2.1362e-01,  7.5402e-02, -3.7041e-02],\n",
      "        [ 1.0138e-01, -6.2835e-02,  1.7627e-01, -5.7331e-02, -1.9657e-01,\n",
      "         -2.0547e-01,  1.7669e-01,  1.3539e-01, -1.0713e-01, -1.1973e-01,\n",
      "          2.0908e-02,  8.8434e-02, -1.1483e-01, -1.3526e-01, -1.0322e-01,\n",
      "         -1.4319e-01, -1.6524e-01, -1.8961e-01,  9.6836e-02,  1.6752e-01],\n",
      "        [ 1.7962e-01,  5.7932e-02, -5.2530e-02,  5.7380e-02,  6.8102e-03,\n",
      "          6.0822e-02,  1.6464e-01,  8.8658e-02, -4.5748e-02,  6.3138e-02,\n",
      "          1.2497e-01, -1.5970e-01, -1.6144e-01,  7.8475e-02, -5.5433e-02,\n",
      "         -1.0012e-01,  1.9481e-01,  1.9273e-01, -1.7653e-02,  1.2184e-01],\n",
      "        [ 1.9057e-01,  1.1626e-01, -5.2133e-02,  1.7433e-01, -2.3690e-02,\n",
      "         -1.5567e-01, -1.0357e-01,  2.1284e-01,  1.6928e-01, -6.7187e-02,\n",
      "          1.9342e-01,  7.2474e-02,  9.9209e-02,  2.1824e-01, -9.4044e-02,\n",
      "          2.5063e-03,  1.0722e-01, -6.5562e-03,  1.9095e-01, -1.3088e-01],\n",
      "        [-2.0377e-01,  2.9676e-02,  1.6928e-01,  1.5142e-01, -1.9096e-01,\n",
      "         -3.0117e-02, -1.5441e-01,  1.6666e-01, -8.6647e-02,  1.7322e-01,\n",
      "         -1.0485e-01,  1.1937e-01,  1.3524e-01,  1.9146e-01, -3.1778e-02,\n",
      "          1.4700e-01, -6.8488e-02, -2.2770e-02,  1.4473e-01, -2.9194e-02],\n",
      "        [ 1.4731e-01,  2.0101e-01,  1.3865e-02,  1.8699e-01, -1.1813e-01,\n",
      "         -2.5728e-02, -5.1391e-02, -9.8540e-02, -3.6041e-02, -3.2393e-02,\n",
      "          1.0281e-02,  1.7676e-01,  3.0421e-02,  1.5473e-01,  1.7827e-01,\n",
      "         -1.4852e-01,  1.3418e-01, -2.1385e-01,  2.0264e-01, -1.6931e-01],\n",
      "        [-5.9459e-02,  1.9856e-01, -6.9267e-02,  4.3409e-02, -7.6728e-02,\n",
      "          1.2664e-01, -1.1795e-01,  1.3241e-01, -1.9269e-01, -1.3428e-01,\n",
      "          2.0182e-01,  9.4701e-02, -2.2276e-01, -6.1186e-02, -3.0807e-02,\n",
      "          1.6060e-01, -2.0306e-01,  1.0383e-01,  3.9381e-02,  1.8396e-01],\n",
      "        [-1.9322e-01, -7.9670e-02, -1.0709e-01,  1.6822e-01,  2.1078e-01,\n",
      "          4.1039e-03, -1.5668e-01, -1.0020e-01,  5.6989e-02, -2.1533e-01,\n",
      "          5.7626e-03,  9.0392e-02, -3.4654e-02,  1.9502e-01, -1.6254e-01,\n",
      "          1.7259e-02,  1.8109e-01,  1.0827e-01,  1.4878e-01,  8.6136e-02],\n",
      "        [ 3.3106e-02,  2.2087e-01,  2.1303e-01, -7.3146e-02,  6.2384e-02,\n",
      "         -9.4015e-03, -2.0739e-01,  1.7459e-01, -9.3865e-02, -2.0247e-01,\n",
      "          1.0564e-01, -2.0855e-03,  9.6437e-02, -1.6118e-01, -2.1920e-01,\n",
      "          1.1527e-01,  1.7692e-01,  1.3753e-01, -1.9866e-01, -1.5030e-01],\n",
      "        [-1.9897e-01,  1.3348e-01,  1.4223e-01, -8.4900e-02, -3.9599e-02,\n",
      "         -2.0498e-01, -2.1988e-02,  1.3444e-01, -9.9493e-02, -1.0123e-01,\n",
      "         -8.8235e-02, -1.1410e-01,  1.3854e-01,  1.7388e-02,  8.5947e-02,\n",
      "          7.8747e-03, -1.3552e-01, -7.0086e-02, -1.5250e-01, -1.0462e-01],\n",
      "        [-1.1205e-01, -1.4151e-01, -1.0656e-01, -1.8707e-01, -9.0505e-02,\n",
      "          6.0617e-02, -1.6309e-02, -6.3475e-02, -1.5573e-01,  1.7681e-01,\n",
      "          1.4888e-01, -9.4818e-02,  1.0601e-02, -1.7939e-01, -1.4206e-01,\n",
      "         -1.0813e-02, -9.6004e-02, -1.4053e-01,  1.3726e-01,  1.8632e-01],\n",
      "        [ 5.7797e-03, -7.0342e-02,  1.5610e-01,  1.6872e-01,  1.3832e-01,\n",
      "         -6.3978e-02,  1.9407e-01, -5.8997e-03, -1.7374e-01,  2.2993e-02,\n",
      "         -6.8249e-02, -1.1180e-02,  2.7041e-02,  1.7501e-01, -8.8682e-02,\n",
      "         -8.2478e-02, -4.6037e-02,  1.5213e-01, -9.5734e-03, -1.6916e-01],\n",
      "        [ 1.6881e-01,  1.9244e-01, -2.1229e-01, -5.0880e-02, -1.0383e-01,\n",
      "         -2.0532e-01, -1.1805e-01,  1.9595e-01,  2.0602e-01,  4.7693e-02,\n",
      "         -1.7515e-01,  1.6420e-01,  6.4934e-02, -4.4630e-02,  1.4859e-01,\n",
      "          1.0277e-01, -7.6739e-02, -1.7057e-01,  6.2445e-02, -7.3163e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0377,  0.0685,  0.0681, -0.1509, -0.0575, -0.1604, -0.0224, -0.0639,\n",
      "         0.1113, -0.2153,  0.1638, -0.1576,  0.1918, -0.1890, -0.0553,  0.1495,\n",
      "        -0.2027, -0.1072, -0.0908, -0.1304], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.8112e-01,  1.4254e-01, -1.4660e-01,  5.4479e-02,  3.5253e-02,\n",
      "          2.1481e-02,  1.1361e-01, -1.9586e-01,  9.7973e-02,  3.6983e-02,\n",
      "          1.4158e-02, -1.1069e-01, -7.7064e-02,  7.3585e-02,  2.2105e-01,\n",
      "          2.1518e-01, -1.2422e-01,  1.3611e-01,  8.5258e-02,  1.8301e-01],\n",
      "        [ 1.0829e-01,  1.9514e-01, -2.0655e-01,  7.9170e-02,  2.9564e-02,\n",
      "          3.1894e-02, -8.9587e-02, -6.6424e-02, -2.0586e-01,  1.3180e-01,\n",
      "         -3.6264e-02,  1.5234e-01,  1.6833e-01,  1.3680e-01, -6.4339e-02,\n",
      "         -2.0256e-01,  5.4167e-02,  1.4871e-01, -5.5893e-03, -3.6663e-02],\n",
      "        [-6.7453e-02,  2.1646e-03,  7.2981e-02,  4.8309e-02,  2.0503e-01,\n",
      "         -2.0059e-01,  3.3271e-02,  1.7051e-01, -4.5226e-02,  7.9508e-02,\n",
      "          1.2508e-01,  4.2260e-02, -1.7692e-01, -2.1938e-01,  6.7608e-02,\n",
      "         -2.1452e-01, -3.8447e-02, -1.1406e-01, -2.0444e-01,  1.9631e-02],\n",
      "        [ 4.4186e-03, -8.3523e-02,  1.6618e-01,  8.9265e-02,  2.6735e-02,\n",
      "         -2.1464e-01, -3.5645e-02,  3.8411e-02, -6.1529e-02, -3.4503e-02,\n",
      "         -1.9583e-01,  9.8220e-02,  1.4114e-01, -2.1337e-01,  7.1877e-02,\n",
      "          1.8875e-01,  1.0166e-01,  1.2285e-01,  1.0344e-01,  1.7934e-01],\n",
      "        [-4.0225e-02, -6.4288e-02,  1.9998e-01, -3.7583e-02, -5.8658e-02,\n",
      "          1.7714e-01,  1.0550e-04, -6.4171e-02, -2.1365e-01,  9.7779e-02,\n",
      "         -1.5320e-01, -6.4925e-02,  1.0292e-01,  4.9225e-02,  2.0713e-01,\n",
      "          1.9934e-01, -6.6956e-02, -1.7660e-01, -1.7650e-02,  3.2319e-02],\n",
      "        [ 1.5139e-01, -1.8680e-01,  1.7806e-01,  2.7955e-02, -5.3246e-02,\n",
      "          4.0416e-02, -8.5570e-02,  2.2649e-02,  2.4558e-02,  2.7166e-02,\n",
      "          1.6830e-01, -6.2857e-02, -1.3751e-01,  1.1588e-01, -1.2002e-01,\n",
      "          1.4166e-01,  2.7886e-03, -1.8409e-01,  2.9488e-02,  3.8102e-02],\n",
      "        [ 1.3057e-01, -3.8314e-02, -1.3680e-01,  2.1310e-01, -1.3852e-02,\n",
      "          1.2522e-01, -2.4656e-02, -1.5589e-01,  2.1218e-01,  1.8573e-01,\n",
      "          1.6953e-01, -1.4708e-01, -1.3968e-01, -1.4366e-01,  2.1097e-01,\n",
      "         -1.7038e-01, -1.8788e-01, -1.1198e-01,  1.3012e-01, -3.3638e-02],\n",
      "        [-1.8075e-01, -1.6039e-01, -5.8042e-02,  7.6673e-02, -1.5864e-01,\n",
      "          2.0873e-01, -1.7395e-01,  7.4829e-02,  1.8629e-01, -2.2325e-01,\n",
      "         -1.0932e-03,  6.0779e-02, -1.5929e-01,  1.5706e-01, -1.4510e-01,\n",
      "         -1.6518e-01, -9.6159e-02,  7.9139e-02, -2.0135e-01,  2.1856e-01],\n",
      "        [-6.7831e-02,  1.0906e-01, -1.1086e-01, -8.3047e-02, -1.0127e-01,\n",
      "          1.4211e-01,  1.5039e-01,  1.4329e-02, -1.9938e-01,  2.1484e-01,\n",
      "         -4.0672e-02,  4.4031e-02, -1.4476e-01, -2.0984e-02,  2.1944e-01,\n",
      "          1.2456e-01, -2.0443e-01,  8.7104e-03,  3.2184e-02, -4.8392e-02],\n",
      "        [ 7.8105e-02, -9.0756e-04,  1.2695e-01, -1.9380e-01,  9.1247e-02,\n",
      "         -1.8261e-01, -7.8753e-02,  1.1822e-01,  1.4951e-01, -5.8039e-02,\n",
      "         -2.0796e-01, -1.9506e-01, -2.0549e-01, -8.3701e-02,  1.0715e-02,\n",
      "          1.6490e-01,  2.0466e-02,  1.8116e-01, -9.1340e-02,  8.0163e-02],\n",
      "        [ 2.1480e-01,  9.3469e-02, -1.8224e-01, -1.7528e-01,  1.5370e-01,\n",
      "          3.9695e-02,  1.7414e-01, -1.9924e-01,  6.2526e-02, -2.0946e-01,\n",
      "         -1.4067e-01, -1.3706e-01,  4.8083e-02,  1.3814e-01,  5.6575e-02,\n",
      "          4.5079e-02,  1.0066e-01, -2.4239e-02,  8.7862e-02, -8.6115e-02],\n",
      "        [-2.1349e-01,  4.2572e-02, -1.9789e-01, -1.0052e-01, -2.7966e-02,\n",
      "         -1.8550e-01, -1.1659e-01,  8.9376e-02,  1.4896e-02,  7.2139e-02,\n",
      "         -1.0476e-01, -6.4157e-02, -9.7571e-02,  3.4483e-04,  1.6688e-01,\n",
      "         -1.6733e-01,  8.8050e-02, -2.1564e-01, -6.1142e-02, -3.0401e-03],\n",
      "        [-1.2344e-01,  1.8944e-01,  2.1374e-01, -5.7402e-02,  2.0896e-01,\n",
      "          7.5190e-02,  1.4777e-01,  3.8222e-02, -1.2572e-01, -9.8249e-04,\n",
      "          2.2086e-01, -4.6997e-02, -1.3044e-01,  5.8854e-02, -1.7968e-01,\n",
      "         -1.7511e-01,  3.2675e-02,  1.9132e-01,  1.5811e-01, -1.3880e-01],\n",
      "        [-4.9329e-02,  7.0870e-02,  1.3495e-02,  1.2711e-01, -1.7805e-01,\n",
      "         -1.0216e-01,  1.0517e-01, -5.7768e-02,  2.0134e-02,  1.3676e-01,\n",
      "         -1.0709e-01, -8.9497e-02,  2.0588e-01, -1.9189e-01,  4.7253e-02,\n",
      "          4.4556e-02, -4.9342e-02,  1.7527e-01, -1.7206e-01, -1.2874e-01],\n",
      "        [-6.4133e-02, -7.8082e-02,  1.3518e-01,  9.7080e-02,  1.0171e-01,\n",
      "         -7.1772e-02,  8.5105e-02, -3.7560e-02,  1.2873e-01, -1.4208e-01,\n",
      "         -1.5433e-02,  2.0867e-01, -5.7713e-02, -1.6937e-01, -1.9640e-01,\n",
      "         -1.5347e-01,  1.3831e-01,  2.0940e-01,  3.3341e-02,  1.2929e-01],\n",
      "        [ 1.8295e-01, -3.4993e-02, -1.8535e-01,  1.4089e-01,  1.3480e-01,\n",
      "          1.0510e-01,  3.7867e-02,  1.6438e-01, -8.0902e-03, -7.5039e-02,\n",
      "          5.7242e-02, -3.8462e-02, -8.7200e-02,  1.9399e-01,  6.3607e-02,\n",
      "          6.6015e-02, -8.2978e-02,  9.8814e-02, -1.7406e-01,  2.2293e-01],\n",
      "        [ 1.5256e-02,  1.2482e-01, -1.1943e-01, -6.2849e-02,  1.2045e-02,\n",
      "         -2.1777e-01, -1.0199e-01, -7.2089e-02,  1.2890e-01, -1.6641e-01,\n",
      "         -1.5820e-02,  1.0851e-01, -1.6568e-01, -5.8653e-02,  2.1725e-01,\n",
      "          1.9397e-01, -8.1081e-02,  4.9424e-03, -1.5338e-01,  1.4620e-01],\n",
      "        [-1.1939e-01, -1.1728e-01,  2.0284e-01,  9.6837e-02,  1.7845e-01,\n",
      "          2.5563e-02, -1.5102e-01,  1.0133e-02, -1.4422e-01,  1.6729e-01,\n",
      "         -4.8021e-02,  2.1692e-01, -1.9132e-01, -1.9626e-01,  2.1118e-01,\n",
      "          4.7416e-02,  1.8776e-01, -5.0164e-02, -2.0811e-01, -9.9156e-02],\n",
      "        [ 1.4591e-02,  5.0079e-02, -1.5107e-01,  1.2548e-01, -1.5974e-01,\n",
      "         -1.5200e-01,  4.7691e-02, -1.0130e-01,  1.5385e-01,  1.0011e-01,\n",
      "          1.6754e-01,  5.6896e-02,  1.7734e-01, -1.4939e-01,  2.0324e-01,\n",
      "          1.8601e-01,  1.0709e-01,  2.3813e-02, -8.8234e-02, -1.1586e-01],\n",
      "        [ 7.6760e-02,  9.8386e-02, -6.6335e-02,  1.2955e-01,  2.1788e-01,\n",
      "         -2.0335e-02,  1.5455e-01, -2.1315e-01, -1.7193e-01,  1.8720e-01,\n",
      "         -5.4878e-02,  6.8003e-02,  1.6010e-01,  6.1428e-02, -6.1539e-02,\n",
      "          6.1153e-02, -3.5252e-02,  2.0409e-01, -2.2230e-01, -1.5115e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1421, -0.1819,  0.0454, -0.1694, -0.1612,  0.0878, -0.1243, -0.1465,\n",
      "         0.0177, -0.1257,  0.1942, -0.0539, -0.1270,  0.0836, -0.2149,  0.1881,\n",
      "        -0.1565, -0.0906,  0.0351,  0.0588], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1782, -0.0822, -0.1695,  0.0258, -0.0117,  0.1247,  0.0470, -0.0895,\n",
      "         -0.1381, -0.1493, -0.1408,  0.1441, -0.0846, -0.0969, -0.0182,  0.0431,\n",
      "         -0.0245,  0.2227, -0.0843, -0.0898],\n",
      "        [ 0.0452,  0.0006, -0.1836, -0.1933, -0.1661, -0.1819,  0.0556, -0.1241,\n",
      "         -0.1722,  0.1017, -0.0054, -0.0796,  0.1669, -0.0035,  0.1012, -0.0991,\n",
      "          0.2143, -0.0473, -0.2186, -0.1968],\n",
      "        [ 0.2129,  0.0056, -0.1716,  0.1951,  0.0461,  0.0097, -0.1884, -0.0233,\n",
      "          0.1425, -0.0608,  0.1111,  0.0769, -0.1638, -0.1194,  0.1509,  0.0454,\n",
      "          0.0807, -0.1460,  0.1656,  0.0829],\n",
      "        [ 0.0777, -0.1869,  0.0178, -0.0563,  0.1964, -0.0740, -0.1681,  0.0190,\n",
      "         -0.1582, -0.0380, -0.1723, -0.1122, -0.1977,  0.2166, -0.1379, -0.1748,\n",
      "         -0.1214, -0.0884, -0.1324,  0.1277],\n",
      "        [ 0.1121, -0.0991, -0.0524, -0.0139,  0.0962, -0.1763, -0.1855, -0.1049,\n",
      "          0.1434,  0.1645,  0.2024,  0.1373,  0.0477, -0.1889,  0.0243,  0.0074,\n",
      "          0.1894,  0.2196,  0.0492,  0.0023],\n",
      "        [ 0.0190,  0.1835, -0.2155, -0.1223,  0.1353,  0.1147,  0.0556,  0.1272,\n",
      "         -0.0575,  0.1170, -0.2059, -0.1551, -0.1413, -0.0957,  0.0637,  0.0648,\n",
      "         -0.0457, -0.1031, -0.1657, -0.0951],\n",
      "        [-0.0215,  0.0181,  0.0244,  0.0532,  0.0249, -0.1617,  0.0555,  0.0977,\n",
      "         -0.1654, -0.0128,  0.1669,  0.0174,  0.0715, -0.0599,  0.0980, -0.1584,\n",
      "          0.0976, -0.1141,  0.0787,  0.0259],\n",
      "        [-0.1182,  0.1155, -0.0815,  0.0634,  0.1421,  0.2143, -0.1256, -0.0108,\n",
      "          0.0683, -0.1077, -0.0698, -0.0131,  0.0498, -0.0870,  0.0073,  0.1461,\n",
      "         -0.0737, -0.1846, -0.1821,  0.1725],\n",
      "        [-0.1702,  0.1082,  0.0068, -0.2193,  0.0018,  0.0663,  0.1791, -0.1240,\n",
      "          0.2229,  0.0654, -0.2033, -0.0082,  0.0570,  0.1380, -0.1765,  0.2061,\n",
      "          0.1625,  0.1678, -0.1972,  0.0245],\n",
      "        [-0.2010,  0.2047, -0.1195,  0.1115,  0.1064, -0.1616,  0.0900, -0.0500,\n",
      "         -0.1791, -0.1331,  0.0335, -0.0814, -0.1391,  0.0896,  0.0056,  0.1677,\n",
      "         -0.1483, -0.1978, -0.0702, -0.1415],\n",
      "        [ 0.0719, -0.0463,  0.0152,  0.1329,  0.0966,  0.1178,  0.1349,  0.2015,\n",
      "          0.1536, -0.0740, -0.0227,  0.1943, -0.1544,  0.0308, -0.0583, -0.0327,\n",
      "          0.1413,  0.1569,  0.1382, -0.2026],\n",
      "        [ 0.1017, -0.1260, -0.1951, -0.1723,  0.0775, -0.1596, -0.1591,  0.1560,\n",
      "          0.0265, -0.1745, -0.1910,  0.1877,  0.1407,  0.0433, -0.1613, -0.1126,\n",
      "         -0.0431, -0.1678,  0.0860, -0.2170],\n",
      "        [ 0.0966, -0.1114, -0.2135,  0.0757, -0.1972, -0.1478, -0.0799, -0.0136,\n",
      "          0.2206, -0.1654,  0.1251,  0.0459,  0.1555,  0.1396,  0.0075,  0.0444,\n",
      "         -0.1729,  0.1082,  0.1407, -0.1696],\n",
      "        [ 0.0761,  0.0752,  0.1576, -0.1353, -0.1135, -0.1768, -0.1972, -0.0257,\n",
      "         -0.1186,  0.2077,  0.1847, -0.1185,  0.0064,  0.1390, -0.0444,  0.0387,\n",
      "          0.2086, -0.1321, -0.1767,  0.1109],\n",
      "        [ 0.0702,  0.0135, -0.2053,  0.1813,  0.1983, -0.1876,  0.1192,  0.2235,\n",
      "         -0.1951, -0.2109, -0.1492, -0.0814, -0.1289,  0.1710, -0.1467, -0.0099,\n",
      "          0.1957,  0.1180, -0.1696, -0.1733],\n",
      "        [ 0.1840, -0.2025, -0.1302, -0.0770, -0.0714,  0.1012, -0.0551,  0.0104,\n",
      "          0.0262, -0.1965,  0.0535, -0.1308, -0.1714, -0.0161,  0.0834, -0.2037,\n",
      "         -0.0639,  0.1179,  0.1995,  0.1470],\n",
      "        [-0.0374, -0.0716,  0.0783, -0.0044,  0.1396, -0.1994,  0.1164,  0.0936,\n",
      "          0.1535,  0.0537, -0.0505, -0.0916,  0.2221, -0.0670,  0.0024,  0.1929,\n",
      "         -0.1558, -0.0592, -0.1698, -0.0271],\n",
      "        [ 0.0676,  0.1334,  0.1516, -0.1642,  0.1225,  0.0957,  0.0320,  0.2201,\n",
      "         -0.2158,  0.0331, -0.0981,  0.2184,  0.1002, -0.1631,  0.1831,  0.1471,\n",
      "          0.0822, -0.0594,  0.1724,  0.0440],\n",
      "        [-0.1560,  0.2112, -0.0244,  0.0998,  0.1999,  0.1712, -0.0779, -0.0865,\n",
      "          0.0379,  0.1110,  0.1644,  0.1408,  0.1206, -0.1328,  0.0250, -0.1614,\n",
      "          0.0440,  0.0174,  0.0293,  0.0730],\n",
      "        [-0.1934, -0.1010,  0.0996, -0.0931, -0.0505, -0.1897, -0.1956, -0.1835,\n",
      "         -0.0918, -0.0985,  0.1470,  0.1029, -0.1515,  0.1832, -0.0410,  0.0115,\n",
      "          0.0952, -0.1507, -0.0306,  0.0066]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1056, -0.0605,  0.2155, -0.1735,  0.2046, -0.0874,  0.0020, -0.1972,\n",
      "        -0.1672, -0.2104, -0.2034,  0.0303,  0.0959,  0.1752, -0.2197,  0.1411,\n",
      "         0.0228, -0.2189,  0.2096, -0.2081], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0666, -0.0744,  0.0660, -0.0979, -0.0217,  0.1953, -0.1731,  0.1686,\n",
      "         -0.1591, -0.0767,  0.1192,  0.1746,  0.2126, -0.1612, -0.0046, -0.0863,\n",
      "          0.0875, -0.0841, -0.0757, -0.0736],\n",
      "        [ 0.0860,  0.2012, -0.0798, -0.0580, -0.1371,  0.0750,  0.1401,  0.0790,\n",
      "          0.0268,  0.2162,  0.0291,  0.1900,  0.0654, -0.1233,  0.1046,  0.1072,\n",
      "         -0.2225, -0.1294, -0.1261,  0.2110],\n",
      "        [-0.1948,  0.1128,  0.0989,  0.1383, -0.0669, -0.1107,  0.1323,  0.0896,\n",
      "          0.2186, -0.2113,  0.1955,  0.1988,  0.1076,  0.0709, -0.0916,  0.0971,\n",
      "          0.1391,  0.1590,  0.0644, -0.1470],\n",
      "        [ 0.0140,  0.1640,  0.1305, -0.0857,  0.0027,  0.2204, -0.0245, -0.1372,\n",
      "         -0.0574, -0.1888, -0.0807, -0.1787,  0.0097,  0.0251,  0.0938,  0.2152,\n",
      "         -0.1077, -0.0547,  0.0873,  0.1651],\n",
      "        [ 0.0932,  0.0238,  0.0265,  0.0141, -0.0821,  0.1894, -0.1037,  0.0358,\n",
      "         -0.1672,  0.1111,  0.2198, -0.2186,  0.1290,  0.0290,  0.0664,  0.2049,\n",
      "          0.2097,  0.0488,  0.0822,  0.0155],\n",
      "        [-0.1715, -0.0782, -0.1656,  0.2210,  0.0899,  0.1146, -0.1562,  0.1058,\n",
      "          0.1618, -0.0182,  0.1674,  0.0844, -0.1035,  0.1323,  0.0805, -0.1372,\n",
      "         -0.0849,  0.1854,  0.0414,  0.1663],\n",
      "        [-0.0540, -0.0717, -0.0692, -0.1000,  0.0109, -0.0336, -0.0634,  0.1175,\n",
      "          0.0810,  0.0713,  0.1967, -0.0615, -0.0093,  0.1980,  0.1031, -0.1947,\n",
      "         -0.2044,  0.1886,  0.1317, -0.1928],\n",
      "        [ 0.0320,  0.2022, -0.1914, -0.0069,  0.0780,  0.1687, -0.1993, -0.1545,\n",
      "         -0.1171, -0.0072, -0.2220, -0.1026, -0.1529, -0.1222,  0.1370, -0.0685,\n",
      "          0.0784,  0.0163, -0.1446, -0.0460],\n",
      "        [-0.2023, -0.1619, -0.1557,  0.1438, -0.0664, -0.0633,  0.1347, -0.1454,\n",
      "          0.0066, -0.0592, -0.0507,  0.0046, -0.1856,  0.1038, -0.0658, -0.2008,\n",
      "         -0.1428, -0.1578,  0.1739,  0.1680],\n",
      "        [-0.0838,  0.1316, -0.1597,  0.0673,  0.2209, -0.0785, -0.0271, -0.1237,\n",
      "          0.1935, -0.1433, -0.2005,  0.1156, -0.1622,  0.1472, -0.1720,  0.1945,\n",
      "          0.1142, -0.0316,  0.0070,  0.2167],\n",
      "        [-0.0692, -0.2214,  0.1622, -0.1809, -0.0134,  0.0288, -0.0467,  0.1711,\n",
      "         -0.1237, -0.0047, -0.0334, -0.1206,  0.1168,  0.2086,  0.1312, -0.0506,\n",
      "          0.0840,  0.0965, -0.1685, -0.0207],\n",
      "        [-0.1193,  0.1845, -0.1470, -0.1401, -0.0604,  0.2211,  0.1024,  0.1438,\n",
      "         -0.0426, -0.0243, -0.0394,  0.1686,  0.0959,  0.1991, -0.2130, -0.0108,\n",
      "          0.0848, -0.0228,  0.0093,  0.1966],\n",
      "        [ 0.1593,  0.0406,  0.0821, -0.1628, -0.1809,  0.2142,  0.1183,  0.1374,\n",
      "          0.1280,  0.1459,  0.2050, -0.1040,  0.2212, -0.1442,  0.2154, -0.2032,\n",
      "         -0.1637,  0.2228,  0.1178,  0.0818],\n",
      "        [ 0.1106, -0.1805,  0.1273, -0.1449,  0.1703, -0.0609,  0.0137, -0.0275,\n",
      "         -0.0655,  0.1712,  0.2135,  0.0477, -0.2119,  0.1052, -0.2210, -0.1750,\n",
      "          0.0337,  0.1087,  0.1753,  0.0690],\n",
      "        [-0.1723, -0.0514,  0.0912,  0.0242, -0.0321, -0.2012, -0.1247, -0.0854,\n",
      "          0.0714,  0.1627,  0.2211,  0.1983,  0.1185, -0.1711,  0.0900, -0.0239,\n",
      "          0.1361,  0.0497, -0.1412, -0.1376],\n",
      "        [-0.0131,  0.2231,  0.0770,  0.1142, -0.1312,  0.1071, -0.0463, -0.1260,\n",
      "         -0.0052,  0.0312,  0.0769,  0.1087, -0.1418, -0.1262,  0.1168,  0.1602,\n",
      "          0.1940,  0.0732, -0.2130,  0.0233],\n",
      "        [ 0.1627, -0.1869, -0.1007, -0.2030,  0.0065, -0.0737,  0.0267, -0.0102,\n",
      "         -0.0992,  0.2046, -0.1960,  0.1288,  0.0615,  0.1657, -0.0735,  0.0376,\n",
      "          0.1708,  0.0141, -0.1348,  0.1975],\n",
      "        [ 0.0881,  0.0271, -0.1819, -0.2186,  0.1770,  0.0460, -0.0076,  0.2025,\n",
      "          0.2103, -0.0803, -0.0425, -0.1732,  0.0558, -0.1094, -0.0653, -0.2033,\n",
      "         -0.2176, -0.2083,  0.2106,  0.1711],\n",
      "        [-0.0788, -0.1089, -0.1677,  0.1577, -0.1026, -0.1727, -0.0433, -0.0764,\n",
      "         -0.0769, -0.0768, -0.2016, -0.0833, -0.1336, -0.1626,  0.1862, -0.1210,\n",
      "         -0.0303,  0.0817, -0.1479,  0.1356],\n",
      "        [-0.0510,  0.2045, -0.1698, -0.1799, -0.2120, -0.0822,  0.0111, -0.0698,\n",
      "         -0.1201, -0.1212,  0.0232, -0.1026, -0.0014, -0.0382,  0.1771, -0.0575,\n",
      "         -0.1066, -0.0630,  0.2177, -0.1244]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2058,  0.0818, -0.2038,  0.1287,  0.1375, -0.1653, -0.2027, -0.0724,\n",
      "        -0.0237,  0.1329, -0.1381, -0.0281, -0.1350, -0.1855, -0.1525, -0.0608,\n",
      "        -0.1219, -0.1418,  0.1646, -0.0775], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0876, -0.0747, -0.1960, -0.2202, -0.2043,  0.1417,  0.0276,  0.0763,\n",
      "         -0.1630,  0.1854,  0.1091, -0.1352, -0.1342, -0.1156,  0.0863,  0.2106,\n",
      "         -0.1067, -0.0499,  0.0563,  0.0466],\n",
      "        [ 0.1901, -0.0092, -0.1909,  0.0800,  0.0684, -0.1871, -0.0959, -0.1087,\n",
      "         -0.1651, -0.1833, -0.1300,  0.1806, -0.0042,  0.1123, -0.2205, -0.1354,\n",
      "          0.0160, -0.0344,  0.1668, -0.1525],\n",
      "        [ 0.1137,  0.0627,  0.1665,  0.1409, -0.1020,  0.1311,  0.1708,  0.0126,\n",
      "          0.1694, -0.0317, -0.1422, -0.1506, -0.0745, -0.1504,  0.2150,  0.2101,\n",
      "         -0.2189, -0.0556, -0.1617, -0.2191],\n",
      "        [-0.0451, -0.1322,  0.1310, -0.0589, -0.1397, -0.0151, -0.1555, -0.0436,\n",
      "          0.0891, -0.1879, -0.1059, -0.1284, -0.1337,  0.1045,  0.1397,  0.1678,\n",
      "         -0.0475, -0.1736, -0.0585,  0.0683],\n",
      "        [-0.1210, -0.2115,  0.1043, -0.2091, -0.0140,  0.1838, -0.1833,  0.2156,\n",
      "         -0.0674, -0.1314,  0.0691, -0.1478, -0.1992, -0.1232, -0.1297,  0.0549,\n",
      "         -0.1322, -0.1154, -0.0594,  0.0684],\n",
      "        [ 0.2135, -0.1144, -0.0917, -0.2070, -0.0513, -0.1940,  0.1524,  0.1778,\n",
      "          0.0524,  0.0568, -0.1485, -0.1544, -0.1723, -0.0676,  0.1216,  0.0666,\n",
      "         -0.1174,  0.0231,  0.2222,  0.1732],\n",
      "        [ 0.1466,  0.1728,  0.1393,  0.1694, -0.1963, -0.2088, -0.1089,  0.0884,\n",
      "          0.0771,  0.0947,  0.1775,  0.1566,  0.1782, -0.1879, -0.1468, -0.2169,\n",
      "         -0.1157,  0.1850,  0.0738, -0.0223],\n",
      "        [ 0.1922,  0.0566, -0.0767,  0.1803,  0.0283, -0.1818,  0.1275,  0.0013,\n",
      "          0.1997,  0.1906,  0.0286, -0.1399,  0.1150,  0.1443,  0.0365, -0.1781,\n",
      "          0.1344,  0.1322,  0.1892,  0.1507],\n",
      "        [ 0.1345,  0.0626, -0.1380, -0.0608, -0.0790, -0.2170, -0.1715,  0.0746,\n",
      "         -0.1104,  0.2226,  0.0088, -0.0654, -0.0655, -0.0769,  0.1590,  0.0341,\n",
      "         -0.2072, -0.0627,  0.1108,  0.1246],\n",
      "        [ 0.2036,  0.0362,  0.1846,  0.1073,  0.0476,  0.2172, -0.1301, -0.1780,\n",
      "          0.0658, -0.1801,  0.0587, -0.0038, -0.2211,  0.1321,  0.0709, -0.0455,\n",
      "         -0.0787,  0.1698,  0.0648, -0.0351],\n",
      "        [ 0.0594,  0.1471, -0.1454,  0.0071, -0.0479, -0.0239,  0.0850, -0.0133,\n",
      "         -0.0066, -0.0072,  0.0805,  0.1758,  0.0458,  0.1913,  0.0463,  0.1845,\n",
      "          0.0562,  0.1631,  0.1120,  0.0876],\n",
      "        [ 0.0164, -0.0340,  0.1099, -0.0587, -0.1328, -0.0901, -0.2021,  0.0094,\n",
      "          0.1231, -0.2130,  0.0942,  0.1819, -0.0495, -0.0238,  0.0317, -0.1070,\n",
      "         -0.1626,  0.0755,  0.0974, -0.1301],\n",
      "        [-0.0179,  0.2142,  0.1337,  0.1748,  0.0935,  0.0110, -0.1134,  0.1157,\n",
      "          0.0815, -0.2206,  0.2016, -0.0140,  0.1548, -0.0873,  0.0883,  0.1003,\n",
      "         -0.0547, -0.1923,  0.0187, -0.1745],\n",
      "        [-0.0785, -0.0035,  0.1944, -0.0251, -0.1383, -0.1809, -0.1487, -0.0278,\n",
      "         -0.1727,  0.0392, -0.0501,  0.1336,  0.2161,  0.1194,  0.1956,  0.1610,\n",
      "         -0.1325, -0.1863,  0.2223, -0.0622],\n",
      "        [-0.0767, -0.1411,  0.0014,  0.0005, -0.0240,  0.1827, -0.1850,  0.1398,\n",
      "          0.0592, -0.0038, -0.0860, -0.1479,  0.1559,  0.1390, -0.1291, -0.1001,\n",
      "         -0.1966, -0.0937, -0.1670,  0.2069],\n",
      "        [-0.2208,  0.2117, -0.1018, -0.1363,  0.0751, -0.0768,  0.0495, -0.1117,\n",
      "          0.1080, -0.1304, -0.0173, -0.2154,  0.1253, -0.2167, -0.0615,  0.1500,\n",
      "          0.0705, -0.0646,  0.2177,  0.0640],\n",
      "        [-0.0879,  0.1647, -0.0871,  0.0748, -0.0636, -0.1858, -0.0726, -0.1646,\n",
      "          0.0618,  0.0577, -0.0731,  0.2073,  0.1740,  0.0743,  0.0007, -0.1343,\n",
      "          0.1055, -0.2208, -0.0982, -0.0864],\n",
      "        [ 0.1362,  0.0502, -0.0177,  0.1608,  0.2230,  0.1531,  0.2085, -0.1426,\n",
      "         -0.0493,  0.1959,  0.1427,  0.0103, -0.1716,  0.0877,  0.1224, -0.0863,\n",
      "          0.0323,  0.0521, -0.1299,  0.0207],\n",
      "        [ 0.0688,  0.1561, -0.1532,  0.1216,  0.1010,  0.0892, -0.0369, -0.0341,\n",
      "          0.0960, -0.0252, -0.0072,  0.1530,  0.0027, -0.1004, -0.1861,  0.1971,\n",
      "          0.2078,  0.1658, -0.1070, -0.0264],\n",
      "        [ 0.0526, -0.1262,  0.0994,  0.2060,  0.0420, -0.1633, -0.1048,  0.1453,\n",
      "          0.1621,  0.2111,  0.0738, -0.0870, -0.0281,  0.1771, -0.1494,  0.2165,\n",
      "          0.1314,  0.1620, -0.1543,  0.0199]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1631,  0.0316, -0.1457, -0.0739,  0.0012, -0.1910, -0.0596,  0.0844,\n",
      "        -0.1013,  0.1842, -0.2053, -0.1713,  0.0968,  0.1644, -0.1217,  0.0372,\n",
      "         0.0350, -0.1648,  0.1756,  0.0713], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1224, -0.0003, -0.0682,  0.1473,  0.1545, -0.0472,  0.1019, -0.0068,\n",
      "         -0.0037,  0.1279,  0.0869, -0.1551,  0.1011,  0.1481, -0.0372,  0.0092,\n",
      "         -0.1378, -0.1788,  0.0641,  0.2073],\n",
      "        [-0.1018, -0.1612,  0.2098, -0.0252,  0.1041, -0.1995, -0.0350,  0.0015,\n",
      "          0.0986, -0.1967,  0.1525,  0.2012,  0.0562, -0.0278,  0.1855,  0.0389,\n",
      "          0.0537, -0.1850, -0.0162, -0.1630],\n",
      "        [-0.1133, -0.0868, -0.0580, -0.1512,  0.1389,  0.0742, -0.2000,  0.0321,\n",
      "          0.0706,  0.0692,  0.0124, -0.1957,  0.1687, -0.1978, -0.1302,  0.0976,\n",
      "         -0.0506, -0.0225,  0.1857,  0.1478],\n",
      "        [ 0.0017, -0.1092, -0.1186, -0.1761,  0.2197,  0.0678, -0.0348,  0.1120,\n",
      "         -0.1939, -0.0958, -0.2133, -0.1923,  0.0049, -0.0604,  0.0767, -0.0495,\n",
      "          0.1299,  0.0864,  0.0803, -0.1933],\n",
      "        [-0.0660,  0.1976, -0.1078, -0.0933, -0.1879, -0.1228,  0.0461,  0.1165,\n",
      "          0.0037,  0.1618, -0.0733,  0.0022,  0.2031, -0.2190,  0.1533,  0.2194,\n",
      "         -0.0173,  0.0951,  0.2122, -0.0833],\n",
      "        [-0.2060, -0.0635,  0.0785,  0.0881, -0.1279, -0.0134, -0.1093,  0.2112,\n",
      "         -0.1794, -0.1902,  0.0743, -0.1409, -0.1395,  0.1437,  0.0898, -0.0086,\n",
      "         -0.0470, -0.1051, -0.1263,  0.1306],\n",
      "        [-0.2231, -0.1693, -0.2125, -0.0373,  0.2079,  0.0739,  0.0451, -0.1370,\n",
      "          0.1694, -0.1673,  0.1373,  0.2053,  0.1939, -0.1962,  0.1271,  0.2108,\n",
      "         -0.1251,  0.1287,  0.0229, -0.1714],\n",
      "        [-0.1444, -0.1291, -0.0270, -0.1255, -0.2132,  0.0643, -0.0708,  0.0335,\n",
      "          0.0297, -0.1201,  0.0848, -0.2072,  0.0483, -0.1986,  0.1159,  0.0725,\n",
      "         -0.2128, -0.1099, -0.1537,  0.1847],\n",
      "        [-0.0050, -0.0904,  0.0058,  0.2139,  0.1086,  0.0654, -0.1567, -0.1584,\n",
      "         -0.1894, -0.0681,  0.0950, -0.0157,  0.0486, -0.1907,  0.1143, -0.0940,\n",
      "          0.1170, -0.0303, -0.1127, -0.1033],\n",
      "        [ 0.0298, -0.1152,  0.0626,  0.1437, -0.1709,  0.0609, -0.0496,  0.0700,\n",
      "          0.0766,  0.1456,  0.0209, -0.0861,  0.0691,  0.0816,  0.0690, -0.0214,\n",
      "          0.1674, -0.0886, -0.1198,  0.1218],\n",
      "        [-0.0607,  0.1355,  0.0167,  0.0899, -0.0631, -0.0893, -0.1232,  0.0844,\n",
      "         -0.1962, -0.1051, -0.1117, -0.0602, -0.0315, -0.0892,  0.0429,  0.0595,\n",
      "         -0.0890,  0.2201, -0.0572, -0.0555],\n",
      "        [ 0.0577,  0.0023, -0.1441,  0.0342, -0.1457, -0.0117, -0.1238,  0.1395,\n",
      "          0.0422, -0.1336, -0.0861,  0.1990,  0.0375,  0.1633, -0.2006,  0.1057,\n",
      "         -0.0243, -0.1268, -0.0880, -0.0582],\n",
      "        [ 0.1303,  0.1936,  0.1689, -0.0297,  0.1539, -0.2069,  0.0832,  0.0505,\n",
      "         -0.1040,  0.1962,  0.0141, -0.2165, -0.0902,  0.2135,  0.0213, -0.0566,\n",
      "         -0.1228,  0.1610, -0.0694, -0.0463],\n",
      "        [ 0.1313, -0.1383, -0.1979, -0.0438, -0.0786,  0.1418, -0.0716,  0.0284,\n",
      "         -0.2163,  0.0473, -0.1447, -0.0504, -0.1840, -0.1603, -0.2045, -0.0546,\n",
      "          0.0803, -0.1008,  0.0254, -0.0898],\n",
      "        [-0.1282,  0.1960,  0.1266,  0.0869, -0.1897,  0.0759, -0.2154, -0.1341,\n",
      "          0.0878,  0.1487, -0.2077, -0.1086, -0.1374,  0.1491, -0.1433, -0.1511,\n",
      "          0.0126,  0.1521, -0.0273, -0.0720],\n",
      "        [-0.1665, -0.1515, -0.0990,  0.1963,  0.0356, -0.0918,  0.2153,  0.1434,\n",
      "         -0.1028,  0.1659,  0.1546, -0.0427, -0.0451, -0.0844, -0.1739, -0.1368,\n",
      "         -0.1499,  0.2213,  0.1966,  0.0647],\n",
      "        [ 0.0462,  0.1737,  0.1054,  0.1689, -0.1168,  0.0924,  0.0624,  0.0393,\n",
      "          0.1410, -0.1164,  0.2088, -0.2087,  0.0845,  0.2135,  0.0542, -0.0086,\n",
      "          0.1249, -0.0765,  0.0086,  0.0539],\n",
      "        [ 0.1573, -0.0146, -0.0358,  0.0226, -0.0346, -0.1677,  0.1330, -0.2162,\n",
      "          0.1443, -0.1412,  0.0878,  0.0266, -0.0117,  0.1687,  0.0579, -0.0226,\n",
      "          0.0827, -0.0349,  0.1881,  0.0625],\n",
      "        [-0.0178, -0.1253, -0.0796, -0.1559, -0.0412, -0.0889, -0.1374,  0.1257,\n",
      "         -0.0416, -0.1743, -0.0535, -0.0631, -0.0054, -0.0594, -0.0313,  0.1891,\n",
      "         -0.2077, -0.2231, -0.1786, -0.1138],\n",
      "        [-0.2060, -0.0544, -0.1780,  0.1483, -0.2131,  0.0912,  0.1344,  0.1561,\n",
      "          0.1393, -0.1356, -0.1218,  0.0422, -0.0158, -0.1351, -0.0648,  0.1611,\n",
      "          0.0450,  0.0393, -0.1881,  0.0613]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2074,  0.0488,  0.0330,  0.2070, -0.1216,  0.1360, -0.0847,  0.1304,\n",
      "         0.1520, -0.2224,  0.0603, -0.0750, -0.1337,  0.2068, -0.1435,  0.0307,\n",
      "         0.0227, -0.2099, -0.1792, -0.1507], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0533, -0.0753,  0.0994,  0.1694,  0.0151, -0.0389,  0.0307, -0.1562,\n",
      "          0.0009, -0.1477, -0.0403,  0.1077, -0.1312, -0.2017,  0.0710,  0.0857,\n",
      "          0.2159, -0.1066,  0.1231,  0.1518],\n",
      "        [ 0.0111,  0.2089, -0.1248,  0.1049,  0.0988, -0.1874,  0.0861, -0.0165,\n",
      "         -0.1223, -0.0978,  0.0691, -0.0772,  0.0138,  0.1833,  0.1093, -0.1226,\n",
      "          0.0720,  0.0394,  0.1874,  0.1735],\n",
      "        [ 0.0375, -0.0394, -0.0676,  0.0718, -0.2086,  0.0560, -0.1917, -0.0373,\n",
      "         -0.1436, -0.0817,  0.1463,  0.1355, -0.1379, -0.0486, -0.1894, -0.0209,\n",
      "         -0.1247, -0.1007, -0.1241,  0.1567],\n",
      "        [-0.1482,  0.0494,  0.0773,  0.0332,  0.1685, -0.0034,  0.1171, -0.1838,\n",
      "          0.0117,  0.1760,  0.0934, -0.0463, -0.0569,  0.1187,  0.1619,  0.0381,\n",
      "         -0.1214, -0.1731, -0.0653,  0.1753],\n",
      "        [ 0.0919, -0.1471, -0.2138,  0.2068,  0.2080,  0.1298, -0.0354, -0.0990,\n",
      "          0.0354,  0.1061,  0.1073, -0.1873,  0.0998,  0.1700,  0.1734, -0.0611,\n",
      "         -0.0090,  0.1706, -0.0748, -0.1973],\n",
      "        [-0.1869, -0.0781,  0.0112,  0.1396, -0.0654,  0.0913, -0.0546,  0.0515,\n",
      "         -0.1974, -0.1316, -0.0591, -0.0514, -0.1397, -0.1929,  0.1778, -0.0567,\n",
      "         -0.0035,  0.2056, -0.1000, -0.1806],\n",
      "        [ 0.0033, -0.0877, -0.1901,  0.1979,  0.0009,  0.0738,  0.0918,  0.1995,\n",
      "          0.1239, -0.0665,  0.0792, -0.1305,  0.0702,  0.1012, -0.0658, -0.0991,\n",
      "         -0.0612, -0.1986, -0.1253, -0.0358],\n",
      "        [ 0.0245,  0.0552, -0.1914, -0.0984, -0.0665,  0.1763, -0.1025, -0.0067,\n",
      "          0.0371, -0.1514, -0.0628, -0.2039,  0.1768, -0.0829, -0.1123, -0.1419,\n",
      "          0.0441, -0.0947,  0.1910,  0.0013],\n",
      "        [-0.1800,  0.1043,  0.0645, -0.1203,  0.1986, -0.0425, -0.1281,  0.0506,\n",
      "          0.1425, -0.0372, -0.0731, -0.1256,  0.1919, -0.0940,  0.0256,  0.1778,\n",
      "         -0.1711, -0.0029,  0.0909, -0.1231],\n",
      "        [-0.1834, -0.0570, -0.0570,  0.0708,  0.1679, -0.0975, -0.0197, -0.0159,\n",
      "         -0.1576,  0.0561, -0.0481, -0.0962,  0.0904, -0.1876, -0.1981, -0.0172,\n",
      "         -0.0157,  0.0833,  0.0461,  0.0022]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0142, -0.1273,  0.2072,  0.1579,  0.0410,  0.0863, -0.1904, -0.2047,\n",
      "        -0.1346,  0.0039], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1683,  0.1755,  0.0357,  0.0696,  0.0016, -0.2346,  0.2807,  0.0138,\n",
      "          0.3082,  0.1530],\n",
      "        [ 0.2775,  0.3094, -0.0186, -0.1966,  0.1675,  0.2277,  0.3111, -0.0850,\n",
      "          0.1798, -0.0494],\n",
      "        [-0.1387,  0.1026,  0.1692,  0.0470,  0.1029,  0.0319, -0.0513,  0.2274,\n",
      "         -0.2684, -0.1849],\n",
      "        [ 0.1750, -0.1938,  0.0005, -0.2771, -0.1685, -0.2077, -0.0781,  0.2304,\n",
      "         -0.1715,  0.1768]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1913,  0.3046, -0.2113,  0.0866], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1222,  0.1658],\n",
      "        [ 0.5895, -0.3936],\n",
      "        [ 0.2091, -0.5809],\n",
      "        [-0.4007,  0.0204],\n",
      "        [ 0.1673, -0.0865],\n",
      "        [ 0.5830,  0.4924],\n",
      "        [-0.0268,  0.6524],\n",
      "        [-0.5950, -0.2160],\n",
      "        [-0.4126,  0.6124],\n",
      "        [-0.5405,  0.4084]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0664, 0.2410, 0.3305, 0.6887, 0.3434, 0.5088, 0.3717, 0.4372, 0.1544,\n",
      "        0.1809], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1688, -0.2424,  0.0703, -0.2438, -0.2912, -0.1119,  0.0598,  0.0370,\n",
      "          0.0648, -0.2732],\n",
      "        [-0.2781,  0.1087,  0.2509,  0.1278, -0.2996,  0.2610,  0.0176, -0.0150,\n",
      "         -0.2205,  0.1103],\n",
      "        [ 0.3105, -0.0903, -0.0865, -0.2828, -0.0955,  0.2810,  0.1354, -0.0152,\n",
      "         -0.1009, -0.2294],\n",
      "        [-0.1536, -0.2163,  0.1678, -0.0528,  0.0907,  0.1916, -0.0731,  0.0107,\n",
      "         -0.2257,  0.2674],\n",
      "        [ 0.1839,  0.1633, -0.0474,  0.1110, -0.1045, -0.1706,  0.2257, -0.0482,\n",
      "          0.2111,  0.2065],\n",
      "        [ 0.2043,  0.2661, -0.0664,  0.1999,  0.0162,  0.0594, -0.1101,  0.1939,\n",
      "          0.0193,  0.0099],\n",
      "        [-0.2081,  0.0295, -0.0260, -0.1764, -0.2394,  0.2533, -0.1854,  0.1065,\n",
      "          0.0536, -0.0261],\n",
      "        [-0.1798,  0.2449,  0.0201,  0.0826, -0.2264,  0.0379,  0.2978, -0.3122,\n",
      "         -0.1170, -0.0215],\n",
      "        [ 0.0350, -0.3022, -0.1695, -0.2917, -0.0796, -0.1076, -0.0373,  0.2145,\n",
      "          0.1145, -0.1889],\n",
      "        [ 0.2202, -0.0444, -0.2358,  0.2827, -0.0123, -0.0758, -0.0369,  0.0319,\n",
      "         -0.1437,  0.2497],\n",
      "        [-0.0798, -0.0735,  0.2078, -0.2807,  0.2609, -0.0148,  0.2453, -0.1092,\n",
      "         -0.0202, -0.1155],\n",
      "        [-0.2074, -0.3109, -0.0017, -0.2974, -0.1619, -0.1342,  0.0589, -0.1159,\n",
      "          0.1868, -0.2665],\n",
      "        [-0.0234, -0.0380, -0.2129, -0.2915,  0.0506, -0.0610,  0.0057,  0.0070,\n",
      "          0.1818, -0.2376],\n",
      "        [ 0.2472, -0.0014,  0.1257, -0.1276,  0.2127,  0.0744,  0.3127,  0.0328,\n",
      "         -0.2977,  0.2353],\n",
      "        [-0.2493,  0.2708,  0.2958,  0.0588, -0.1253, -0.0879,  0.0853, -0.1486,\n",
      "         -0.0114,  0.1650],\n",
      "        [-0.0249,  0.2986,  0.0720, -0.2239,  0.2183, -0.0747,  0.0258,  0.2119,\n",
      "          0.1018,  0.0913],\n",
      "        [-0.1020,  0.2113,  0.2634,  0.0087, -0.1719,  0.2319,  0.2304, -0.1650,\n",
      "         -0.1172,  0.2466],\n",
      "        [-0.2058,  0.0006, -0.1830,  0.2142,  0.0363, -0.1488,  0.2661, -0.1973,\n",
      "          0.1709,  0.1257],\n",
      "        [-0.0288,  0.2612, -0.3046,  0.2533,  0.1208, -0.0792, -0.1525,  0.2325,\n",
      "         -0.2734, -0.1203],\n",
      "        [ 0.1324,  0.2028,  0.0532, -0.1496, -0.2476, -0.2941, -0.1717,  0.2776,\n",
      "          0.2053, -0.1393]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0602, -0.1199,  0.2745, -0.1796,  0.1739,  0.2552,  0.1982, -0.1716,\n",
      "         0.2211, -0.0233, -0.1710, -0.2429,  0.2318,  0.2750,  0.1428,  0.0127,\n",
      "        -0.0779,  0.2972, -0.0384, -0.2070], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2150,  0.0646, -0.0958,  0.1289,  0.0008,  0.1188,  0.0275,  0.0883,\n",
      "         -0.1498,  0.0097, -0.0318,  0.0392,  0.1864,  0.0826, -0.0825,  0.0559,\n",
      "         -0.1583, -0.0923,  0.1421,  0.0139],\n",
      "        [ 0.1875, -0.0607,  0.1877,  0.1105, -0.0310,  0.2223, -0.1289, -0.2066,\n",
      "         -0.0597,  0.0161,  0.0916, -0.2138, -0.0955, -0.1124,  0.0730, -0.0719,\n",
      "         -0.0389, -0.1867, -0.0302, -0.0460],\n",
      "        [ 0.2134,  0.0055, -0.1504, -0.0489,  0.1626,  0.0926, -0.0092,  0.0610,\n",
      "         -0.1383, -0.1774,  0.2019, -0.2091, -0.0564, -0.1114,  0.1601,  0.2046,\n",
      "         -0.0171, -0.0711, -0.0381, -0.1485],\n",
      "        [-0.0110, -0.1156,  0.0814,  0.1340,  0.1373,  0.1764, -0.1775,  0.2219,\n",
      "         -0.0962, -0.1353, -0.1641,  0.0062, -0.1514, -0.2057, -0.1272,  0.0306,\n",
      "          0.1479, -0.1423,  0.0063, -0.0551],\n",
      "        [ 0.0928, -0.1917,  0.0275, -0.0751,  0.2229, -0.0434, -0.1665,  0.1861,\n",
      "         -0.0309, -0.0180, -0.0207, -0.0446,  0.0366, -0.1720,  0.1846,  0.2016,\n",
      "         -0.1366, -0.0584,  0.0444,  0.2166],\n",
      "        [-0.0999,  0.0910,  0.1110,  0.0270, -0.1772,  0.2038, -0.1648,  0.0720,\n",
      "         -0.0621,  0.0534,  0.1169,  0.1697, -0.1463, -0.0608,  0.0748,  0.0824,\n",
      "          0.0625,  0.1206,  0.0829,  0.1620],\n",
      "        [-0.1310, -0.0348,  0.1933,  0.0534, -0.1223,  0.0798,  0.0394,  0.1548,\n",
      "         -0.1697,  0.0694, -0.1942, -0.1732, -0.1927,  0.2065, -0.0253, -0.0222,\n",
      "          0.1803, -0.0003,  0.1442, -0.1415],\n",
      "        [ 0.0255, -0.1697,  0.1861,  0.0320,  0.0592, -0.0218,  0.1268,  0.1826,\n",
      "          0.0399,  0.2209,  0.1377,  0.1708, -0.1960,  0.0631,  0.2227, -0.2048,\n",
      "          0.0485,  0.0119,  0.0145,  0.1436],\n",
      "        [-0.1463, -0.1369, -0.1302, -0.0131, -0.0835,  0.0236,  0.0540,  0.0397,\n",
      "         -0.0780, -0.1487,  0.0558,  0.1258, -0.1696, -0.1368, -0.0643,  0.1423,\n",
      "         -0.1457, -0.2219, -0.1447,  0.0114],\n",
      "        [-0.0207, -0.0497, -0.0898, -0.2201,  0.0337,  0.1259,  0.1423, -0.1149,\n",
      "          0.0008,  0.1158,  0.0619, -0.1705,  0.1893,  0.1149, -0.1728, -0.0900,\n",
      "          0.0201,  0.0903, -0.1361, -0.2102],\n",
      "        [-0.2024, -0.0477,  0.2233,  0.1570,  0.1112, -0.1911,  0.0165, -0.1921,\n",
      "         -0.0811,  0.2149, -0.0709,  0.1612,  0.0982,  0.1040,  0.1909,  0.0955,\n",
      "          0.1304,  0.2181, -0.1789, -0.1800],\n",
      "        [ 0.1813, -0.1785,  0.0943, -0.0075,  0.0360, -0.1091,  0.1541, -0.1505,\n",
      "         -0.0859,  0.2204, -0.0475,  0.1804,  0.1416,  0.1461,  0.0603, -0.0245,\n",
      "          0.0030, -0.0638, -0.0285, -0.0079],\n",
      "        [ 0.0352, -0.1439, -0.1108, -0.0119, -0.1236, -0.1751, -0.0510, -0.1686,\n",
      "         -0.0927, -0.1888,  0.0879,  0.0774,  0.2146,  0.1970, -0.0853,  0.0657,\n",
      "          0.1143,  0.0368,  0.0010, -0.1643],\n",
      "        [-0.1240,  0.0815,  0.0333, -0.0909, -0.0561, -0.1734,  0.2138,  0.2183,\n",
      "          0.1564, -0.0478, -0.0712, -0.0291, -0.1156,  0.1305,  0.1606, -0.0958,\n",
      "          0.2185, -0.2119, -0.0609,  0.0307],\n",
      "        [ 0.1824,  0.0777, -0.0606, -0.2036, -0.1710, -0.1395,  0.1364,  0.1597,\n",
      "          0.1835, -0.0557, -0.0486,  0.2002, -0.2188, -0.1390, -0.0749, -0.0798,\n",
      "          0.2021,  0.1779,  0.2184, -0.1218],\n",
      "        [-0.0693,  0.0381, -0.1268, -0.0059, -0.1239,  0.0172, -0.0542,  0.0471,\n",
      "          0.0956, -0.1405,  0.1806,  0.1531,  0.1771,  0.1520,  0.1176, -0.2178,\n",
      "         -0.1820, -0.0423,  0.0685, -0.0260],\n",
      "        [-0.0831, -0.1853,  0.1743,  0.0414,  0.1825, -0.1961, -0.1946, -0.0811,\n",
      "          0.1691,  0.0933, -0.2142,  0.1247, -0.0407,  0.1387,  0.1303, -0.0171,\n",
      "         -0.1986, -0.1584, -0.1828, -0.1125],\n",
      "        [ 0.2014,  0.1963,  0.1026, -0.1752, -0.1423,  0.1039, -0.1180,  0.0058,\n",
      "          0.0253,  0.1258,  0.1943, -0.2207,  0.0529,  0.0249,  0.0383,  0.1123,\n",
      "         -0.0541,  0.1239, -0.0320,  0.1318],\n",
      "        [ 0.1893,  0.0994,  0.0396, -0.1874,  0.1183,  0.0454, -0.0255, -0.1437,\n",
      "          0.0313, -0.1919,  0.1745,  0.1585, -0.1643, -0.0732, -0.0782,  0.1041,\n",
      "         -0.1725, -0.0485, -0.0114,  0.1007],\n",
      "        [-0.2017,  0.0399, -0.0748,  0.1424,  0.1236,  0.0806,  0.0182,  0.0967,\n",
      "          0.1126,  0.1050, -0.0853,  0.1724, -0.1298,  0.2067,  0.1957,  0.0028,\n",
      "          0.0415,  0.0412, -0.0663,  0.0584]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0755,  0.0749,  0.1195, -0.0306, -0.1542, -0.0735, -0.1811,  0.2099,\n",
      "         0.1734, -0.0609, -0.0567,  0.1864,  0.2040, -0.0493,  0.0998,  0.1306,\n",
      "        -0.0995, -0.1177,  0.1271,  0.1276], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6418e-01, -2.6162e-02, -2.1865e-01,  2.1150e-01, -2.1577e-01,\n",
      "          6.6830e-02,  1.7441e-01, -1.7433e-01, -1.3416e-01,  1.3481e-01,\n",
      "         -4.3957e-02,  2.0361e-01, -7.9122e-02, -1.5905e-01, -1.5355e-01,\n",
      "         -3.6352e-02,  1.0018e-02, -4.1348e-02, -2.4317e-02, -2.6906e-02],\n",
      "        [ 3.4943e-03,  1.9947e-01,  2.0565e-01,  2.1833e-01,  2.0986e-01,\n",
      "          1.4550e-01,  8.2415e-02,  2.0007e-01, -1.1176e-01, -1.7393e-01,\n",
      "         -1.9879e-01,  7.6585e-02,  6.6390e-02, -4.6889e-02,  7.4499e-02,\n",
      "         -1.2460e-01,  1.9554e-01,  4.5592e-02, -6.2833e-02,  1.1434e-01],\n",
      "        [-6.5815e-02,  2.8433e-02,  2.2164e-02, -1.5422e-01,  1.5059e-01,\n",
      "         -1.4513e-01, -2.1232e-01,  1.2728e-01, -1.2431e-01,  1.5973e-01,\n",
      "         -7.6781e-02,  1.2474e-01,  1.0194e-01,  1.8733e-01, -5.4477e-02,\n",
      "          2.1967e-01,  2.4967e-03,  6.8201e-02,  1.4483e-01,  1.8548e-01],\n",
      "        [-1.8901e-01, -8.6383e-02,  8.7290e-02,  1.8750e-01, -2.2051e-01,\n",
      "          2.1366e-01,  1.8027e-01, -1.8354e-01,  2.1912e-01,  1.6532e-01,\n",
      "          1.9568e-01, -4.1380e-02, -1.1029e-02,  1.5037e-01, -7.0078e-02,\n",
      "          1.3833e-01,  2.9404e-02,  2.2680e-02, -3.0749e-02, -6.3238e-02],\n",
      "        [ 2.1093e-01, -5.6559e-02,  1.1644e-01,  2.1360e-01, -3.4315e-02,\n",
      "         -1.8373e-01,  1.2119e-01,  3.5307e-02, -4.2738e-02,  1.0041e-01,\n",
      "          7.6601e-02,  1.8969e-01,  1.8132e-01,  2.1614e-01, -1.7365e-01,\n",
      "          1.5188e-01, -1.5345e-01, -1.4808e-02, -1.9347e-01, -9.2870e-03],\n",
      "        [-5.0557e-02, -1.5208e-01,  8.4369e-02,  1.0910e-01,  1.2052e-01,\n",
      "          9.9900e-02,  2.0394e-01, -1.2796e-01,  1.3710e-01,  1.4440e-01,\n",
      "          7.5325e-02,  1.8275e-01,  1.9922e-01,  2.9827e-02,  7.8721e-02,\n",
      "          1.2308e-01, -1.8692e-01, -1.9939e-01, -1.8758e-01,  1.3367e-01],\n",
      "        [-4.5318e-02, -1.8691e-02,  1.8648e-01,  8.3674e-02,  5.1921e-02,\n",
      "         -1.3345e-01, -5.5966e-03, -1.0864e-01,  1.0464e-01, -1.6670e-01,\n",
      "          2.9324e-04, -1.5177e-01,  3.1628e-02, -9.3669e-02, -6.0591e-02,\n",
      "         -1.2120e-01, -4.4764e-02, -3.3079e-02,  3.3046e-02,  2.0586e-01],\n",
      "        [-1.7388e-01, -1.8467e-01, -1.3716e-02,  1.5126e-01,  1.1136e-01,\n",
      "         -1.4008e-01, -1.6532e-01, -2.1875e-01,  1.6579e-01,  4.7430e-02,\n",
      "         -1.8535e-01,  2.0782e-01,  9.2355e-03, -1.6814e-01, -7.4610e-02,\n",
      "          9.8356e-02,  1.7542e-01,  1.3940e-01,  7.0750e-02, -6.3201e-02],\n",
      "        [ 1.5922e-01, -1.9940e-01,  1.8614e-01,  1.9329e-01, -7.7274e-02,\n",
      "         -1.4175e-03, -9.0641e-02, -1.9262e-01,  2.1119e-01,  1.3678e-01,\n",
      "         -2.0221e-01, -1.6308e-01, -4.1777e-02,  1.9564e-01,  1.8611e-01,\n",
      "          1.4284e-02,  1.3858e-01,  1.1012e-01, -1.8995e-01,  7.8861e-03],\n",
      "        [-1.2384e-01, -1.7630e-01, -2.8305e-02,  3.9319e-02, -1.0663e-02,\n",
      "          1.7324e-01,  1.0211e-01,  4.0602e-02, -1.6883e-01,  1.1887e-01,\n",
      "         -1.3390e-02, -2.1215e-01,  2.2256e-01, -1.5477e-01,  2.0861e-01,\n",
      "          1.8138e-01, -1.2964e-01,  2.1534e-01,  1.3348e-01,  1.1467e-01],\n",
      "        [-1.6467e-01,  7.0469e-03, -2.2072e-01, -1.4132e-01, -2.2068e-02,\n",
      "          2.2243e-01,  1.8760e-01,  2.2020e-01,  1.6079e-01,  1.2861e-01,\n",
      "          1.0936e-01,  1.4964e-01,  6.7599e-02,  1.5241e-01, -6.0872e-02,\n",
      "          6.4593e-02, -1.6677e-01, -1.7233e-01,  2.0734e-01,  2.2781e-02],\n",
      "        [-5.2930e-03, -1.6759e-01,  1.2214e-01, -1.4090e-01, -1.1275e-01,\n",
      "          2.2131e-01, -2.0852e-01,  1.4871e-01,  1.3028e-01,  4.5739e-02,\n",
      "          1.7296e-01,  5.2233e-02, -1.0016e-02,  1.2278e-01,  1.3661e-01,\n",
      "         -1.6369e-01,  1.2007e-01, -1.4257e-01,  9.7480e-02, -8.0958e-02],\n",
      "        [-4.7309e-02,  1.0996e-01, -6.9206e-02, -2.0554e-02, -3.9262e-02,\n",
      "         -1.5250e-01, -1.9506e-01,  1.2808e-01, -2.2254e-01,  7.7264e-02,\n",
      "         -7.3059e-02, -6.9661e-03,  5.8827e-02, -4.4494e-02, -1.3103e-01,\n",
      "          2.0523e-01, -6.5555e-02, -1.1400e-01,  7.4885e-02,  3.3833e-02],\n",
      "        [ 4.5888e-02,  1.1878e-01, -2.2275e-01,  1.3624e-01, -2.1197e-01,\n",
      "          1.3002e-01,  5.8436e-02,  1.3288e-01,  4.9325e-02,  1.0358e-01,\n",
      "         -1.2907e-01, -7.3687e-02, -1.3162e-01,  1.0639e-01, -1.4600e-01,\n",
      "          5.8053e-03,  1.4604e-01,  4.5627e-02,  7.0921e-02, -3.4627e-02],\n",
      "        [ 7.3124e-02,  9.5196e-02,  1.7324e-01, -1.7269e-01,  1.5295e-01,\n",
      "         -6.7891e-02, -4.3888e-02,  2.4270e-02, -5.8746e-02,  1.6714e-01,\n",
      "          9.0469e-02,  2.1718e-01,  9.6568e-02,  1.0206e-01,  1.6756e-01,\n",
      "          1.2978e-01,  1.8010e-02,  5.4253e-02, -1.0816e-02, -2.5456e-02],\n",
      "        [ 9.9085e-02, -9.2086e-02,  8.5481e-02, -1.1010e-01,  2.9863e-02,\n",
      "          3.8538e-02, -2.0294e-01,  1.5491e-01,  2.0635e-01,  1.1674e-02,\n",
      "          8.9349e-02, -1.9923e-01,  1.9945e-01,  1.8419e-01,  2.0179e-04,\n",
      "          1.7849e-01,  1.7952e-01, -2.3970e-02, -1.6059e-01, -1.2350e-01],\n",
      "        [ 1.0105e-01, -1.5355e-01,  1.8700e-01, -2.1820e-01,  1.5231e-01,\n",
      "          1.9317e-01, -2.2226e-02, -1.7086e-01, -1.3412e-01,  8.5490e-02,\n",
      "          4.2656e-02,  2.2120e-01,  1.1023e-01, -1.2791e-01, -1.6437e-02,\n",
      "          1.5538e-01, -1.8978e-01, -1.8703e-01,  7.8021e-02, -5.8609e-02],\n",
      "        [-1.0772e-03, -1.6641e-01,  1.0870e-01,  9.9312e-02, -1.1626e-02,\n",
      "          6.9110e-02,  3.0921e-02,  8.1451e-02, -2.1732e-01, -2.3277e-02,\n",
      "          3.1209e-02, -2.0082e-01, -1.3306e-01,  9.1419e-02,  1.8488e-01,\n",
      "          4.3403e-02, -1.6325e-02, -1.3835e-01, -1.7316e-01,  4.7846e-02],\n",
      "        [-1.7484e-01, -1.9157e-01, -5.5137e-02,  2.9306e-02,  5.1613e-02,\n",
      "          1.3827e-01,  4.2731e-02, -2.1201e-01,  1.1385e-01,  1.0268e-02,\n",
      "          3.4028e-02,  2.1817e-01,  1.2054e-01,  2.0500e-01, -5.1971e-02,\n",
      "          2.0043e-01,  1.0786e-01,  1.1979e-02, -2.4225e-02,  1.5005e-01],\n",
      "        [ 6.9208e-02,  1.7265e-01,  5.0548e-02, -2.7090e-02, -5.5912e-02,\n",
      "         -5.5304e-02, -1.2228e-02,  2.1250e-01,  1.1820e-01,  7.2563e-02,\n",
      "         -1.9747e-01,  1.8180e-01, -1.7044e-01, -2.1553e-01, -8.4723e-02,\n",
      "          1.3218e-01, -8.2486e-02,  2.1976e-01,  1.7066e-01,  1.8677e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0291,  0.1514, -0.0658, -0.0010,  0.1674, -0.1211, -0.0015, -0.2229,\n",
      "        -0.2167, -0.1162,  0.0796,  0.0339, -0.1731, -0.2091, -0.0767,  0.0659,\n",
      "        -0.1052,  0.1168, -0.0036, -0.1907], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.3530e-01,  2.1974e-02,  9.9631e-02, -1.0140e-01, -8.8711e-04,\n",
      "          6.2024e-02,  2.0388e-01,  9.1905e-02, -4.8150e-02,  1.6447e-02,\n",
      "          8.3819e-02, -6.7608e-02, -1.9359e-01,  1.5451e-01, -2.3329e-02,\n",
      "         -1.9699e-01, -1.9980e-01,  1.6841e-01, -4.8852e-02,  6.6318e-02],\n",
      "        [-1.3085e-01,  9.0136e-02, -3.7956e-02,  1.9124e-01, -1.9222e-01,\n",
      "         -2.0047e-01,  2.2277e-01,  5.0186e-02,  3.4081e-02, -1.1520e-01,\n",
      "          1.2110e-01,  2.1989e-01, -1.8405e-01, -1.1044e-01,  1.4077e-01,\n",
      "          1.5136e-02, -4.5887e-02, -1.2908e-01, -1.0646e-01, -7.2063e-02],\n",
      "        [-1.7211e-01,  6.1395e-02,  4.6204e-02, -1.2505e-01,  1.1588e-01,\n",
      "          9.5773e-03,  6.6930e-02,  1.5177e-01, -1.0023e-01, -1.9602e-01,\n",
      "         -1.5062e-01,  1.5165e-01, -1.8751e-01,  2.9509e-02, -1.3073e-01,\n",
      "         -1.8946e-03,  9.7344e-02,  1.6073e-01,  4.2215e-02, -1.5639e-01],\n",
      "        [-7.4242e-03,  4.8245e-02, -1.5425e-01,  1.2218e-01,  8.3095e-02,\n",
      "         -3.4513e-02,  6.9782e-02, -2.1818e-01,  1.6065e-01, -1.1351e-01,\n",
      "          1.0155e-01, -1.0106e-01,  9.1945e-04,  1.4751e-01, -1.4883e-01,\n",
      "         -1.1378e-01,  1.6639e-01,  2.2464e-02,  1.5734e-01, -3.6566e-02],\n",
      "        [ 1.5565e-01,  1.9074e-01,  1.4195e-01, -2.7298e-02,  1.3805e-01,\n",
      "          2.4551e-02, -1.0643e-01,  2.0984e-01, -1.9356e-02,  1.7554e-01,\n",
      "          2.1560e-01,  2.2387e-02,  3.1961e-02, -1.4766e-01, -1.5295e-01,\n",
      "          1.0651e-02,  1.3975e-02,  1.5981e-02, -5.2513e-02, -3.1826e-02],\n",
      "        [ 1.0546e-01, -3.7553e-02,  1.4781e-02,  1.4928e-01, -9.4684e-02,\n",
      "         -6.8574e-02,  1.0757e-03,  6.6947e-02, -9.9253e-02,  9.9465e-02,\n",
      "          1.2320e-02, -1.9624e-01,  5.6436e-02,  3.5881e-03, -4.9459e-02,\n",
      "         -1.6589e-01,  1.8098e-01,  4.4428e-02,  5.8149e-02, -5.7575e-02],\n",
      "        [ 4.5951e-02,  2.8189e-02,  1.6946e-01,  4.3289e-02,  2.1839e-01,\n",
      "          1.8704e-01,  8.1412e-02, -1.8730e-01,  1.8498e-01,  9.9366e-02,\n",
      "         -9.2241e-02,  5.4617e-02,  1.4864e-01, -2.1737e-01,  1.7591e-01,\n",
      "         -1.7112e-01,  7.1700e-02, -6.4453e-02, -3.1698e-02, -1.7979e-01],\n",
      "        [-2.0718e-01, -1.2656e-01, -1.3073e-01, -1.6153e-01, -1.3565e-01,\n",
      "          2.3308e-02, -1.7356e-01,  2.1521e-01, -6.9867e-02,  8.6826e-02,\n",
      "          1.1421e-01, -1.4333e-01, -9.3034e-02,  1.4332e-01, -8.5061e-02,\n",
      "          1.3419e-01,  1.0049e-03,  4.1033e-02,  8.3623e-02, -1.9825e-01],\n",
      "        [ 9.2346e-02,  9.0067e-02, -1.6346e-03,  4.7711e-02,  1.8272e-03,\n",
      "         -1.3790e-01,  1.1943e-01,  9.0270e-02,  1.5543e-01, -2.9404e-02,\n",
      "          1.9981e-01,  8.1257e-02, -1.1091e-01,  1.2367e-01, -6.6593e-02,\n",
      "         -1.1816e-01, -2.1288e-01, -1.2990e-01,  5.6848e-02,  8.9006e-02],\n",
      "        [-7.6697e-03,  1.7414e-01, -2.0766e-01,  4.7834e-02, -8.0077e-02,\n",
      "          8.2581e-02,  9.5560e-02, -1.2370e-01, -1.7129e-01,  1.6930e-01,\n",
      "          2.0007e-01, -1.1599e-01,  2.9712e-02, -1.9244e-01,  3.9015e-02,\n",
      "          9.3088e-03,  1.2968e-01,  1.4576e-01, -5.1151e-02,  1.7115e-01],\n",
      "        [-1.0787e-01, -8.1576e-02, -2.0892e-01,  1.9211e-01, -1.6431e-02,\n",
      "          1.8401e-01,  8.2434e-02,  6.9743e-02, -1.5289e-01,  9.9571e-02,\n",
      "          6.1589e-02, -6.2227e-02,  8.1715e-02,  1.5003e-01, -1.0056e-01,\n",
      "         -1.5027e-01,  1.7134e-01,  5.4332e-02,  4.8875e-02, -7.9324e-03],\n",
      "        [ 2.0008e-01,  1.3145e-03,  1.6141e-01, -4.0138e-02,  1.0188e-01,\n",
      "         -1.2468e-02, -1.1249e-01,  6.6497e-02, -5.3508e-02,  3.1444e-02,\n",
      "         -1.9725e-01, -1.5062e-01,  2.9665e-02, -6.0957e-02,  1.5493e-02,\n",
      "          2.2062e-01, -1.0265e-01, -1.2647e-01, -1.2957e-01,  1.5279e-01],\n",
      "        [-1.8204e-01,  1.6883e-01,  1.3955e-01, -1.0111e-01, -5.1844e-02,\n",
      "          1.5347e-01, -1.1528e-03,  1.5625e-01, -9.7759e-02, -1.0810e-01,\n",
      "         -5.1024e-02, -1.3574e-01,  1.1118e-01,  1.3139e-01,  2.0610e-01,\n",
      "         -1.6171e-01, -1.6948e-01, -1.5494e-01,  8.1989e-02, -1.2780e-01],\n",
      "        [ 2.0577e-01,  3.5979e-02, -1.3334e-01,  1.5191e-01, -6.7058e-02,\n",
      "         -2.0179e-01,  3.9858e-02,  6.4501e-02,  3.8707e-02,  1.4340e-01,\n",
      "         -1.5742e-02, -1.7804e-01,  2.9494e-02, -4.0323e-02,  4.8960e-03,\n",
      "          1.9030e-01, -1.5998e-01, -1.3898e-01,  3.5924e-02,  7.4172e-02],\n",
      "        [ 1.2826e-01, -2.5957e-02, -4.8312e-02, -1.2600e-01,  2.0533e-01,\n",
      "         -4.1648e-02, -7.0885e-02,  1.4435e-01,  9.7915e-02,  1.5774e-01,\n",
      "         -1.7785e-01,  2.0557e-02,  1.9291e-01, -2.0812e-01,  3.1830e-02,\n",
      "          1.1213e-01, -1.9705e-01, -4.9912e-02, -2.1213e-01,  1.6459e-01],\n",
      "        [-4.7865e-02,  1.7243e-01,  9.8378e-02,  6.0027e-02, -1.1206e-01,\n",
      "          1.1481e-02,  2.1252e-01,  1.3039e-01,  1.4111e-01, -4.8407e-02,\n",
      "         -1.9952e-01,  7.8933e-02, -1.4059e-01,  2.0903e-01, -1.8810e-01,\n",
      "          2.9040e-02,  2.2524e-02,  9.6492e-03,  1.9052e-01,  3.1356e-02],\n",
      "        [-1.8076e-01,  1.7871e-01, -1.5635e-01, -1.8950e-01, -1.1452e-04,\n",
      "         -1.9484e-01,  1.1786e-01, -1.1805e-01, -9.6452e-03, -9.0114e-02,\n",
      "          1.9357e-01, -8.8878e-02, -2.1996e-01,  1.7601e-01, -1.5984e-01,\n",
      "         -9.9557e-02, -1.8269e-01, -1.4771e-01, -1.5244e-01,  1.6821e-01],\n",
      "        [-2.1126e-01,  5.9296e-03,  8.9008e-02, -5.0721e-02,  1.1981e-01,\n",
      "         -2.1417e-01, -1.6582e-01,  8.5669e-02,  2.6216e-02,  7.8388e-02,\n",
      "         -3.3688e-02,  1.5923e-01, -5.5828e-02,  1.7051e-01, -2.3759e-02,\n",
      "          7.9986e-04,  1.2708e-01,  2.2106e-01,  5.5006e-02, -1.8660e-01],\n",
      "        [-9.9629e-02, -5.0183e-02, -6.2432e-02,  1.3203e-01,  4.9301e-02,\n",
      "          1.6576e-01, -3.8202e-03,  1.2292e-01,  4.9073e-02,  1.0254e-01,\n",
      "          2.2318e-01, -1.7417e-01, -2.2191e-01,  1.4730e-01, -1.9384e-01,\n",
      "          2.1906e-01, -1.0054e-01, -1.2158e-03,  7.2575e-02,  5.8475e-02],\n",
      "        [-3.8496e-02, -3.2163e-02, -4.2567e-02, -1.6850e-01, -1.8423e-01,\n",
      "         -5.4490e-02, -7.7585e-02, -1.3105e-01,  3.3899e-02, -2.0458e-01,\n",
      "          1.2396e-01,  2.0281e-01,  7.3189e-02, -2.1809e-01, -1.3625e-01,\n",
      "         -1.8808e-01, -1.7429e-01,  1.4229e-01, -2.7149e-02,  5.9002e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1830,  0.1025, -0.1721, -0.0924,  0.0061, -0.1672, -0.0805,  0.0416,\n",
      "        -0.1568,  0.0208,  0.1468, -0.1052,  0.1975, -0.0877,  0.1079, -0.1652,\n",
      "        -0.0061,  0.0183, -0.1620, -0.0269], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0804,  0.1153,  0.0133, -0.0650,  0.1159,  0.0089,  0.1046, -0.0635,\n",
      "          0.1481,  0.0217, -0.1716, -0.1657,  0.0184, -0.1380,  0.0485,  0.1171,\n",
      "         -0.0117, -0.0230, -0.0815, -0.1159],\n",
      "        [-0.0992, -0.2183,  0.0150, -0.0770,  0.1855, -0.1332,  0.0812, -0.1538,\n",
      "         -0.0672,  0.1621, -0.0095,  0.0090, -0.1859, -0.0521,  0.1520, -0.1532,\n",
      "         -0.0693, -0.1345, -0.0957, -0.1419],\n",
      "        [ 0.0418,  0.1052,  0.1410,  0.0198, -0.1708, -0.1825, -0.0662, -0.2023,\n",
      "          0.1544, -0.1769,  0.1025, -0.0588, -0.2230,  0.0637,  0.1981,  0.1741,\n",
      "         -0.1545, -0.1060, -0.1158, -0.1194],\n",
      "        [-0.2059,  0.1730,  0.2086,  0.0565, -0.0085,  0.0598, -0.1550, -0.0921,\n",
      "          0.0146, -0.0897,  0.0158, -0.0279, -0.0563,  0.1662,  0.0224,  0.0923,\n",
      "          0.1823, -0.0231,  0.0316, -0.1590],\n",
      "        [ 0.0220,  0.1441,  0.1857, -0.1186, -0.1275,  0.1553, -0.1251,  0.0490,\n",
      "          0.2013,  0.1703, -0.0884, -0.1821, -0.0940,  0.0402, -0.0539, -0.0433,\n",
      "          0.1884,  0.0529,  0.1941,  0.0013],\n",
      "        [-0.1205,  0.0289, -0.0544,  0.1591, -0.0129,  0.1686,  0.0574,  0.1139,\n",
      "          0.0270, -0.0788, -0.0977, -0.0605,  0.1566, -0.0163, -0.2018,  0.1475,\n",
      "         -0.1973, -0.0830, -0.0640, -0.1897],\n",
      "        [-0.0188,  0.1401, -0.1881,  0.0095,  0.2069, -0.1802, -0.1516,  0.0018,\n",
      "         -0.0103,  0.0946, -0.0309,  0.2057,  0.0596, -0.0209, -0.0293, -0.1240,\n",
      "          0.1270,  0.1809,  0.0107, -0.0111],\n",
      "        [-0.2079,  0.2048, -0.2037,  0.2060, -0.0302,  0.0480,  0.0845, -0.1081,\n",
      "         -0.1536,  0.0989, -0.1743,  0.1473, -0.2015,  0.0880,  0.1531, -0.0981,\n",
      "         -0.1760,  0.2064, -0.1633,  0.2089],\n",
      "        [ 0.0177,  0.2196,  0.0006, -0.0537, -0.0488,  0.0479, -0.2170,  0.1263,\n",
      "         -0.1718,  0.1063,  0.1760,  0.0152,  0.1872, -0.1159, -0.0920, -0.1429,\n",
      "         -0.0097,  0.1722,  0.1829,  0.1830],\n",
      "        [ 0.0214,  0.1826, -0.0031, -0.0049, -0.2191, -0.0925, -0.2113,  0.0157,\n",
      "          0.1380, -0.0343,  0.1622,  0.0268, -0.1050,  0.0315,  0.2058,  0.0279,\n",
      "          0.0105,  0.0563, -0.1979, -0.0438],\n",
      "        [ 0.0424, -0.1745, -0.0658,  0.1333,  0.0147, -0.0296,  0.1691, -0.0935,\n",
      "         -0.2062, -0.0176,  0.0195,  0.0109, -0.1220, -0.0802, -0.2096,  0.2198,\n",
      "          0.1815,  0.0299, -0.1860,  0.1147],\n",
      "        [ 0.0122, -0.1425, -0.2078, -0.1698,  0.1700, -0.1929,  0.1243,  0.0305,\n",
      "          0.1738, -0.1543,  0.1252, -0.2001, -0.0126, -0.1991, -0.1470,  0.0240,\n",
      "         -0.0672,  0.1442,  0.0963, -0.1286],\n",
      "        [-0.1847,  0.1783, -0.1729,  0.2210, -0.0120, -0.0709,  0.1842, -0.0578,\n",
      "          0.0337, -0.0601, -0.1315, -0.1089,  0.1727, -0.0930,  0.1002,  0.0847,\n",
      "         -0.0318,  0.0691,  0.1579, -0.1618],\n",
      "        [-0.1089,  0.0262,  0.1493, -0.2107, -0.0562,  0.1714, -0.0105,  0.1824,\n",
      "         -0.0642,  0.0437, -0.1615, -0.1484, -0.1316, -0.0256, -0.1716,  0.0152,\n",
      "         -0.1777, -0.1214,  0.0827,  0.1759],\n",
      "        [-0.1448, -0.0349, -0.0292, -0.1691, -0.0629,  0.2122, -0.0671,  0.1679,\n",
      "         -0.1898, -0.0933, -0.0716,  0.1528,  0.0599, -0.2041,  0.1312,  0.1046,\n",
      "         -0.1357,  0.0692,  0.1679, -0.1603],\n",
      "        [-0.0061,  0.1459, -0.1842, -0.0647,  0.1153, -0.0387,  0.1609, -0.1789,\n",
      "         -0.0925, -0.1635,  0.2112, -0.1684, -0.1493, -0.2061,  0.1415, -0.1927,\n",
      "         -0.0087, -0.1548, -0.0989, -0.1193],\n",
      "        [ 0.0836,  0.0227, -0.0171, -0.0525, -0.1539,  0.1644, -0.0101,  0.0448,\n",
      "         -0.0679, -0.1636, -0.0240, -0.1764,  0.0804,  0.1404,  0.0714, -0.1048,\n",
      "          0.1639, -0.0234,  0.0163,  0.1409],\n",
      "        [-0.1281,  0.1431, -0.0099, -0.1252,  0.0170,  0.1519, -0.0552, -0.1282,\n",
      "         -0.0324,  0.1326,  0.1248,  0.0800,  0.0120,  0.1611, -0.1219, -0.1331,\n",
      "          0.0559, -0.1574,  0.0178, -0.2059],\n",
      "        [-0.1367, -0.1397,  0.1883,  0.0433,  0.0816,  0.1616, -0.0714,  0.1617,\n",
      "         -0.0115,  0.1767, -0.1147,  0.1494,  0.0848, -0.1150, -0.0145,  0.1847,\n",
      "          0.1584,  0.1592,  0.1008,  0.0050],\n",
      "        [ 0.0659, -0.0113,  0.0316, -0.1212,  0.1366,  0.1290,  0.0492, -0.2090,\n",
      "         -0.0127,  0.0208,  0.1189,  0.0781,  0.0060, -0.1689, -0.0140, -0.1497,\n",
      "          0.0692,  0.0973,  0.2180, -0.0639]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0526, -0.2149,  0.1707, -0.0497, -0.2225,  0.0867, -0.0914, -0.1695,\n",
      "        -0.2095,  0.0485,  0.0890,  0.0941,  0.0511,  0.1685, -0.2161,  0.1341,\n",
      "        -0.1357,  0.0239, -0.0559, -0.2044], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2186,  0.1140,  0.2041, -0.1550, -0.1389, -0.0148, -0.1553, -0.1306,\n",
      "         -0.2101,  0.1806, -0.1559, -0.1201, -0.2036, -0.2060,  0.1395, -0.1111,\n",
      "          0.0151,  0.1638,  0.0953, -0.1125],\n",
      "        [ 0.1779, -0.0112,  0.1814,  0.0902,  0.0493, -0.1544,  0.0955,  0.0841,\n",
      "         -0.0899,  0.0571,  0.2134, -0.0520, -0.0692, -0.1601,  0.0173,  0.1264,\n",
      "         -0.0736, -0.1622,  0.0354,  0.2081],\n",
      "        [-0.1065,  0.1735, -0.1147,  0.0178, -0.0984, -0.0612, -0.1424, -0.0642,\n",
      "          0.0552, -0.1553,  0.1785,  0.1336, -0.0067,  0.0117, -0.0454,  0.1382,\n",
      "         -0.1506, -0.1011, -0.1060, -0.0926],\n",
      "        [-0.1386,  0.0321, -0.0163,  0.1205,  0.1118,  0.1188,  0.2185,  0.2047,\n",
      "         -0.0803, -0.1905, -0.0443,  0.1191, -0.0228, -0.0039,  0.0580,  0.0188,\n",
      "         -0.0455, -0.2018,  0.0070,  0.1058],\n",
      "        [ 0.1399,  0.0172, -0.1746,  0.0756,  0.1117, -0.1206,  0.1225, -0.1901,\n",
      "         -0.1747, -0.1903, -0.1436,  0.0599, -0.1379,  0.1476, -0.0255, -0.0543,\n",
      "         -0.0245, -0.0664,  0.1772,  0.0571],\n",
      "        [-0.1724,  0.0541,  0.1964,  0.1012,  0.1931, -0.2157,  0.1732, -0.2140,\n",
      "          0.1207,  0.0292,  0.1594, -0.1848,  0.2195,  0.1733, -0.1058,  0.0739,\n",
      "         -0.0257, -0.0494, -0.0726, -0.0646],\n",
      "        [-0.1669,  0.1077,  0.0779, -0.1689, -0.1547, -0.2112, -0.0102,  0.1753,\n",
      "          0.0622, -0.1385,  0.0972, -0.0644, -0.1239, -0.1868, -0.0367, -0.2150,\n",
      "         -0.0086,  0.1291, -0.1331,  0.1455],\n",
      "        [-0.1690, -0.0349,  0.2043,  0.0127,  0.0491, -0.0215, -0.1900, -0.1601,\n",
      "          0.1370,  0.1829, -0.2148,  0.0636, -0.1733, -0.1194,  0.1687, -0.0430,\n",
      "          0.1003,  0.0566, -0.0663, -0.1090],\n",
      "        [ 0.0871, -0.1111, -0.1776, -0.0628, -0.1166, -0.0831, -0.0697,  0.0759,\n",
      "         -0.0539,  0.0606, -0.1079,  0.0008, -0.0974,  0.2049, -0.0045,  0.1950,\n",
      "         -0.1184, -0.0991, -0.0086,  0.1689],\n",
      "        [ 0.2035, -0.0981,  0.1523,  0.1269, -0.1723, -0.0141,  0.0209, -0.1120,\n",
      "          0.1087,  0.1827,  0.1350,  0.0091, -0.2062,  0.1848, -0.1996, -0.1667,\n",
      "         -0.0238, -0.1360,  0.1823, -0.0442],\n",
      "        [-0.0039,  0.0156,  0.2116,  0.0647, -0.1421,  0.0628,  0.1795, -0.0392,\n",
      "          0.2102,  0.0308, -0.0109, -0.0282, -0.1926, -0.0090,  0.1534,  0.0847,\n",
      "         -0.1968,  0.1032, -0.0152, -0.0114],\n",
      "        [-0.1162, -0.2044, -0.0087,  0.0395,  0.0506,  0.2079,  0.0328, -0.1872,\n",
      "          0.0138, -0.2199, -0.0004,  0.0958, -0.0660,  0.0735, -0.0636, -0.0998,\n",
      "         -0.1965, -0.1973,  0.0583, -0.0917],\n",
      "        [-0.1083,  0.1745, -0.1172, -0.1135, -0.1529, -0.0304, -0.1252, -0.0249,\n",
      "         -0.1221,  0.1094,  0.0764,  0.0250,  0.1745, -0.0414,  0.0724,  0.1794,\n",
      "          0.1523,  0.1624, -0.2100,  0.0139],\n",
      "        [ 0.1533,  0.1080, -0.2186, -0.0598, -0.1175,  0.1258,  0.0023,  0.1213,\n",
      "          0.1504, -0.0331, -0.0111,  0.1224, -0.1789,  0.1795, -0.0782,  0.1659,\n",
      "         -0.1829,  0.1155,  0.0394,  0.0145],\n",
      "        [ 0.1344,  0.1804, -0.1293,  0.1091, -0.1304,  0.2154, -0.0943,  0.0470,\n",
      "         -0.2088, -0.1063,  0.1387,  0.1067, -0.0915,  0.2232,  0.1138,  0.1611,\n",
      "          0.1053,  0.0174,  0.0967, -0.1481],\n",
      "        [ 0.1091,  0.1740,  0.2205,  0.2146, -0.0170,  0.1852,  0.0531,  0.1227,\n",
      "         -0.0897, -0.0792, -0.0628,  0.1167, -0.2081,  0.1139,  0.1251, -0.1554,\n",
      "          0.1274, -0.2148, -0.2122, -0.0505],\n",
      "        [-0.1026, -0.2229, -0.1010, -0.0038, -0.1566, -0.1285,  0.0065,  0.0303,\n",
      "          0.1328,  0.0182,  0.0035,  0.1823,  0.2168,  0.1403, -0.1350,  0.1515,\n",
      "          0.1146,  0.1502, -0.1600,  0.1900],\n",
      "        [-0.0891,  0.0363,  0.0609, -0.1041,  0.0231,  0.0872, -0.1360, -0.0051,\n",
      "          0.0966, -0.1992,  0.1267, -0.2095,  0.1546, -0.1197,  0.0028,  0.1342,\n",
      "         -0.0810,  0.0608, -0.0913, -0.0223],\n",
      "        [ 0.0945,  0.0244,  0.1843, -0.0443, -0.1057,  0.1681, -0.0368, -0.1748,\n",
      "         -0.1387, -0.1217,  0.1398,  0.0226,  0.2154,  0.1308,  0.1578, -0.1963,\n",
      "          0.1007,  0.0395,  0.0211, -0.1106],\n",
      "        [ 0.0253,  0.1092,  0.0447,  0.0309, -0.1931,  0.1609, -0.0227, -0.1224,\n",
      "         -0.0248, -0.0170,  0.0681,  0.2037,  0.0368,  0.1034,  0.1207,  0.0822,\n",
      "         -0.0914,  0.0189, -0.0995,  0.1981]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1514,  0.0771,  0.1954,  0.0163,  0.1725,  0.0820, -0.1130,  0.2136,\n",
      "         0.1375,  0.1761,  0.0155, -0.0577,  0.1840,  0.1536, -0.0665, -0.1680,\n",
      "         0.0320,  0.0836,  0.1920, -0.0039], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.5424e-01,  1.8004e-02, -4.4716e-02,  9.2018e-02,  5.1088e-02,\n",
      "          2.1161e-01,  1.2329e-01,  5.0002e-02, -3.0159e-02,  1.0621e-01,\n",
      "         -1.9274e-01, -1.7568e-01, -1.8933e-01, -4.7246e-02,  1.1822e-01,\n",
      "         -1.6265e-01, -8.9886e-02,  7.2602e-02,  2.7549e-02, -2.4762e-02],\n",
      "        [-2.3916e-02,  1.6659e-01,  2.0215e-01,  2.7488e-03,  5.3615e-02,\n",
      "          1.3579e-02,  2.2352e-01, -7.3213e-02, -4.4854e-02, -2.2650e-02,\n",
      "          2.1764e-01, -1.3404e-01, -1.8830e-01, -1.4964e-01,  6.3713e-02,\n",
      "         -3.8213e-02,  7.1652e-03, -4.4256e-02, -1.4725e-04, -3.1710e-02],\n",
      "        [-8.6433e-02, -1.6626e-01,  1.2881e-01,  1.2257e-01, -1.1945e-01,\n",
      "          1.8838e-01,  1.4806e-01,  1.4444e-01,  1.1262e-01,  2.2014e-01,\n",
      "          9.6990e-02, -4.3040e-02, -1.2338e-01,  9.1318e-02,  3.1281e-02,\n",
      "          2.3370e-02,  2.0285e-01, -8.3218e-02, -2.3399e-02, -1.3755e-02],\n",
      "        [ 3.0247e-03, -6.8333e-02,  2.1450e-01, -2.2033e-01,  1.1109e-01,\n",
      "          1.8559e-01,  1.3298e-01,  5.0771e-02,  2.0664e-01,  1.5046e-01,\n",
      "         -1.9025e-01, -1.4044e-01, -7.6295e-02,  1.8336e-01, -2.0053e-01,\n",
      "         -4.1530e-02,  3.4884e-02, -1.5360e-02,  9.4493e-02,  5.2989e-02],\n",
      "        [-4.7181e-02, -5.1143e-02,  4.4013e-02, -7.3854e-02, -7.9386e-02,\n",
      "          2.1251e-01, -6.6701e-02,  1.2546e-01, -1.1267e-01, -1.6318e-01,\n",
      "         -3.8606e-02,  1.3807e-01,  5.1057e-03,  6.7565e-04,  1.6326e-01,\n",
      "          2.1682e-01, -7.4627e-02, -1.7646e-01, -1.4695e-01, -6.2647e-02],\n",
      "        [ 2.0267e-01,  1.1662e-01,  1.8559e-01, -1.6634e-01, -9.4463e-02,\n",
      "          5.9169e-02, -8.6689e-02,  2.3124e-02,  9.3528e-02,  3.3531e-02,\n",
      "          9.4132e-02,  1.2519e-01, -1.7553e-01,  6.4438e-02, -1.5318e-01,\n",
      "          1.1606e-01,  2.0036e-01, -2.0611e-01, -3.7746e-02,  8.2977e-02],\n",
      "        [ 7.3139e-03,  5.5020e-02, -2.1441e-01, -1.4569e-01, -9.4787e-02,\n",
      "         -2.0866e-01, -1.2555e-01, -1.0868e-01,  1.1781e-01,  7.2065e-02,\n",
      "         -1.5427e-01,  9.4272e-02, -1.7471e-01,  6.9169e-02, -2.1998e-01,\n",
      "          8.1814e-02,  7.3046e-02, -3.4044e-03, -1.6624e-01,  1.6485e-01],\n",
      "        [ 1.2686e-01, -1.3254e-01, -1.7682e-01,  3.5154e-02, -1.3080e-01,\n",
      "         -9.5721e-02,  1.5923e-01, -2.0417e-01, -7.1780e-02, -4.7600e-02,\n",
      "          1.1636e-01, -1.7311e-02, -2.0241e-01, -1.3472e-01,  8.9887e-02,\n",
      "         -1.2601e-01,  9.1574e-02,  1.5148e-01, -4.0152e-02,  1.6564e-01],\n",
      "        [-1.1839e-01, -1.0045e-01,  5.2504e-02, -9.3876e-02,  2.1586e-01,\n",
      "          1.7949e-01,  1.0401e-01, -1.1313e-01, -1.5435e-01, -1.9614e-01,\n",
      "         -6.1086e-02, -7.8832e-02,  1.6799e-01,  4.5019e-03,  1.4438e-01,\n",
      "         -4.9225e-02,  1.8970e-01, -1.9160e-01, -3.8827e-02,  1.4495e-01],\n",
      "        [ 2.6220e-02, -7.3164e-02, -7.4938e-02,  1.3762e-01, -1.4474e-02,\n",
      "         -1.8198e-01,  2.2023e-01,  3.2194e-03,  7.5572e-02, -1.4880e-01,\n",
      "         -3.4616e-02,  1.1794e-03, -1.1639e-01,  2.1983e-02, -1.7149e-01,\n",
      "         -1.2558e-01, -1.3026e-01,  1.2530e-01,  1.8777e-01, -1.6388e-01],\n",
      "        [ 1.7676e-01,  1.1656e-01, -9.8444e-02, -2.2269e-01, -1.5693e-01,\n",
      "         -2.3101e-03, -1.1684e-01, -1.9401e-01, -1.9580e-01, -1.3289e-01,\n",
      "          1.1650e-01,  2.2002e-01, -3.0929e-02,  3.7738e-03, -1.9527e-01,\n",
      "          4.6084e-02,  1.3682e-01,  1.2193e-01, -1.7294e-01,  7.9308e-02],\n",
      "        [ 1.7145e-01, -1.7618e-01,  1.4846e-01, -2.1289e-01,  8.3842e-03,\n",
      "          7.4141e-03, -1.6744e-01, -1.5212e-01, -1.1451e-01, -1.5257e-01,\n",
      "         -5.5947e-02, -4.0522e-02,  3.6669e-02, -1.8727e-01,  1.3035e-01,\n",
      "         -3.4736e-02, -1.1898e-01, -1.4627e-01, -9.7430e-02,  5.4031e-02],\n",
      "        [-1.8837e-01, -8.0822e-02, -2.3869e-02, -2.6038e-02, -1.4506e-01,\n",
      "         -3.8000e-02,  6.3100e-02, -3.6456e-02,  9.7964e-02,  1.7990e-01,\n",
      "          2.3571e-02, -1.3257e-01,  9.7908e-02, -1.0477e-01, -1.3535e-02,\n",
      "          1.4785e-02,  9.7818e-02, -1.4328e-01,  6.5690e-02, -2.9720e-02],\n",
      "        [-2.1135e-01,  1.9588e-01, -5.4321e-02,  3.5687e-02,  7.9689e-02,\n",
      "         -1.1026e-02, -1.3695e-01, -5.3887e-02, -9.1589e-02, -1.6388e-01,\n",
      "         -9.3253e-03,  5.7775e-02, -6.2712e-02,  7.1338e-02, -1.8357e-01,\n",
      "          7.8335e-02, -1.7321e-02,  1.7799e-01,  1.9140e-02, -8.0765e-02],\n",
      "        [-1.1376e-02, -1.1422e-03,  1.1825e-01,  9.9282e-02,  1.2991e-02,\n",
      "         -3.2998e-02, -8.2969e-02, -1.8938e-01, -7.9798e-02,  1.4433e-01,\n",
      "          1.4542e-01,  9.2342e-03,  1.5960e-01, -1.1347e-01,  1.0423e-01,\n",
      "          1.4090e-01, -1.6505e-01, -1.2925e-01,  9.8238e-02,  3.7404e-02],\n",
      "        [ 1.8243e-01,  1.3107e-01, -1.2600e-01, -1.3620e-01, -1.9624e-01,\n",
      "          6.0155e-02,  1.9472e-01,  1.2071e-01,  5.5896e-02,  1.4114e-01,\n",
      "         -7.0491e-02, -1.7323e-01, -6.8955e-02, -9.6756e-02, -8.8075e-02,\n",
      "          3.9518e-02, -1.4397e-01,  8.3032e-02, -1.0166e-01, -1.5374e-01],\n",
      "        [ 1.5365e-01, -1.9569e-01,  4.4037e-02, -1.1053e-01,  1.0052e-01,\n",
      "          1.8392e-02, -6.1287e-02, -5.2201e-02, -1.6293e-01, -1.3254e-01,\n",
      "         -1.3018e-01, -1.9479e-01, -1.8497e-01, -2.2119e-01,  1.1921e-01,\n",
      "          2.1764e-01,  4.5408e-04, -9.3733e-02,  1.4101e-01, -1.4948e-03],\n",
      "        [ 5.3097e-02,  4.2267e-02, -1.6018e-01, -2.0048e-01, -8.2736e-03,\n",
      "          6.3836e-02, -2.1288e-01,  1.6941e-01,  2.0214e-01,  1.6709e-01,\n",
      "         -1.7785e-01,  1.3529e-01,  1.7895e-01,  2.0960e-01,  1.1209e-01,\n",
      "         -2.1319e-01,  1.2824e-01, -1.1792e-01, -1.9519e-01,  1.1435e-01],\n",
      "        [-1.4017e-01,  8.7599e-02,  1.0337e-01, -2.2314e-01,  1.6178e-01,\n",
      "          7.9312e-02,  1.1638e-01, -3.4251e-02,  1.6313e-01, -9.1549e-02,\n",
      "         -1.7140e-01,  8.9665e-02,  1.1535e-01, -3.7622e-02, -1.6558e-01,\n",
      "          3.8773e-02, -1.9039e-01,  5.3353e-02, -5.9278e-02, -1.9148e-01],\n",
      "        [ 5.7514e-02, -3.6572e-02, -1.5269e-01, -1.4507e-01, -3.6462e-02,\n",
      "         -1.8254e-01, -3.4163e-02, -1.9610e-01, -1.9212e-01, -1.6064e-01,\n",
      "          1.0935e-01,  2.0246e-01,  1.4539e-01, -1.6359e-01,  7.6983e-02,\n",
      "          2.1944e-01,  1.9997e-01,  1.2847e-01,  1.6022e-02,  2.1007e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1014,  0.0499, -0.0332, -0.1357, -0.0271, -0.1981, -0.0712,  0.0931,\n",
      "         0.0931, -0.1353, -0.0453,  0.1454, -0.1583, -0.0061, -0.0155,  0.2172,\n",
      "         0.1000,  0.2085, -0.0638,  0.2108], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.2161e-01,  7.3952e-02, -8.9702e-02, -1.9305e-01,  6.7043e-02,\n",
      "         -1.0515e-01,  2.5652e-03,  1.9094e-01, -6.8456e-02,  2.1206e-01,\n",
      "         -1.2347e-01,  1.1398e-01, -2.0649e-01, -1.3840e-01, -1.1846e-02,\n",
      "          7.7794e-02, -1.9788e-01,  1.0326e-01,  1.1287e-01,  1.0760e-01],\n",
      "        [ 5.1264e-02,  8.9671e-02,  1.6315e-01,  1.8926e-01,  2.1458e-01,\n",
      "          2.1966e-01,  2.8711e-02, -1.1952e-01, -2.1371e-01,  7.6009e-02,\n",
      "         -1.5581e-01,  8.1518e-02,  1.6197e-01,  1.8215e-01, -1.5784e-01,\n",
      "          4.5892e-02, -1.6906e-01,  5.4798e-02,  1.3344e-01,  1.2024e-01],\n",
      "        [ 2.0217e-01,  3.0170e-02, -1.3772e-01, -5.8198e-02, -9.0062e-02,\n",
      "          5.5808e-02, -7.1702e-02, -3.0938e-02, -4.4865e-03,  6.3170e-02,\n",
      "         -2.2232e-01,  1.0723e-01,  1.1725e-01,  1.9721e-02, -2.1951e-01,\n",
      "          4.0509e-02,  9.3867e-02,  1.0192e-01, -1.3033e-02,  7.7099e-02],\n",
      "        [-2.1653e-02,  1.0590e-01, -1.6445e-01,  9.8841e-02,  1.0876e-01,\n",
      "          7.7977e-02, -6.2818e-02,  5.9617e-02,  2.2079e-02,  6.9427e-02,\n",
      "         -9.1253e-02, -1.4489e-01,  1.9718e-01,  1.8125e-01,  9.3241e-02,\n",
      "         -1.7755e-01,  1.2528e-01, -1.0084e-01, -1.6563e-01, -3.8078e-03],\n",
      "        [-1.7708e-01, -2.1760e-01, -7.8764e-02, -7.0947e-03,  1.9893e-01,\n",
      "          4.2354e-03, -2.0376e-01,  1.0543e-01, -1.2448e-01, -9.7282e-03,\n",
      "         -2.9356e-02,  1.9154e-01,  1.0569e-01, -2.1192e-01, -8.7369e-02,\n",
      "         -1.5677e-01,  9.9987e-02, -5.4818e-02, -1.7907e-01,  1.9116e-01],\n",
      "        [ 1.9950e-04, -1.8559e-01, -6.9188e-02,  2.1030e-01,  1.6926e-01,\n",
      "         -4.3045e-02,  8.1254e-02,  1.1990e-01,  6.1515e-03, -3.4872e-02,\n",
      "         -1.3700e-01,  5.7781e-02, -7.3871e-02,  1.7069e-01,  1.5376e-01,\n",
      "         -1.9261e-01,  7.4790e-02, -1.7036e-01,  7.4321e-02,  8.1704e-02],\n",
      "        [ 3.5308e-02, -2.2298e-01, -1.0107e-01, -4.1823e-02,  1.2587e-01,\n",
      "         -6.8018e-02,  1.7166e-01, -6.9868e-02,  7.8099e-02,  1.7429e-01,\n",
      "          1.6087e-01, -1.2818e-02,  8.4105e-02, -9.5638e-03, -1.8646e-01,\n",
      "          4.9389e-02,  2.0487e-01,  1.0383e-01,  5.8115e-06, -2.0490e-01],\n",
      "        [-1.7155e-01, -7.8063e-02, -2.1775e-01, -1.5356e-02, -8.6297e-02,\n",
      "         -9.2288e-02, -2.5643e-02,  1.5126e-01, -1.3264e-02,  1.7301e-01,\n",
      "          7.9363e-02,  1.4483e-01,  1.9145e-01,  1.3797e-01, -1.5744e-01,\n",
      "         -1.3453e-01, -5.0562e-02, -1.5716e-02,  9.2397e-02, -2.2245e-01],\n",
      "        [-1.1161e-01, -1.4078e-01,  7.7677e-02, -1.8292e-01,  9.1994e-02,\n",
      "          1.4760e-02, -1.1050e-01, -1.8297e-01, -2.0702e-01, -1.5798e-01,\n",
      "         -2.1937e-01, -9.1496e-02,  1.7622e-03,  1.7632e-01, -8.1938e-02,\n",
      "         -2.1824e-01, -1.6897e-01,  9.7384e-02, -9.1624e-03,  5.9740e-02],\n",
      "        [-1.9465e-01,  5.5728e-02,  2.2950e-02, -1.7287e-01, -1.8818e-01,\n",
      "         -1.2135e-01, -2.0939e-01,  1.1582e-01, -2.0225e-01,  2.1680e-01,\n",
      "          5.2658e-02, -8.7424e-02,  1.5443e-01,  6.1933e-02,  2.1176e-01,\n",
      "         -1.9603e-01, -1.9611e-01, -1.5398e-01, -1.0928e-01, -9.1768e-02],\n",
      "        [-2.7914e-02, -1.4528e-01,  4.2455e-02,  9.2771e-02, -2.2619e-02,\n",
      "         -9.9806e-02, -2.2092e-01,  1.4152e-01, -1.1026e-01,  1.2729e-01,\n",
      "          1.3616e-03,  1.0871e-01,  1.4460e-01, -1.9317e-01,  6.7073e-02,\n",
      "         -1.4708e-01,  1.1126e-01,  2.0782e-01, -1.0715e-01, -8.6336e-02],\n",
      "        [-2.1647e-01,  9.9909e-02,  2.0035e-01, -1.3697e-01,  7.7717e-02,\n",
      "         -2.0566e-01,  1.2906e-01,  1.9525e-01,  2.0639e-01,  1.1852e-01,\n",
      "         -1.6646e-01, -1.7392e-01, -1.4948e-01, -6.5396e-02, -1.2592e-01,\n",
      "         -1.8898e-01,  2.0443e-01, -8.5892e-02,  1.9146e-02, -7.2750e-02],\n",
      "        [ 3.9344e-02,  6.0335e-02, -5.5213e-02,  1.5725e-02, -9.8298e-02,\n",
      "          1.9572e-01,  2.9448e-02,  5.5822e-02,  4.5758e-02, -3.4661e-02,\n",
      "          1.9395e-01,  8.4504e-02, -1.3954e-02, -1.7187e-01,  2.5832e-02,\n",
      "         -1.6303e-01,  8.7092e-03,  1.2362e-01, -2.1814e-01,  1.0418e-01],\n",
      "        [ 1.3176e-02,  4.9773e-02, -2.2321e-01, -1.4094e-01, -2.0045e-01,\n",
      "         -9.8845e-02, -8.5236e-02, -3.9334e-02, -2.2211e-01, -4.0308e-02,\n",
      "          1.6595e-01,  2.2244e-01, -1.1861e-01,  1.1246e-01, -3.4260e-02,\n",
      "          1.3215e-01, -1.0325e-01, -2.2260e-01,  5.5649e-02,  1.8468e-01],\n",
      "        [ 1.8052e-01,  1.2281e-01, -2.0056e-01,  8.3657e-04,  7.6868e-02,\n",
      "         -1.8364e-01, -8.7418e-02, -3.6736e-02, -5.8143e-02,  7.7557e-02,\n",
      "          4.5829e-02, -9.9557e-02,  2.0422e-01, -3.3338e-02,  1.2400e-01,\n",
      "          9.4648e-02, -2.1937e-01,  3.3534e-02, -1.3918e-01, -1.7363e-01],\n",
      "        [-1.3203e-01,  1.1667e-02, -4.6664e-02,  1.0677e-01,  1.8986e-01,\n",
      "         -1.4607e-01, -4.1312e-02, -4.1883e-02, -2.0755e-01,  4.3149e-02,\n",
      "          4.0864e-03,  2.3969e-02,  1.3375e-02, -1.6545e-02, -2.3617e-02,\n",
      "         -9.9530e-02,  1.3844e-01, -8.1766e-02, -9.9511e-02,  1.0167e-02],\n",
      "        [ 1.3244e-01,  3.0581e-02, -4.9801e-02,  1.8218e-01,  1.8110e-01,\n",
      "          5.6189e-02, -9.9832e-02, -1.8020e-01, -2.4533e-02,  5.2507e-02,\n",
      "          2.4527e-03, -1.6787e-02,  1.3671e-02, -8.5540e-02, -8.3157e-02,\n",
      "          2.1515e-01, -1.3059e-01, -2.2017e-02, -1.3781e-01,  2.4864e-02],\n",
      "        [ 2.1337e-01,  1.2914e-01,  1.7610e-02, -2.7635e-02, -2.6932e-02,\n",
      "          9.5655e-02, -1.9089e-01, -7.3102e-02,  1.7611e-03,  7.3276e-02,\n",
      "         -6.8091e-02,  1.1474e-01, -1.3743e-01, -2.0253e-01,  9.4104e-02,\n",
      "          8.3583e-02, -1.5623e-01, -6.8634e-02, -1.1528e-01, -2.0293e-01],\n",
      "        [ 1.3431e-01,  1.4368e-01, -8.9038e-02, -1.5685e-01,  7.9958e-02,\n",
      "          6.6997e-02, -9.9663e-02, -1.1901e-01,  1.1131e-01,  3.5022e-02,\n",
      "         -4.1043e-02, -9.2422e-02,  7.7823e-02, -8.3122e-03,  9.8252e-02,\n",
      "          9.0480e-02,  2.1679e-01, -1.3494e-01, -2.0531e-01,  1.0876e-01],\n",
      "        [ 1.8281e-01,  9.2893e-02, -6.1049e-02, -1.0791e-01, -1.8273e-02,\n",
      "          7.5605e-02,  1.6459e-01,  1.3964e-01,  1.2599e-01,  1.6953e-02,\n",
      "          1.8238e-01,  2.1407e-01,  8.6564e-02,  2.1501e-01, -1.1101e-01,\n",
      "         -1.4433e-01,  2.0099e-02, -3.4239e-02, -1.3096e-01,  1.4393e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0875, -0.0566,  0.0526,  0.0721, -0.2048,  0.1395, -0.0293, -0.2033,\n",
      "        -0.1152,  0.1199, -0.1852,  0.2207, -0.1541,  0.1442,  0.1182, -0.0053,\n",
      "         0.1791, -0.0468, -0.1299, -0.2189], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0401,  0.0613,  0.0162,  0.1791,  0.0283, -0.0232, -0.0760, -0.1376,\n",
      "         -0.1488,  0.2053,  0.0882, -0.2151, -0.1506, -0.0389,  0.0093,  0.1050,\n",
      "          0.2120, -0.0013,  0.0914, -0.1803],\n",
      "        [-0.1857,  0.1679, -0.0710,  0.2069, -0.1888, -0.1794,  0.0329, -0.0743,\n",
      "          0.1133, -0.0231,  0.1148,  0.1092, -0.0158,  0.1065,  0.1283, -0.1572,\n",
      "         -0.0731, -0.2008, -0.0105,  0.0598],\n",
      "        [ 0.1692,  0.1262, -0.0161,  0.1626,  0.0836,  0.0553, -0.1930,  0.1505,\n",
      "          0.0958,  0.1576,  0.0736,  0.1894,  0.1182, -0.0437, -0.0949,  0.0993,\n",
      "         -0.1485, -0.1425,  0.1189, -0.0352],\n",
      "        [ 0.1763,  0.0576, -0.1882, -0.1430,  0.0085, -0.1060,  0.0037, -0.2113,\n",
      "          0.0755,  0.1367,  0.1984, -0.1157, -0.2174,  0.1201,  0.0302, -0.0926,\n",
      "         -0.0472,  0.2187, -0.0944,  0.2230],\n",
      "        [-0.0533, -0.1766, -0.1506, -0.1198,  0.0497,  0.0425,  0.1258,  0.0195,\n",
      "         -0.1905,  0.1044, -0.1864,  0.0973, -0.1084,  0.1345,  0.0436,  0.0950,\n",
      "         -0.0724,  0.0418,  0.1424, -0.0070]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1920, -0.0177,  0.1864,  0.0910,  0.0425], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0218, -0.3045],\n",
      "        [ 0.2175, -0.6085]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1979, -0.4531], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in obj1.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59iLYeT-NRsG"
   },
   "source": [
    "# RUN for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ekz8oACnb3tA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "====> Epoch: 1 total_train_loss: 1.003043 Total_test_loss: 0.861384 Total_BCE_test_loss: 0.789584 Total_KLD_test_loss: 0.000008 Total_CEP_test_loss: 0.071792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n",
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 total_train_loss: 0.945444 Total_test_loss: 0.858863 Total_BCE_test_loss: 0.788193 Total_KLD_test_loss: 0.000005 Total_CEP_test_loss: 0.070665\n",
      "====> Epoch: 3 total_train_loss: 0.905730 Total_test_loss: 0.859141 Total_BCE_test_loss: 0.787589 Total_KLD_test_loss: 0.000003 Total_CEP_test_loss: 0.071548\n",
      "====> Epoch: 4 total_train_loss: 0.891592 Total_test_loss: 0.859819 Total_BCE_test_loss: 0.787736 Total_KLD_test_loss: 0.000004 Total_CEP_test_loss: 0.072078\n",
      "====> Epoch: 5 total_train_loss: 0.886691 Total_test_loss: 0.859672 Total_BCE_test_loss: 0.788107 Total_KLD_test_loss: 0.000011 Total_CEP_test_loss: 0.071554\n",
      "====> Epoch: 6 total_train_loss: 0.879897 Total_test_loss: 0.857871 Total_BCE_test_loss: 0.785951 Total_KLD_test_loss: 0.000024 Total_CEP_test_loss: 0.071896\n",
      "====> Epoch: 7 total_train_loss: 0.873857 Total_test_loss: 0.854008 Total_BCE_test_loss: 0.783077 Total_KLD_test_loss: 0.000046 Total_CEP_test_loss: 0.070884\n",
      "====> Epoch: 8 total_train_loss: 0.879096 Total_test_loss: 0.848122 Total_BCE_test_loss: 0.776764 Total_KLD_test_loss: 0.000076 Total_CEP_test_loss: 0.071283\n",
      "====> Epoch: 9 total_train_loss: 0.871832 Total_test_loss: 0.843524 Total_BCE_test_loss: 0.771984 Total_KLD_test_loss: 0.000109 Total_CEP_test_loss: 0.071431\n",
      "====> Epoch: 10 total_train_loss: 0.863653 Total_test_loss: 0.842460 Total_BCE_test_loss: 0.770634 Total_KLD_test_loss: 0.000143 Total_CEP_test_loss: 0.071683\n",
      "====> Epoch: 11 total_train_loss: 0.863480 Total_test_loss: 0.834703 Total_BCE_test_loss: 0.762959 Total_KLD_test_loss: 0.000174 Total_CEP_test_loss: 0.071570\n",
      "====> Epoch: 12 total_train_loss: 0.861153 Total_test_loss: 0.831346 Total_BCE_test_loss: 0.760454 Total_KLD_test_loss: 0.000192 Total_CEP_test_loss: 0.070700\n",
      "====> Epoch: 13 total_train_loss: 0.854159 Total_test_loss: 0.826971 Total_BCE_test_loss: 0.754945 Total_KLD_test_loss: 0.000208 Total_CEP_test_loss: 0.071817\n",
      "====> Epoch: 14 total_train_loss: 0.855086 Total_test_loss: 0.822289 Total_BCE_test_loss: 0.750532 Total_KLD_test_loss: 0.000225 Total_CEP_test_loss: 0.071531\n",
      "====> Epoch: 15 total_train_loss: 0.839237 Total_test_loss: 0.816965 Total_BCE_test_loss: 0.745579 Total_KLD_test_loss: 0.000261 Total_CEP_test_loss: 0.071125\n",
      "====> Epoch: 16 total_train_loss: 0.846296 Total_test_loss: 0.814407 Total_BCE_test_loss: 0.742735 Total_KLD_test_loss: 0.000303 Total_CEP_test_loss: 0.071369\n",
      "====> Epoch: 17 total_train_loss: 0.831325 Total_test_loss: 0.806745 Total_BCE_test_loss: 0.735743 Total_KLD_test_loss: 0.000340 Total_CEP_test_loss: 0.070663\n",
      "====> Epoch: 18 total_train_loss: 0.826902 Total_test_loss: 0.800386 Total_BCE_test_loss: 0.729212 Total_KLD_test_loss: 0.000384 Total_CEP_test_loss: 0.070791\n",
      "====> Epoch: 19 total_train_loss: 0.816873 Total_test_loss: 0.793980 Total_BCE_test_loss: 0.723129 Total_KLD_test_loss: 0.000434 Total_CEP_test_loss: 0.070417\n",
      "====> Epoch: 20 total_train_loss: 0.815182 Total_test_loss: 0.789084 Total_BCE_test_loss: 0.718689 Total_KLD_test_loss: 0.000477 Total_CEP_test_loss: 0.069918\n",
      "====> Epoch: 21 total_train_loss: 0.808533 Total_test_loss: 0.779004 Total_BCE_test_loss: 0.708386 Total_KLD_test_loss: 0.000526 Total_CEP_test_loss: 0.070092\n",
      "====> Epoch: 22 total_train_loss: 0.797645 Total_test_loss: 0.770515 Total_BCE_test_loss: 0.699690 Total_KLD_test_loss: 0.000558 Total_CEP_test_loss: 0.070267\n",
      "====> Epoch: 23 total_train_loss: 0.797559 Total_test_loss: 0.767665 Total_BCE_test_loss: 0.697219 Total_KLD_test_loss: 0.000595 Total_CEP_test_loss: 0.069851\n",
      "====> Epoch: 24 total_train_loss: 0.790348 Total_test_loss: 0.761890 Total_BCE_test_loss: 0.691702 Total_KLD_test_loss: 0.000635 Total_CEP_test_loss: 0.069553\n",
      "====> Epoch: 25 total_train_loss: 0.787816 Total_test_loss: 0.757433 Total_BCE_test_loss: 0.687108 Total_KLD_test_loss: 0.000679 Total_CEP_test_loss: 0.069646\n",
      "====> Epoch: 26 total_train_loss: 0.783183 Total_test_loss: 0.752764 Total_BCE_test_loss: 0.682489 Total_KLD_test_loss: 0.000710 Total_CEP_test_loss: 0.069565\n",
      "====> Epoch: 27 total_train_loss: 0.769818 Total_test_loss: 0.740040 Total_BCE_test_loss: 0.669898 Total_KLD_test_loss: 0.000756 Total_CEP_test_loss: 0.069386\n",
      "====> Epoch: 28 total_train_loss: 0.762865 Total_test_loss: 0.734667 Total_BCE_test_loss: 0.664395 Total_KLD_test_loss: 0.000815 Total_CEP_test_loss: 0.069457\n",
      "====> Epoch: 29 total_train_loss: 0.759850 Total_test_loss: 0.732803 Total_BCE_test_loss: 0.662493 Total_KLD_test_loss: 0.000880 Total_CEP_test_loss: 0.069430\n",
      "====> Epoch: 30 total_train_loss: 0.749141 Total_test_loss: 0.731284 Total_BCE_test_loss: 0.660857 Total_KLD_test_loss: 0.000947 Total_CEP_test_loss: 0.069479\n",
      "====> Epoch: 31 total_train_loss: 0.750807 Total_test_loss: 0.731106 Total_BCE_test_loss: 0.660701 Total_KLD_test_loss: 0.001013 Total_CEP_test_loss: 0.069392\n",
      "====> Epoch: 32 total_train_loss: 0.749507 Total_test_loss: 0.724531 Total_BCE_test_loss: 0.654123 Total_KLD_test_loss: 0.001076 Total_CEP_test_loss: 0.069333\n",
      "====> Epoch: 33 total_train_loss: 0.742149 Total_test_loss: 0.716517 Total_BCE_test_loss: 0.646049 Total_KLD_test_loss: 0.001133 Total_CEP_test_loss: 0.069335\n",
      "====> Epoch: 34 total_train_loss: 0.736893 Total_test_loss: 0.708933 Total_BCE_test_loss: 0.638443 Total_KLD_test_loss: 0.001191 Total_CEP_test_loss: 0.069299\n",
      "====> Epoch: 35 total_train_loss: 0.731368 Total_test_loss: 0.705829 Total_BCE_test_loss: 0.635296 Total_KLD_test_loss: 0.001241 Total_CEP_test_loss: 0.069292\n",
      "====> Epoch: 36 total_train_loss: 0.732565 Total_test_loss: 0.699832 Total_BCE_test_loss: 0.629217 Total_KLD_test_loss: 0.001311 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 37 total_train_loss: 0.724967 Total_test_loss: 0.698834 Total_BCE_test_loss: 0.628169 Total_KLD_test_loss: 0.001395 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 38 total_train_loss: 0.727012 Total_test_loss: 0.697622 Total_BCE_test_loss: 0.626874 Total_KLD_test_loss: 0.001473 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 39 total_train_loss: 0.721927 Total_test_loss: 0.694753 Total_BCE_test_loss: 0.623928 Total_KLD_test_loss: 0.001522 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 40 total_train_loss: 0.719102 Total_test_loss: 0.690841 Total_BCE_test_loss: 0.619959 Total_KLD_test_loss: 0.001588 Total_CEP_test_loss: 0.069294\n",
      "====> Epoch: 41 total_train_loss: 0.716604 Total_test_loss: 0.692079 Total_BCE_test_loss: 0.621201 Total_KLD_test_loss: 0.001591 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 42 total_train_loss: 0.712185 Total_test_loss: 0.690620 Total_BCE_test_loss: 0.619719 Total_KLD_test_loss: 0.001614 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 43 total_train_loss: 0.711195 Total_test_loss: 0.687322 Total_BCE_test_loss: 0.616347 Total_KLD_test_loss: 0.001683 Total_CEP_test_loss: 0.069292\n",
      "====> Epoch: 44 total_train_loss: 0.717375 Total_test_loss: 0.689378 Total_BCE_test_loss: 0.618385 Total_KLD_test_loss: 0.001737 Total_CEP_test_loss: 0.069255\n",
      "====> Epoch: 45 total_train_loss: 0.713068 Total_test_loss: 0.684097 Total_BCE_test_loss: 0.612996 Total_KLD_test_loss: 0.001752 Total_CEP_test_loss: 0.069349\n",
      "====> Epoch: 46 total_train_loss: 0.711577 Total_test_loss: 0.682241 Total_BCE_test_loss: 0.611209 Total_KLD_test_loss: 0.001756 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 47 total_train_loss: 0.708009 Total_test_loss: 0.678635 Total_BCE_test_loss: 0.607579 Total_KLD_test_loss: 0.001795 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 48 total_train_loss: 0.708004 Total_test_loss: 0.676422 Total_BCE_test_loss: 0.605333 Total_KLD_test_loss: 0.001850 Total_CEP_test_loss: 0.069239\n",
      "====> Epoch: 49 total_train_loss: 0.706161 Total_test_loss: 0.674645 Total_BCE_test_loss: 0.603473 Total_KLD_test_loss: 0.001885 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 50 total_train_loss: 0.704887 Total_test_loss: 0.673434 Total_BCE_test_loss: 0.602210 Total_KLD_test_loss: 0.001912 Total_CEP_test_loss: 0.069313\n",
      "====> Epoch: 51 total_train_loss: 0.702433 Total_test_loss: 0.672967 Total_BCE_test_loss: 0.601705 Total_KLD_test_loss: 0.001919 Total_CEP_test_loss: 0.069344\n",
      "====> Epoch: 52 total_train_loss: 0.697620 Total_test_loss: 0.669094 Total_BCE_test_loss: 0.597788 Total_KLD_test_loss: 0.001934 Total_CEP_test_loss: 0.069372\n",
      "====> Epoch: 53 total_train_loss: 0.702427 Total_test_loss: 0.666905 Total_BCE_test_loss: 0.595564 Total_KLD_test_loss: 0.001961 Total_CEP_test_loss: 0.069380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 54 total_train_loss: 0.703826 Total_test_loss: 0.667410 Total_BCE_test_loss: 0.596103 Total_KLD_test_loss: 0.001980 Total_CEP_test_loss: 0.069327\n",
      "====> Epoch: 55 total_train_loss: 0.693999 Total_test_loss: 0.663751 Total_BCE_test_loss: 0.592477 Total_KLD_test_loss: 0.002014 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 56 total_train_loss: 0.696412 Total_test_loss: 0.660103 Total_BCE_test_loss: 0.588854 Total_KLD_test_loss: 0.002026 Total_CEP_test_loss: 0.069223\n",
      "====> Epoch: 57 total_train_loss: 0.693126 Total_test_loss: 0.660357 Total_BCE_test_loss: 0.589098 Total_KLD_test_loss: 0.002045 Total_CEP_test_loss: 0.069214\n",
      "====> Epoch: 58 total_train_loss: 0.696742 Total_test_loss: 0.658088 Total_BCE_test_loss: 0.586780 Total_KLD_test_loss: 0.002062 Total_CEP_test_loss: 0.069246\n",
      "====> Epoch: 59 total_train_loss: 0.690401 Total_test_loss: 0.655209 Total_BCE_test_loss: 0.583859 Total_KLD_test_loss: 0.002095 Total_CEP_test_loss: 0.069255\n",
      "====> Epoch: 60 total_train_loss: 0.684791 Total_test_loss: 0.653032 Total_BCE_test_loss: 0.581644 Total_KLD_test_loss: 0.002133 Total_CEP_test_loss: 0.069255\n",
      "====> Epoch: 61 total_train_loss: 0.691613 Total_test_loss: 0.651367 Total_BCE_test_loss: 0.579975 Total_KLD_test_loss: 0.002131 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 62 total_train_loss: 0.684803 Total_test_loss: 0.649609 Total_BCE_test_loss: 0.578201 Total_KLD_test_loss: 0.002138 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 63 total_train_loss: 0.685792 Total_test_loss: 0.650471 Total_BCE_test_loss: 0.578978 Total_KLD_test_loss: 0.002196 Total_CEP_test_loss: 0.069297\n",
      "====> Epoch: 64 total_train_loss: 0.685500 Total_test_loss: 0.648214 Total_BCE_test_loss: 0.576686 Total_KLD_test_loss: 0.002218 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 65 total_train_loss: 0.684871 Total_test_loss: 0.642314 Total_BCE_test_loss: 0.570783 Total_KLD_test_loss: 0.002189 Total_CEP_test_loss: 0.069343\n",
      "====> Epoch: 66 total_train_loss: 0.683978 Total_test_loss: 0.642112 Total_BCE_test_loss: 0.570557 Total_KLD_test_loss: 0.002219 Total_CEP_test_loss: 0.069336\n",
      "====> Epoch: 67 total_train_loss: 0.681429 Total_test_loss: 0.641346 Total_BCE_test_loss: 0.569769 Total_KLD_test_loss: 0.002263 Total_CEP_test_loss: 0.069315\n",
      "====> Epoch: 68 total_train_loss: 0.676466 Total_test_loss: 0.637166 Total_BCE_test_loss: 0.565544 Total_KLD_test_loss: 0.002328 Total_CEP_test_loss: 0.069294\n",
      "====> Epoch: 69 total_train_loss: 0.677405 Total_test_loss: 0.640306 Total_BCE_test_loss: 0.568702 Total_KLD_test_loss: 0.002326 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 70 total_train_loss: 0.683856 Total_test_loss: 0.639306 Total_BCE_test_loss: 0.567657 Total_KLD_test_loss: 0.002379 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 71 total_train_loss: 0.680333 Total_test_loss: 0.630352 Total_BCE_test_loss: 0.558646 Total_KLD_test_loss: 0.002436 Total_CEP_test_loss: 0.069271\n",
      "====> Epoch: 72 total_train_loss: 0.672403 Total_test_loss: 0.632088 Total_BCE_test_loss: 0.560361 Total_KLD_test_loss: 0.002443 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 73 total_train_loss: 0.675065 Total_test_loss: 0.629222 Total_BCE_test_loss: 0.557438 Total_KLD_test_loss: 0.002500 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 74 total_train_loss: 0.671988 Total_test_loss: 0.625490 Total_BCE_test_loss: 0.553625 Total_KLD_test_loss: 0.002579 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 75 total_train_loss: 0.673406 Total_test_loss: 0.624638 Total_BCE_test_loss: 0.552701 Total_KLD_test_loss: 0.002635 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 76 total_train_loss: 0.669826 Total_test_loss: 0.620290 Total_BCE_test_loss: 0.548307 Total_KLD_test_loss: 0.002649 Total_CEP_test_loss: 0.069334\n",
      "====> Epoch: 77 total_train_loss: 0.666610 Total_test_loss: 0.618772 Total_BCE_test_loss: 0.546828 Total_KLD_test_loss: 0.002616 Total_CEP_test_loss: 0.069328\n",
      "====> Epoch: 78 total_train_loss: 0.665922 Total_test_loss: 0.616423 Total_BCE_test_loss: 0.544517 Total_KLD_test_loss: 0.002575 Total_CEP_test_loss: 0.069331\n",
      "====> Epoch: 79 total_train_loss: 0.665562 Total_test_loss: 0.612799 Total_BCE_test_loss: 0.540849 Total_KLD_test_loss: 0.002573 Total_CEP_test_loss: 0.069376\n",
      "====> Epoch: 80 total_train_loss: 0.657109 Total_test_loss: 0.611840 Total_BCE_test_loss: 0.539875 Total_KLD_test_loss: 0.002558 Total_CEP_test_loss: 0.069407\n",
      "====> Epoch: 81 total_train_loss: 0.664829 Total_test_loss: 0.612314 Total_BCE_test_loss: 0.540433 Total_KLD_test_loss: 0.002561 Total_CEP_test_loss: 0.069320\n",
      "====> Epoch: 82 total_train_loss: 0.657494 Total_test_loss: 0.609596 Total_BCE_test_loss: 0.537731 Total_KLD_test_loss: 0.002578 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 83 total_train_loss: 0.656999 Total_test_loss: 0.606084 Total_BCE_test_loss: 0.534099 Total_KLD_test_loss: 0.002612 Total_CEP_test_loss: 0.069374\n",
      "====> Epoch: 84 total_train_loss: 0.658429 Total_test_loss: 0.603917 Total_BCE_test_loss: 0.531876 Total_KLD_test_loss: 0.002622 Total_CEP_test_loss: 0.069420\n",
      "====> Epoch: 85 total_train_loss: 0.660537 Total_test_loss: 0.603174 Total_BCE_test_loss: 0.531112 Total_KLD_test_loss: 0.002671 Total_CEP_test_loss: 0.069391\n",
      "====> Epoch: 86 total_train_loss: 0.656208 Total_test_loss: 0.607404 Total_BCE_test_loss: 0.535390 Total_KLD_test_loss: 0.002702 Total_CEP_test_loss: 0.069312\n",
      "====> Epoch: 87 total_train_loss: 0.659135 Total_test_loss: 0.608100 Total_BCE_test_loss: 0.536123 Total_KLD_test_loss: 0.002703 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 88 total_train_loss: 0.662528 Total_test_loss: 0.608660 Total_BCE_test_loss: 0.536687 Total_KLD_test_loss: 0.002697 Total_CEP_test_loss: 0.069276\n",
      "====> Epoch: 89 total_train_loss: 0.656756 Total_test_loss: 0.614627 Total_BCE_test_loss: 0.542637 Total_KLD_test_loss: 0.002726 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 90 total_train_loss: 0.659074 Total_test_loss: 0.608020 Total_BCE_test_loss: 0.536047 Total_KLD_test_loss: 0.002739 Total_CEP_test_loss: 0.069235\n",
      "====> Epoch: 91 total_train_loss: 0.655823 Total_test_loss: 0.605976 Total_BCE_test_loss: 0.533910 Total_KLD_test_loss: 0.002741 Total_CEP_test_loss: 0.069325\n",
      "====> Epoch: 92 total_train_loss: 0.657342 Total_test_loss: 0.610663 Total_BCE_test_loss: 0.538506 Total_KLD_test_loss: 0.002773 Total_CEP_test_loss: 0.069385\n",
      "====> Epoch: 93 total_train_loss: 0.656150 Total_test_loss: 0.596187 Total_BCE_test_loss: 0.524084 Total_KLD_test_loss: 0.002769 Total_CEP_test_loss: 0.069334\n",
      "====> Epoch: 94 total_train_loss: 0.651389 Total_test_loss: 0.594518 Total_BCE_test_loss: 0.522448 Total_KLD_test_loss: 0.002800 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 95 total_train_loss: 0.650788 Total_test_loss: 0.591563 Total_BCE_test_loss: 0.519492 Total_KLD_test_loss: 0.002810 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 96 total_train_loss: 0.644340 Total_test_loss: 0.590939 Total_BCE_test_loss: 0.518912 Total_KLD_test_loss: 0.002773 Total_CEP_test_loss: 0.069253\n",
      "====> Epoch: 97 total_train_loss: 0.644496 Total_test_loss: 0.590228 Total_BCE_test_loss: 0.518236 Total_KLD_test_loss: 0.002745 Total_CEP_test_loss: 0.069247\n",
      "====> Epoch: 98 total_train_loss: 0.651728 Total_test_loss: 0.590024 Total_BCE_test_loss: 0.518040 Total_KLD_test_loss: 0.002703 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 99 total_train_loss: 0.644586 Total_test_loss: 0.600714 Total_BCE_test_loss: 0.528751 Total_KLD_test_loss: 0.002641 Total_CEP_test_loss: 0.069322\n",
      "====> Epoch: 100 total_train_loss: 0.646229 Total_test_loss: 0.596672 Total_BCE_test_loss: 0.524681 Total_KLD_test_loss: 0.002653 Total_CEP_test_loss: 0.069339\n",
      "0.001\n",
      "0.001\n",
      "model saved\n",
      "0.001\n",
      "0.0005\n",
      "====> Epoch: 1 total_train_loss: 0.642323 Total_test_loss: 0.589351 Total_BCE_test_loss: 0.517364 Total_KLD_test_loss: 0.002658 Total_CEP_test_loss: 0.069330\n",
      "====> Epoch: 2 total_train_loss: 0.642241 Total_test_loss: 0.586831 Total_BCE_test_loss: 0.514838 Total_KLD_test_loss: 0.002674 Total_CEP_test_loss: 0.069319\n",
      "====> Epoch: 3 total_train_loss: 0.643302 Total_test_loss: 0.585815 Total_BCE_test_loss: 0.513797 Total_KLD_test_loss: 0.002700 Total_CEP_test_loss: 0.069318\n",
      "====> Epoch: 4 total_train_loss: 0.644306 Total_test_loss: 0.584325 Total_BCE_test_loss: 0.512283 Total_KLD_test_loss: 0.002729 Total_CEP_test_loss: 0.069313\n",
      "====> Epoch: 5 total_train_loss: 0.641546 Total_test_loss: 0.583535 Total_BCE_test_loss: 0.511486 Total_KLD_test_loss: 0.002737 Total_CEP_test_loss: 0.069312\n",
      "====> Epoch: 6 total_train_loss: 0.639621 Total_test_loss: 0.582764 Total_BCE_test_loss: 0.510723 Total_KLD_test_loss: 0.002729 Total_CEP_test_loss: 0.069312\n",
      "====> Epoch: 7 total_train_loss: 0.640284 Total_test_loss: 0.583765 Total_BCE_test_loss: 0.511716 Total_KLD_test_loss: 0.002736 Total_CEP_test_loss: 0.069313\n",
      "====> Epoch: 8 total_train_loss: 0.644513 Total_test_loss: 0.582244 Total_BCE_test_loss: 0.510180 Total_KLD_test_loss: 0.002748 Total_CEP_test_loss: 0.069316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 9 total_train_loss: 0.636581 Total_test_loss: 0.582941 Total_BCE_test_loss: 0.510864 Total_KLD_test_loss: 0.002762 Total_CEP_test_loss: 0.069314\n",
      "====> Epoch: 10 total_train_loss: 0.638179 Total_test_loss: 0.581756 Total_BCE_test_loss: 0.509679 Total_KLD_test_loss: 0.002767 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 11 total_train_loss: 0.641211 Total_test_loss: 0.580853 Total_BCE_test_loss: 0.508796 Total_KLD_test_loss: 0.002752 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 12 total_train_loss: 0.641741 Total_test_loss: 0.580737 Total_BCE_test_loss: 0.508658 Total_KLD_test_loss: 0.002782 Total_CEP_test_loss: 0.069297\n",
      "====> Epoch: 13 total_train_loss: 0.641891 Total_test_loss: 0.580403 Total_BCE_test_loss: 0.508345 Total_KLD_test_loss: 0.002767 Total_CEP_test_loss: 0.069292\n",
      "====> Epoch: 14 total_train_loss: 0.636605 Total_test_loss: 0.580291 Total_BCE_test_loss: 0.508256 Total_KLD_test_loss: 0.002745 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 15 total_train_loss: 0.638174 Total_test_loss: 0.580411 Total_BCE_test_loss: 0.508380 Total_KLD_test_loss: 0.002745 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 16 total_train_loss: 0.642028 Total_test_loss: 0.580151 Total_BCE_test_loss: 0.508142 Total_KLD_test_loss: 0.002724 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 17 total_train_loss: 0.633347 Total_test_loss: 0.579403 Total_BCE_test_loss: 0.507421 Total_KLD_test_loss: 0.002702 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 18 total_train_loss: 0.637258 Total_test_loss: 0.579235 Total_BCE_test_loss: 0.507258 Total_KLD_test_loss: 0.002698 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 19 total_train_loss: 0.633907 Total_test_loss: 0.578859 Total_BCE_test_loss: 0.506858 Total_KLD_test_loss: 0.002721 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 20 total_train_loss: 0.637154 Total_test_loss: 0.577549 Total_BCE_test_loss: 0.505491 Total_KLD_test_loss: 0.002778 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 21 total_train_loss: 0.638630 Total_test_loss: 0.577448 Total_BCE_test_loss: 0.505413 Total_KLD_test_loss: 0.002753 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 22 total_train_loss: 0.640027 Total_test_loss: 0.576991 Total_BCE_test_loss: 0.504932 Total_KLD_test_loss: 0.002779 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 23 total_train_loss: 0.637982 Total_test_loss: 0.576385 Total_BCE_test_loss: 0.504336 Total_KLD_test_loss: 0.002771 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 24 total_train_loss: 0.636557 Total_test_loss: 0.576463 Total_BCE_test_loss: 0.504397 Total_KLD_test_loss: 0.002787 Total_CEP_test_loss: 0.069279\n",
      "====> Epoch: 25 total_train_loss: 0.636338 Total_test_loss: 0.575559 Total_BCE_test_loss: 0.503488 Total_KLD_test_loss: 0.002793 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 26 total_train_loss: 0.636888 Total_test_loss: 0.576606 Total_BCE_test_loss: 0.504532 Total_KLD_test_loss: 0.002799 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 27 total_train_loss: 0.636365 Total_test_loss: 0.577738 Total_BCE_test_loss: 0.505704 Total_KLD_test_loss: 0.002765 Total_CEP_test_loss: 0.069270\n",
      "====> Epoch: 28 total_train_loss: 0.638969 Total_test_loss: 0.576761 Total_BCE_test_loss: 0.504739 Total_KLD_test_loss: 0.002757 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 29 total_train_loss: 0.631239 Total_test_loss: 0.575977 Total_BCE_test_loss: 0.503951 Total_KLD_test_loss: 0.002758 Total_CEP_test_loss: 0.069268\n",
      "====> Epoch: 30 total_train_loss: 0.632936 Total_test_loss: 0.575343 Total_BCE_test_loss: 0.503320 Total_KLD_test_loss: 0.002755 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 31 total_train_loss: 0.630098 Total_test_loss: 0.574340 Total_BCE_test_loss: 0.502327 Total_KLD_test_loss: 0.002749 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 32 total_train_loss: 0.637890 Total_test_loss: 0.573885 Total_BCE_test_loss: 0.501838 Total_KLD_test_loss: 0.002780 Total_CEP_test_loss: 0.069267\n",
      "====> Epoch: 33 total_train_loss: 0.634949 Total_test_loss: 0.573559 Total_BCE_test_loss: 0.501520 Total_KLD_test_loss: 0.002775 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 34 total_train_loss: 0.639977 Total_test_loss: 0.574473 Total_BCE_test_loss: 0.502431 Total_KLD_test_loss: 0.002780 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 35 total_train_loss: 0.632072 Total_test_loss: 0.574955 Total_BCE_test_loss: 0.502935 Total_KLD_test_loss: 0.002755 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 36 total_train_loss: 0.633052 Total_test_loss: 0.574750 Total_BCE_test_loss: 0.502741 Total_KLD_test_loss: 0.002746 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 37 total_train_loss: 0.636192 Total_test_loss: 0.573870 Total_BCE_test_loss: 0.501835 Total_KLD_test_loss: 0.002772 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 38 total_train_loss: 0.633764 Total_test_loss: 0.573893 Total_BCE_test_loss: 0.501844 Total_KLD_test_loss: 0.002787 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 39 total_train_loss: 0.634645 Total_test_loss: 0.573510 Total_BCE_test_loss: 0.501434 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 40 total_train_loss: 0.640953 Total_test_loss: 0.573446 Total_BCE_test_loss: 0.501340 Total_KLD_test_loss: 0.002852 Total_CEP_test_loss: 0.069254\n",
      "====> Epoch: 41 total_train_loss: 0.633844 Total_test_loss: 0.573426 Total_BCE_test_loss: 0.501346 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 42 total_train_loss: 0.634810 Total_test_loss: 0.573867 Total_BCE_test_loss: 0.501809 Total_KLD_test_loss: 0.002795 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 43 total_train_loss: 0.632916 Total_test_loss: 0.573394 Total_BCE_test_loss: 0.501326 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069252\n",
      "====> Epoch: 44 total_train_loss: 0.630729 Total_test_loss: 0.574543 Total_BCE_test_loss: 0.502481 Total_KLD_test_loss: 0.002805 Total_CEP_test_loss: 0.069256\n",
      "====> Epoch: 45 total_train_loss: 0.633643 Total_test_loss: 0.573559 Total_BCE_test_loss: 0.501513 Total_KLD_test_loss: 0.002793 Total_CEP_test_loss: 0.069253\n",
      "====> Epoch: 46 total_train_loss: 0.636518 Total_test_loss: 0.573270 Total_BCE_test_loss: 0.501221 Total_KLD_test_loss: 0.002791 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 47 total_train_loss: 0.634528 Total_test_loss: 0.573926 Total_BCE_test_loss: 0.501892 Total_KLD_test_loss: 0.002779 Total_CEP_test_loss: 0.069256\n",
      "====> Epoch: 48 total_train_loss: 0.635413 Total_test_loss: 0.572668 Total_BCE_test_loss: 0.500608 Total_KLD_test_loss: 0.002804 Total_CEP_test_loss: 0.069256\n",
      "====> Epoch: 49 total_train_loss: 0.639327 Total_test_loss: 0.573361 Total_BCE_test_loss: 0.501299 Total_KLD_test_loss: 0.002797 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 50 total_train_loss: 0.633504 Total_test_loss: 0.572502 Total_BCE_test_loss: 0.500420 Total_KLD_test_loss: 0.002818 Total_CEP_test_loss: 0.069264\n",
      "model saved\n",
      "1e-05\n",
      "====> Epoch: 1 total_train_loss: 0.633416 Total_test_loss: 0.572648 Total_BCE_test_loss: 0.500549 Total_KLD_test_loss: 0.002836 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 2 total_train_loss: 0.632679 Total_test_loss: 0.573210 Total_BCE_test_loss: 0.501139 Total_KLD_test_loss: 0.002809 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 3 total_train_loss: 0.634166 Total_test_loss: 0.573183 Total_BCE_test_loss: 0.501102 Total_KLD_test_loss: 0.002821 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 4 total_train_loss: 0.635325 Total_test_loss: 0.573637 Total_BCE_test_loss: 0.501565 Total_KLD_test_loss: 0.002806 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 5 total_train_loss: 0.633839 Total_test_loss: 0.573608 Total_BCE_test_loss: 0.501547 Total_KLD_test_loss: 0.002799 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 6 total_train_loss: 0.636818 Total_test_loss: 0.573130 Total_BCE_test_loss: 0.501054 Total_KLD_test_loss: 0.002814 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 7 total_train_loss: 0.633030 Total_test_loss: 0.572385 Total_BCE_test_loss: 0.500303 Total_KLD_test_loss: 0.002817 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 8 total_train_loss: 0.635449 Total_test_loss: 0.573378 Total_BCE_test_loss: 0.501320 Total_KLD_test_loss: 0.002795 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 9 total_train_loss: 0.633435 Total_test_loss: 0.573642 Total_BCE_test_loss: 0.501588 Total_KLD_test_loss: 0.002794 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 10 total_train_loss: 0.636732 Total_test_loss: 0.572652 Total_BCE_test_loss: 0.500584 Total_KLD_test_loss: 0.002805 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 11 total_train_loss: 0.638576 Total_test_loss: 0.572758 Total_BCE_test_loss: 0.500666 Total_KLD_test_loss: 0.002831 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 12 total_train_loss: 0.634052 Total_test_loss: 0.573151 Total_BCE_test_loss: 0.501065 Total_KLD_test_loss: 0.002825 Total_CEP_test_loss: 0.069261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 13 total_train_loss: 0.635388 Total_test_loss: 0.573540 Total_BCE_test_loss: 0.501458 Total_KLD_test_loss: 0.002823 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 14 total_train_loss: 0.631552 Total_test_loss: 0.573593 Total_BCE_test_loss: 0.501539 Total_KLD_test_loss: 0.002790 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 15 total_train_loss: 0.630506 Total_test_loss: 0.573947 Total_BCE_test_loss: 0.501904 Total_KLD_test_loss: 0.002785 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 16 total_train_loss: 0.635097 Total_test_loss: 0.573337 Total_BCE_test_loss: 0.501285 Total_KLD_test_loss: 0.002788 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 17 total_train_loss: 0.633183 Total_test_loss: 0.573805 Total_BCE_test_loss: 0.501766 Total_KLD_test_loss: 0.002781 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 18 total_train_loss: 0.633649 Total_test_loss: 0.572803 Total_BCE_test_loss: 0.500744 Total_KLD_test_loss: 0.002797 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 19 total_train_loss: 0.632863 Total_test_loss: 0.573087 Total_BCE_test_loss: 0.501028 Total_KLD_test_loss: 0.002794 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 20 total_train_loss: 0.637537 Total_test_loss: 0.573319 Total_BCE_test_loss: 0.501249 Total_KLD_test_loss: 0.002806 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 21 total_train_loss: 0.633484 Total_test_loss: 0.572298 Total_BCE_test_loss: 0.500204 Total_KLD_test_loss: 0.002829 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 22 total_train_loss: 0.631246 Total_test_loss: 0.572816 Total_BCE_test_loss: 0.500735 Total_KLD_test_loss: 0.002821 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 23 total_train_loss: 0.637921 Total_test_loss: 0.573482 Total_BCE_test_loss: 0.501435 Total_KLD_test_loss: 0.002782 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 24 total_train_loss: 0.632504 Total_test_loss: 0.572969 Total_BCE_test_loss: 0.500915 Total_KLD_test_loss: 0.002791 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 25 total_train_loss: 0.634093 Total_test_loss: 0.573075 Total_BCE_test_loss: 0.501025 Total_KLD_test_loss: 0.002787 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 26 total_train_loss: 0.635223 Total_test_loss: 0.573099 Total_BCE_test_loss: 0.501048 Total_KLD_test_loss: 0.002788 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 27 total_train_loss: 0.632308 Total_test_loss: 0.573073 Total_BCE_test_loss: 0.501006 Total_KLD_test_loss: 0.002809 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 28 total_train_loss: 0.636902 Total_test_loss: 0.573640 Total_BCE_test_loss: 0.501584 Total_KLD_test_loss: 0.002790 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 29 total_train_loss: 0.630373 Total_test_loss: 0.573108 Total_BCE_test_loss: 0.501051 Total_KLD_test_loss: 0.002796 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 30 total_train_loss: 0.634203 Total_test_loss: 0.573237 Total_BCE_test_loss: 0.501204 Total_KLD_test_loss: 0.002767 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 31 total_train_loss: 0.636506 Total_test_loss: 0.572856 Total_BCE_test_loss: 0.500797 Total_KLD_test_loss: 0.002801 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 32 total_train_loss: 0.631882 Total_test_loss: 0.573298 Total_BCE_test_loss: 0.501225 Total_KLD_test_loss: 0.002811 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 33 total_train_loss: 0.637314 Total_test_loss: 0.572969 Total_BCE_test_loss: 0.500897 Total_KLD_test_loss: 0.002810 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 34 total_train_loss: 0.633142 Total_test_loss: 0.573175 Total_BCE_test_loss: 0.501096 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 35 total_train_loss: 0.641565 Total_test_loss: 0.572683 Total_BCE_test_loss: 0.500615 Total_KLD_test_loss: 0.002809 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 36 total_train_loss: 0.634102 Total_test_loss: 0.572062 Total_BCE_test_loss: 0.499963 Total_KLD_test_loss: 0.002837 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 37 total_train_loss: 0.634485 Total_test_loss: 0.572176 Total_BCE_test_loss: 0.500038 Total_KLD_test_loss: 0.002877 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 38 total_train_loss: 0.632152 Total_test_loss: 0.572310 Total_BCE_test_loss: 0.500205 Total_KLD_test_loss: 0.002845 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 39 total_train_loss: 0.642289 Total_test_loss: 0.571899 Total_BCE_test_loss: 0.499776 Total_KLD_test_loss: 0.002861 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 40 total_train_loss: 0.634502 Total_test_loss: 0.572778 Total_BCE_test_loss: 0.500681 Total_KLD_test_loss: 0.002834 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 41 total_train_loss: 0.638475 Total_test_loss: 0.573053 Total_BCE_test_loss: 0.500973 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 42 total_train_loss: 0.636921 Total_test_loss: 0.572434 Total_BCE_test_loss: 0.500366 Total_KLD_test_loss: 0.002810 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 43 total_train_loss: 0.630182 Total_test_loss: 0.571895 Total_BCE_test_loss: 0.499835 Total_KLD_test_loss: 0.002797 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 44 total_train_loss: 0.637856 Total_test_loss: 0.572230 Total_BCE_test_loss: 0.500141 Total_KLD_test_loss: 0.002833 Total_CEP_test_loss: 0.069256\n",
      "====> Epoch: 45 total_train_loss: 0.632287 Total_test_loss: 0.571685 Total_BCE_test_loss: 0.499606 Total_KLD_test_loss: 0.002817 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 46 total_train_loss: 0.639594 Total_test_loss: 0.572377 Total_BCE_test_loss: 0.500303 Total_KLD_test_loss: 0.002808 Total_CEP_test_loss: 0.069267\n",
      "====> Epoch: 47 total_train_loss: 0.632863 Total_test_loss: 0.573444 Total_BCE_test_loss: 0.501375 Total_KLD_test_loss: 0.002804 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 48 total_train_loss: 0.631111 Total_test_loss: 0.572018 Total_BCE_test_loss: 0.499934 Total_KLD_test_loss: 0.002818 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 49 total_train_loss: 0.632786 Total_test_loss: 0.571345 Total_BCE_test_loss: 0.499263 Total_KLD_test_loss: 0.002822 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 50 total_train_loss: 0.634307 Total_test_loss: 0.572464 Total_BCE_test_loss: 0.500383 Total_KLD_test_loss: 0.002817 Total_CEP_test_loss: 0.069264\n",
      "5e-06\n",
      "====> Epoch: 1 total_train_loss: 0.631996 Total_test_loss: 0.572821 Total_BCE_test_loss: 0.500755 Total_KLD_test_loss: 0.002803 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 2 total_train_loss: 0.635905 Total_test_loss: 0.572287 Total_BCE_test_loss: 0.500206 Total_KLD_test_loss: 0.002822 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 3 total_train_loss: 0.636945 Total_test_loss: 0.572786 Total_BCE_test_loss: 0.500710 Total_KLD_test_loss: 0.002811 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 4 total_train_loss: 0.639393 Total_test_loss: 0.572946 Total_BCE_test_loss: 0.500863 Total_KLD_test_loss: 0.002820 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 5 total_train_loss: 0.636720 Total_test_loss: 0.573093 Total_BCE_test_loss: 0.501025 Total_KLD_test_loss: 0.002806 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 6 total_train_loss: 0.636615 Total_test_loss: 0.573059 Total_BCE_test_loss: 0.500999 Total_KLD_test_loss: 0.002798 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 7 total_train_loss: 0.634240 Total_test_loss: 0.572396 Total_BCE_test_loss: 0.500322 Total_KLD_test_loss: 0.002811 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 8 total_train_loss: 0.634945 Total_test_loss: 0.572624 Total_BCE_test_loss: 0.500534 Total_KLD_test_loss: 0.002828 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 9 total_train_loss: 0.637084 Total_test_loss: 0.572744 Total_BCE_test_loss: 0.500657 Total_KLD_test_loss: 0.002827 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 10 total_train_loss: 0.637453 Total_test_loss: 0.572100 Total_BCE_test_loss: 0.499991 Total_KLD_test_loss: 0.002849 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 11 total_train_loss: 0.633551 Total_test_loss: 0.571811 Total_BCE_test_loss: 0.499704 Total_KLD_test_loss: 0.002851 Total_CEP_test_loss: 0.069257\n",
      "====> Epoch: 12 total_train_loss: 0.636868 Total_test_loss: 0.571728 Total_BCE_test_loss: 0.499626 Total_KLD_test_loss: 0.002841 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 13 total_train_loss: 0.638144 Total_test_loss: 0.571757 Total_BCE_test_loss: 0.499638 Total_KLD_test_loss: 0.002859 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 14 total_train_loss: 0.633473 Total_test_loss: 0.572112 Total_BCE_test_loss: 0.500015 Total_KLD_test_loss: 0.002835 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 15 total_train_loss: 0.636404 Total_test_loss: 0.571382 Total_BCE_test_loss: 0.499283 Total_KLD_test_loss: 0.002833 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 16 total_train_loss: 0.636782 Total_test_loss: 0.571498 Total_BCE_test_loss: 0.499410 Total_KLD_test_loss: 0.002827 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 17 total_train_loss: 0.631605 Total_test_loss: 0.572202 Total_BCE_test_loss: 0.500128 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 18 total_train_loss: 0.630953 Total_test_loss: 0.572334 Total_BCE_test_loss: 0.500271 Total_KLD_test_loss: 0.002803 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 19 total_train_loss: 0.638529 Total_test_loss: 0.572447 Total_BCE_test_loss: 0.500391 Total_KLD_test_loss: 0.002797 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 20 total_train_loss: 0.635099 Total_test_loss: 0.573262 Total_BCE_test_loss: 0.501228 Total_KLD_test_loss: 0.002772 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 21 total_train_loss: 0.636974 Total_test_loss: 0.573530 Total_BCE_test_loss: 0.501510 Total_KLD_test_loss: 0.002764 Total_CEP_test_loss: 0.069257\n",
      "====> Epoch: 22 total_train_loss: 0.633180 Total_test_loss: 0.572549 Total_BCE_test_loss: 0.500502 Total_KLD_test_loss: 0.002788 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 23 total_train_loss: 0.631967 Total_test_loss: 0.573005 Total_BCE_test_loss: 0.500968 Total_KLD_test_loss: 0.002772 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 24 total_train_loss: 0.636707 Total_test_loss: 0.572095 Total_BCE_test_loss: 0.500050 Total_KLD_test_loss: 0.002785 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 25 total_train_loss: 0.636660 Total_test_loss: 0.573071 Total_BCE_test_loss: 0.501031 Total_KLD_test_loss: 0.002781 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 26 total_train_loss: 0.637022 Total_test_loss: 0.572613 Total_BCE_test_loss: 0.500549 Total_KLD_test_loss: 0.002801 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 27 total_train_loss: 0.638730 Total_test_loss: 0.572723 Total_BCE_test_loss: 0.500629 Total_KLD_test_loss: 0.002833 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 28 total_train_loss: 0.637344 Total_test_loss: 0.572203 Total_BCE_test_loss: 0.500091 Total_KLD_test_loss: 0.002848 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 29 total_train_loss: 0.637293 Total_test_loss: 0.572491 Total_BCE_test_loss: 0.500417 Total_KLD_test_loss: 0.002816 Total_CEP_test_loss: 0.069258\n",
      "====> Epoch: 30 total_train_loss: 0.639559 Total_test_loss: 0.572322 Total_BCE_test_loss: 0.500219 Total_KLD_test_loss: 0.002839 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 31 total_train_loss: 0.639104 Total_test_loss: 0.571991 Total_BCE_test_loss: 0.499906 Total_KLD_test_loss: 0.002824 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 32 total_train_loss: 0.633829 Total_test_loss: 0.571287 Total_BCE_test_loss: 0.499193 Total_KLD_test_loss: 0.002831 Total_CEP_test_loss: 0.069264\n",
      "====> Epoch: 33 total_train_loss: 0.637367 Total_test_loss: 0.572393 Total_BCE_test_loss: 0.500311 Total_KLD_test_loss: 0.002825 Total_CEP_test_loss: 0.069257\n",
      "====> Epoch: 34 total_train_loss: 0.630719 Total_test_loss: 0.571884 Total_BCE_test_loss: 0.499787 Total_KLD_test_loss: 0.002836 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 35 total_train_loss: 0.635012 Total_test_loss: 0.571960 Total_BCE_test_loss: 0.499888 Total_KLD_test_loss: 0.002810 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 36 total_train_loss: 0.632212 Total_test_loss: 0.572875 Total_BCE_test_loss: 0.500795 Total_KLD_test_loss: 0.002820 Total_CEP_test_loss: 0.069260\n",
      "====> Epoch: 37 total_train_loss: 0.633566 Total_test_loss: 0.572257 Total_BCE_test_loss: 0.500186 Total_KLD_test_loss: 0.002809 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 38 total_train_loss: 0.632677 Total_test_loss: 0.571630 Total_BCE_test_loss: 0.499565 Total_KLD_test_loss: 0.002800 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 39 total_train_loss: 0.634662 Total_test_loss: 0.572474 Total_BCE_test_loss: 0.500417 Total_KLD_test_loss: 0.002798 Total_CEP_test_loss: 0.069259\n",
      "====> Epoch: 40 total_train_loss: 0.633667 Total_test_loss: 0.572920 Total_BCE_test_loss: 0.500877 Total_KLD_test_loss: 0.002781 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 41 total_train_loss: 0.635030 Total_test_loss: 0.572172 Total_BCE_test_loss: 0.500087 Total_KLD_test_loss: 0.002824 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 42 total_train_loss: 0.633438 Total_test_loss: 0.571878 Total_BCE_test_loss: 0.499818 Total_KLD_test_loss: 0.002798 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 43 total_train_loss: 0.635629 Total_test_loss: 0.572347 Total_BCE_test_loss: 0.500288 Total_KLD_test_loss: 0.002798 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 44 total_train_loss: 0.635729 Total_test_loss: 0.571339 Total_BCE_test_loss: 0.499272 Total_KLD_test_loss: 0.002805 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 45 total_train_loss: 0.634709 Total_test_loss: 0.572690 Total_BCE_test_loss: 0.500637 Total_KLD_test_loss: 0.002792 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 46 total_train_loss: 0.630152 Total_test_loss: 0.572825 Total_BCE_test_loss: 0.500798 Total_KLD_test_loss: 0.002766 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 47 total_train_loss: 0.635083 Total_test_loss: 0.571999 Total_BCE_test_loss: 0.499943 Total_KLD_test_loss: 0.002795 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 48 total_train_loss: 0.636991 Total_test_loss: 0.572249 Total_BCE_test_loss: 0.500192 Total_KLD_test_loss: 0.002793 Total_CEP_test_loss: 0.069265\n",
      "====> Epoch: 49 total_train_loss: 0.637563 Total_test_loss: 0.573664 Total_BCE_test_loss: 0.501638 Total_KLD_test_loss: 0.002762 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 50 total_train_loss: 0.634707 Total_test_loss: 0.572156 Total_BCE_test_loss: 0.500105 Total_KLD_test_loss: 0.002788 Total_CEP_test_loss: 0.069263\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "if model_tobe_trained:\n",
    "    lr=1e-2\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=100,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=200,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=5e-4\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "\n",
    "    lr=1e-5\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    lr=5e-6\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5tsLzrBNa7E"
   },
   "source": [
    "# Save The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42YFEvnqbE9U",
    "outputId": "555826a8-f613-4331-b6a5-1acc5a1b82a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running the neural network\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "print(\"running the neural network\")\n",
    "#run(obj1,save_address)\n",
    "obj1.model_save(address=save_address+\".pt\")\n",
    "obj1.save_residuals(address=save_address+'_residuals.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUnPPyZ6NfDr"
   },
   "source": [
    "# Visualize Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DStavVSXYRs5",
    "outputId": "047b7d45-2f98-4854-9ea2-17f3781d9842"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAAsTAAALEwEAmpwYAABGdUlEQVR4nO3deXwV1dnA8d+5e/aEJJAQ9n1JIOwCsoMsgntVxApaq6KCVUtBLZb6uu9KrYhWKS51LWALSq0FAZUdlEV2AoQ1BLLnrnPePyaJERKSQELI5fl+PiH33pk785x7yTNnzsw8o7TWCCGEqPsstR2AEEKI6iEJXQghgoQkdCGECBKS0IUQIkhIQhdCiCBhq60Vx8XF6WbNmtXW6oUQok5at27dca11fFnTai2hN2vWjLVr19bW6oUQok5SSu0rb5oMuQghRJCQhC6EEEFCEroQQgSJWhtDF0JUnc/nIz09HbfbXduhiBrmcrlo1KgRdru90u+RhC5EHZKenk5ERATNmjVDKVXb4YgaorUmMzOT9PR0mjdvXun3yZCLEHWI2+0mNjZWknmQU0oRGxtb5T2xChO6UuptpdQxpdTmcqYrpdSrSqldSqkflVJdqxSBEKJKJJlfHM7me65MD30OMOIM00cCrYt+7gBer3IUVbAm7QTPfLkNKfsrhBC/VGFC11ovA06cYZYrgbnatBKIVkolVleAp9qUns3rS3eTVeCrqVUIIcqQmZlJamoqqampJCQkkJSUVPLc6/We8b1r165l8uTJFa6jT58+5xzn4sWLS+IKDw+nbdu2pKamcsstt5w2b1ZWFn/9618rtdzw8PAqvV4bquOgaBJwoNTz9KLXDp86o1LqDsxePE2aNDmrlSVEuQA4kuMmJsxxVssQQlRdbGwsGzduBGDGjBmEh4fz+9//vmS63+/HZis7pXTv3p3u3btXuI7vvvvunOMcPnw4w4cPB2DgwIE8//zz5a67OKHffffd57zeC8F5PSiqtZ6tte6ute4eH19mKYIKlST0bDltS4jaNmHCBO666y569erFH/7wB1avXk3v3r3p0qULffr0Yfv27QAsXbqU0aNHA+bG4LbbbmPgwIG0aNGCV199tWR5xb3dpUuXMnDgQK677jratWvHuHHjSoZZFy1aRLt27ejWrRuTJ08uWW5FXnzxRZKTk0lOTubll18GYNq0aezevZvU1FSmTJlCXl4eQ4YMoWvXrqSkpLBgwYJKfxZaa6ZMmUJycjIpKSl89NFHABw+fJj+/fuTmppKcnIyy5cvJxAIMGHChJJ5X3rppUqv50yqo4d+EGhc6nmjotdqRELkzz10IS5mf/7XFrYeyqnWZXZoGMmfxnSs0nvS09P57rvvsFqt5OTksHz5cmw2G//97395+OGH+eyzz057z7Zt21iyZAm5ubm0bduWiRMnnna+9YYNG9iyZQsNGzakb9++fPvtt3Tv3p0777yTZcuW0bx5c8aOHVupGNetW8c777zDqlWr0FrTq1cvBgwYwNNPP83mzZtL9jz8fj/z5s0jMjKS48ePc8kll3DFFVdU6gDlP//5TzZu3MgPP/zA8ePH6dGjB/379+eDDz5g+PDhPPLIIwQCAQoKCti4cSMHDx5k82bzXJOsrKxKtaMi1dFD/xy4pehsl0uAbK31acMt1SU+wolFwWHpoQtxQfjVr36F1WoFIDs7m1/96lckJydz//33s2XLljLfc/nll+N0OomLi6N+/focPXr0tHl69uxJo0aNsFgspKamkpaWxrZt22jRokXJudmVTegrVqzg6quvJiwsjPDwcK655hqWL19+2nxaax5++GE6derE0KFDOXjwYJmxlbeOsWPHYrVaadCgAQMGDGDNmjX06NGDd955hxkzZrBp0yYiIiJo0aIFe/bsYdKkSXz55ZdERkZWah0VqbCHrpT6BzAQiFNKpQN/AuwAWutZwCJgFLALKABurZbIymG3WoiPcHJUErq4yFW1J11TwsLCSh5Pnz6dQYMGMW/ePNLS0hg4cGCZ73E6nSWPrVYrfr//rOapbu+//z4ZGRmsW7cOu91Os2bNzvmq3P79+7Ns2TIWLlzIhAkTeOCBB7jlllv44YcfWLx4MbNmzeLjjz/m7bffPuf4K3OWy1itdaLW2q61bqS1/pvWelZRMqfo7JZ7tNYttdYpWusar4mbEOnisAy5CHHByc7OJikpCYA5c+ZU+/Lbtm3Lnj17SEtLAygZp65Iv379mD9/PgUFBeTn5zNv3jz69etHREQEubm5JfNlZ2dTv3597HY7S5YsYd++civVlrmOjz76iEAgQEZGBsuWLaNnz57s27ePBg0a8Nvf/pbbb7+d9evXc/z4cQzD4Nprr+Xxxx9n/fr1VfocylMnL/1PiHKx93h+bYchhDjFH/7wB8aPH8/jjz/O5ZdfXu3LDwkJ4a9//SsjRowgLCyMHj16VOp9Xbt2ZcKECfTs2ROA22+/nS5dugDQt29fkpOTGTlyJFOnTmXMmDGkpKTQvXt32rVrV+nYrr76ar7//ns6d+6MUopnn32WhIQE/v73v/Pcc89ht9sJDw9n7ty5HDx4kFtvvRXDMAB46qmnqvhJlE3V1gU63bt312d7g4s/LdjMvA0H+XHG8GqOSogL208//UT79u1rO4xalZeXR3h4OFpr7rnnHlq3bs39999f22HViLK+b6XUOq11medh1slaLglRIeS4/RR4a35MTQhxYXnzzTdJTU2lY8eOZGdnc+edd9Z2SBeMOjrkYh4sOZLtpkX8hXOVlhCi5t1///1B2yM/V3Wzhx4ZAsChLDkwKoQQxepkQm8Rb54mtfd4Xi1HIoQQF446mdDrRzgJd9rYnSFnugghRLE6mdCVUrSID2N3hvTQhRCiWJ1M6AAt48PZfUwSuhDny7mUzwWz4FZZ1RTfeeedkuU4HA5SUlJITU1l2rRpp82blpbGBx98UOG60tLSSE5OrvTrwaJOnuUC0DI+jHkbDpLv8RPmrLPNEKLOqKh8bkWWLl1KeHj4aTXPb731Vm691awY0qxZM5YsWUJcXFyZyyhO6DfddNPZNSLI1ekeOiBXjApRi9atW8eAAQPo1q0bw4cP5/Bhsy7fq6++SocOHejUqRM33ngjaWlpzJo1i5deeonU1NQyC2OVVl4p2mnTprF8+XJSU1N56aWXSEtLo1+/fnTt2pWuXbtWqZ662+3m1ltvJSUlhS5durBkyRIAtmzZQs+ePUlNTaVTp07s3LmT/Px8Lr/8cjp37kxycnKlSw6cb3W2a9uyvpnQd2fkkZwUVcvRCFELvpgGRzZV7zITUmDk05WaVWvNpEmTWLBgAfHx8Xz00Uc88sgjvP322zz99NPs3bsXp9NJVlYW0dHR3HXXXZXu1ZdXivbpp5/m+eef59///jcABQUFfPXVV7hcLnbu3MnYsWOp7BXor732GkopNm3axLZt27jsssvYsWMHs2bN4r777mPcuHF4vV4CgQCLFi2iYcOGLFy4EDBrvlyI6mxCbxobitNmYeWeE1yZmlTb4Qhx0fF4PGzevJlhw4YBEAgESEw07z7ZqVMnxo0bx1VXXcVVV11V5WWXV4r21DKzPp+Pe++9l40bN2K1WtmxY0eV1jFp0iQA2rVrR9OmTdmxYwe9e/fmiSeeID09nWuuuYbWrVuTkpLCgw8+yNSpUxk9ejT9+vWrcpvOhzqb0J02K1elJjFvQzpTR7QlOlRuRycuMpXsSdcUrTUdO3bk+++/P23awoULWbZsGf/617944okn2LSpmvckirz00ks0aNCAH374AcMwcLlc57zMm266iV69erFw4UJGjRrFG2+8weDBg1m/fj2LFi3ij3/8I0OGDOHRRx+thhZUrzo7hg5w66XNcPsM5nyXVtuhCHHRcTqdZGRklCR0n8/Hli1bMAyDAwcOMGjQIJ555hmys7PJy8s7rVTtmZRXirascreJiYlYLBbeffddAoFApePv168f77//PgA7duxg//79JeV5W7RoweTJk7nyyiv58ccfOXToEKGhodx8881MmTKl2srdVrc620MHaJcQyWUdGvDyf3fi9RtMGd62UreKEkKcO4vFwqeffsrkyZPJzs7G7/fzu9/9jjZt2nDzzTeTnZ2N1prJkycTHR3NmDFjuO6661iwYAEzZ84847BFeaVoY2NjsVqtdO7cmQkTJnD33Xdz7bXXMnfu3JKSupV19913M3HiRFJSUrDZbMyZMwen08nHH3/Mu+++i91uJyEhgYcffpg1a9YwZcoULBYLdrud119/vTo+wmpXJ8vnlubxB5jx+Rb+sfoAd/RvwUMj2xEwNFaLkuQugo6Uz724VLV8bp3uoYM5lv7k1SnYrRZmL9tDTqGPJduP0a1pDDPHdsVqkaQuhLg41PmEDmYpgBljOuL1G3y45gDRoXYWbTpC45htPDRKejNCiItDUCR0AItF8cTVKfRpFcelreJ48avtvLFsD50bRzOiYwIW6akLIYJc3Uvo+76HrfMhdRw4w8GbD94CcIRhbdCRKzo3BGD66A5sSs/m7vfXY7cqBratz9QRbWlVP6J24xdCiBpS9xL6kU2w5m+watbp0yIbwdAZkHIdTpuVv03owT/Xp3Moy838jQcZ99YqnrgqhYQol1xdKoQIOnUvofe6A5KvgZ3/ARQ4QsEeBvnHYPWb8M/bYf93MOoF4sKd3NG/JQA39GjM9bO+5/a5a3HZLax6aChRofbabYsQQlSjupfQAcLiILWMamudboCv/wzfvgJ5x+DKv0BIDADtEyP54nf9+G5XJn/47Ec+XLMfX8Dg+u6NqR957leXCRHsMjMzGTJkCABHjhzBarUSHx8PwOrVq3E4zny19tKlS3E4HKdVWwSYM2cOU6ZMISkpCZ/PR/v27Zk7dy6hoaEAPP/887z11lu4XC7sdjuTJk3illtuYeDAgRw+fJiQEPO2lK1ateLTTz89bdlr167lL3/5yzl/Bhe6upnQy2OxwtA/Q3gCfDUdnm0JjXvCjR9AaD0axYRyfY9Q5q5M46kvtgFwONvNE1en1HLgQlz4aqp8brEbbrihJOnedNNNfPTRR9x6663MmjWLr776itWrVxMZGUlOTg7z5s0red/7779P9+5lnpZ90anTl/6XSSnofTf89n/Q9z44uA4+vRX8npJZxvVqCpgFvuZtOEiu21db0QpRp9VE+Vy/309+fj4xMebe9ZNPPsnrr79eUpgrMjKS8ePHn1W8aWlpDB48mE6dOjFkyBD2798PwCeffEJycjKdO3emf//+QNlldC90wdVDLy2xs/lTrwV8fi/M7A6XPw9thnNjj8b0bhFLdqGPK1/7lreW7+V3Q1vLlaWiTnlm9TNsO7GtWpfZrl47pvacWql5q7t87kcffcSKFSs4fPgwbdq0YcyYMeTk5JCbm0uLFi3KjWPcuHElQy7Dhg3jueeeK3feSZMmMX78eMaPH8/bb7/N5MmTmT9/Po899hiLFy8mKSmJrKwsgDLL6F7ogq+Hfqquv4ZbFoAzAj64Hr6Yhgp4aRYXRufG0Qxt34BXvt7JI/M313akQtQppcvnpqam8vjjj5Oeng78XD73vffew2arXL/xhhtuYOPGjRw5coSUlJQzJubS3n//fTZu3MjGjRsrfM/3339fcrejX//616xYsQKAvn37MmHCBN58882SxN27d2+efPJJnnnmGfbt21ey0biQBW8PvbQWA+GOJfDVo7Dqddj/PYz9B0Q25I1fd+PJRT/xtxV7Gdq+PoPbNajtaIWolMr2pGtKTZXPVUoxZswYZs6cybRp0wgPDy+pgFhTZs2axapVq1i4cCHdunVj3bp15ZbRvZAFfw+9mM0JI5+BG/8Bmbtg9kD4bibWgIepI9rRqn44f/h0Ey/+Zzv5Hn9tRyvEBa8my+euWLGCli3NU44feugh7rnnHnJycgDIy8tj7ty5ZxVznz59+PDDDwGzZ19c8XH37t306tWLxx57jPj4eA4cOFBmGd0LXaUSulJqhFJqu1Jql1LqtFtxK6WaKKWWKKU2KKV+VEqNqv5Qq0m7UXDbYohpDv/5I8yfiMOqePmGVJrFhjJzyS7ueHctbt+FP14mRG0qLp87depUOnfuTGpqKt999x2BQICbb7655F6dpcvnzps3r9yDoh999FHJAcgNGzYwffp0ACZOnMigQYPo0aMHycnJ9OvXD4vl59Q1btw4UlNTSU1NZejQoWeMeebMmbzzzjt06tSJd999l1deeQWAKVOmkJKSQnJyMn369KFz5858/PHHJCcnk5qayubNm7nllluq8dOrGRWWz1VKWYEdwDAgHVgDjNVaby01z2xgg9b6daVUB2CR1rrZmZZbXeVzz8mKl+C/M2DgQzDQ3E59ti6dBz/5gd9c2pzpozvUbnxCnELK515cqlo+tzI99J7ALq31Hq21F/gQuPKUeTRQfLO/KOBQlaKuLX1/B51vgqVPwbevAnBtt0aM7dmEv3+Xxs6judRWvXghhKiqyiT0JOBAqefpRa+VNgO4WSmVDiwCJpW1IKXUHUqptUqptRkZGWcRbjVTCq6YCR2vNi9EWvUGAA9e1gaX3cqwl5Yx9MVvOJbrruVAhRCiYtV1UHQsMEdr3QgYBbyrlDpt2Vrr2Vrr7lrr7sWXDFfVT5k/8eG2D9mSuYWT7pOccJ/gYN5BstxZZxe51QbXvAntRsMXf4AvphEXauP923vxu6GtOZTl5rY5a/jhwFkuXwghzpPKnLZ4EGhc6nmjotdK+w0wAkBr/b1SygXEAceqI8jSlqUv4y8bT6/JYFEWeib05Ncdfk2/pH5Vu0jIaodfzTFPa1z5V4hMpHPf++jcOJqUpCgm/WMDV772Ldd0TeLxq5IJdVwcZ3sKIeqWymSmNUBrpVRzzER+I3BqZaz9wBBgjlKqPeACamRM5Y5OdzC65Wi2HN9CRmEGCkWILYSDeQdZsHsB93x9Dze0vYFHej1S9aQ+4ik4mQZLn4aO10B0Y4a0b8DKh4fw5rI9/GXJLtDw4g2pNdE0IYQ4JxUmdK21Xyl1L7AYsAJva623KKUeA9ZqrT8HHgTeVErdj3mAdIKuoaOJSimSwpNICj91GB/u7Hwnr65/lTlb5uD2u/njJX/EZatiJcWRz8BrvWDB3fDr+WCxEumy8+BlbSn0BnjnuzTuH9aGxvVCq6dBQghRTSo1hq61XqS1bqO1bqm1fqLotUeLkjla661a675a685a61St9X9qMujy2C12Huj2ABM7T2TB7gUM/2w407+djifgqfjNxaKbmEl97zJY+ADk/byj8Zt+zbEomL1sTw1EL8SFLTMzs+R874SEBJKSkkqee73eCt+/dOlSvvvuuzKnzZkzh3vvvRcAwzAYP348t912G1prmjVrxvHjx0+bPz4+ni5dutC6dWuGDx9e7rJnzJjB888/X8XW1k1BNxislOLu1LtJjU9l/u75zN81H0Mb/F/f/8Ny+nHasnX5tXlnpNVvwpb5MO5TaNyDxKgQbujRmHdX7qN7sxiuTD19L0GIYFXT5XPBLCdw11134fP5eOedd844bFq63O6SJUu45pprWLJkyUV9nn7QXvrfJ6kPz/Z/lomdJ/L57s8ZM28My9PLL9n5C0rBqOfg7pXmDTLmXgE/fATAHy/vQK/m9bjvw41MfG8d2YVSeldcvKq7fO7kyZPJzMxk7ty5v7gatCKDBg3ijjvuYPbs2Wecb+PGjVxyySV06tSJq6++mpMnT5YZL8A333xTsgfSpUuXSpctqE1B10M/1V2d76JpZFPe2vQWk/43iYd6PsT1ba+v3AHT+u3MMgGfTIB5d8CepbhGPcffJvTgjW9289qSXdSPcPLnK5NrvB1CnOrIk0/i+al6y+c627cj4eGHKzVvdZfP/eCDD2jfvj1Lly6tdIXG0rp27cobb7xxxnluueUWZs6cyYABA3j00Uf585//zMsvv3xavGDeJem1116jb9++5OXl4XJd+Hc2C9oeejGLsnB5i8t5b9R79G7Ym8dXPc605dMo8BVUbgERDWD8v2DAVPjhH/Dv3xHutPHgZW0Z16sp763az63vrOa2OWv4cPX+mm2MEBeQ6i6f27VrV/bt28fq1avPKp6KzsPIzs4mKyuLAQMGADB+/HiWLVtWbrx9+/blgQce4NVXXyUrK+usNjLn24UfYTUJs4fx2pDXePPHN/nrD39la+ZW7km9h8uaXVbx2LrVBoMeBr/bLBEwYCrEteaBYW34autR0jILUMC0f27CFzD4de9m56NJ4iJX2Z50Tanu8rnt2rXjscce4/rrr2fx4sV07NixSvFs2LDhrMfPy4p32rRpXH755SxatIi+ffuyePFi2rVrd1bLP1+CvodemkVZuLPznbw57E0ApiybwmPfP1b5ei29J4HNBV8/BoZBTJiD7x8azJLfD+SrBwYwpF19ZvxrKx+tkZ66CH41UT63T58+vP7664wePbrk9nCV8c033zB79mx++9vfljtPVFQUMTExJeP37777LgMGDCg33t27d5OSksLUqVPp0aMH27ZV7/BWTbhoeuil9UzsyYKrFjBzw0ze2vQWMa4Y7ut6X8VvDI+H/g/C/x6HD34FXcejmveHkGisFsWrY7sw8f31TP1sE/tPFHDzJU2JDnEQ4rDWfKOEOM+Ky+dOnjyZ7Oxs/H4/v/vd72jTpg0333wz2dnZaK1/UT73uuuuY8GCBcycObOkFvmpxowZw/HjxxkxYkRJ8u3UqVPJQdLrr7+eTp06ldyyrqCggObNm/PZZ59V2EP/+9//zl133UVBQQEtWrTgnXfeKSn3e2q806dPZ8mSJVgsFjp27MjIkSOr9wOsARWWz60pF0L5XK01/7fy//hkxyf8vvvvGd+xkjeeXTnLTOreXFAWcEZC+9Ew+hV8WJg+fzMfrjHrmTWIdLLgnkupF+bAYbuodohEDZDyuReXqpbPvSh76MWUUjzS6xGyPdk8v9a88OCWDrdUfAbMJXdBj99A+hrzAqTM3bDhPQDsV/yFp65JoXfLWI5ku3n1650MffEb8r1+7hrQkt9f1harRW5GLYSofhd1QgewWqw83f9pWAbPr32eLce38FjfxyouGWC1Q9M+5g9ATDNY9izU74jqfXfJRUdtGkTw9rd7CXfaeH3pbhZvOcKEPs24pmsjwp3mx388z0OYwyZDM0KIc3LRJ3QwSwY82/9Z2m1ux6sbXiUpIqlyY+qlDXwIjm01b2uXkALNzfHBQe3qM6hdfbTWLNp0hNnLdvPogi288t+dPHF1MpEhdu6Yu47oUDvTR3egT8tYIlz2GmilECLYXdRj6GV5ZMUjLNq7iH9e8U+aRzWv2pvdOfDWECg4AeM+hqRuZc62bt9J/jh/Mz8dNm962yw2FA3syywgxG7lN5c2p2X9MDokRtGmQXjVqkaKoCZj6BeXqo6hS0I/xfHC41wx/wqaRzVnzog52C1V7C0f3wlvj4CC49Djdhj5HJRxCbPHH+Drn45xKKuQK1IbEumys27fST5YtZ+Fmw6XzNcoJoQh7eozvGMCvVvGsvd4Pt/syCApOoTLOiaca3NFHSMJ/eIiCb0afJn2JVO+mcLIZiO5K/UuWkS1qNoC3Dmw5ElY9To06QPN+prDMK2Hg73iy4eP5rjJdftYk3aSr386yopdx3H7DPq1jmPlnkx8AY3Nolg4uR+t64djsSgKvQEsFnDaZBw+mElCv7hIQq8mr6x/hb9v+TsAT/d7msuaXVb1hXz/Gqz7O2TuBG1AWLx5lWnbUeDNg9jWZfbeT1XoDfD60l28+r9dDGobz4OXteXXf1uFBnIKfQxsW58N+08SG+7k/dt70SDSRYHXL3dWCkK1mdAzMzMZMmQIAEeOHMFqtVJ8K8nVq1fjcDjO+P6lS5ficDjKrbb4xRdfMH36dAoKCnA6nQwePJgXXniBGTNm8Oabb1L6tpVLly5l48aNXHnllTRv3hyPx8ONN97In/70p18sMy0tjdGjR7N58+ZzaXqtkdMWq8l9Xe9jXPtxPLD0AR785kFuP3E796Teg81ShY+s9z3mj88N+7+H5S/Aot+bPwChcdDzt5B/HJr2huRry1xMiMPKA5e15ebeTYkLc2KxKJ68OoVXvt7JZR0a8MWmI7RLjGDroRyuf+N77uzfkhmfb+G2S5szbeSFfamyqDtqsnzu5s2buffee1m4cCHt2rUjEAj8onLi/fffX+a6+vXrx7///W/y8/NJTU1lzJgxdO3ateqNCxKS0M8gLiSO2cNm8/Tqp3lr01usO7qO5/o/R4OwBlVbkN0FLQdBi4Hw0+eQcxic4bBlHix9Ciw2WPMm7P4fpI77+VTIU9SP+Hm4ZmRKIiNTEgF49rrOAKzff5Lxb6/m4XmbiHDZmPXNbj5Ze4ACb4DoUDv1whw0qRfKoLb1GdO5ISEOK4XeALO+2U3/NnF0a1rvrD4ncfFat24dDzzwAHl5ecTFxTFnzhwSExN59dVXmTVrFjabjQ4dOvD0008za9YsrFYr77333mlXij777LM88sgjJbVSrFYrEydOrHQcYWFhdOvWjV27dpWb0N1uNxMnTmTt2rXYbDZefPFFBg0axJYtW7j11lvxer0YhsFnn31Gw4YNuf7660lPTycQCDB9+nRuuOGGc/uwzgNJ6BVw2VzM6DODHgk9+PP3f2b6t9OZfdmZay6XSynocOXPz7vcDCf3QVicWR9m7TvmBUqthpkHVFsMrNSYe7GuTWL4x28v4f1V+7h/WBveX7mfw9mFRLrsZBX6yMzzsOlgNl9sPsLr3+zmT2M68LcVe1m+8zivfL2TSYNb8eBlbc+ubeK8W/7xDo4fyKvWZcY1Dqff9W0qNW91ls/dvHkzDz74YLnreumll3jvPfPivZiYGJYsWfKL6ZmZmaxcuZLp06eXu4zXXnsNpRSbNm1i27ZtXHbZZezYsYNZs2Zx3333MW7cOLxeL4FAgEWLFtGwYUMWLlwImJUa6wJJ6JV0eYvLySjI4IV1L7Dx2EZS66dWz4Jjmpq/Rz4DQx6FtW/Dsudh11fgCIemfc267FGNIfUmcISdcXHJSVE8dU0nAO4fdvofptaaZTuP8+DHPzDhnTUoBf93VTIb9p9k5v92seNoLpl5Xq5IbUiu20+PZvXo2Vx67uJ0pcvnAgQCARITzb3G4nK0V111FVddddU5r6u8IZfly5fTpUsXLBYL06ZNO2OFxhUrVjBp0iTArOzYtGlTduzYQe/evXniiSdIT0/nmmuuoXXr1qSkpPDggw8ydepURo8eXW7dmQuNJPQquL7t9by9+W2eX/s8s4fNJtRezTeKdoRBn0nQ805IWwY//Qv2rzKHYgyfWbr3kokQ1QiiG0PDLlVehVKKAW3i+fJ3/fjhQBat60fQJDaUG7o3Zl9mAUu3Z9AwOoRHF2wpec/Q9g2YOLAljWNCiAlzYLdKTZoLQWV70jWlOsvnduzYkXXr1tG5c+cqxVA8hn4ubrrpJnr16sXChQsZNWoUb7zxBoMHD2b9+vUsWrSIP/7xjwwZMoRHH330nNZzPkhCr4JQeyjTek7j4RUPc9PCm7iy1ZVcmnQpiWGJhNnDqu8CIJsDWg01fwC0hn3fwX8egcUP/Txf8nVw2f9BZMMqryIu3MmQ9j8fC3DYLHzw2174AppQu5Wdx/KIC3fw4ZoDvL50N//96SgAdqvi2es6kX6ikAMnC3jy6hRsZSR4ty/A0u0Z9GkVS6Rc+RqUSpfP7d27Nz6fjx07dtC+ffuScrSXXnopH374YUn53JycnDKXNWXKFK655houvfRS2rRpg2EYzJ49m7vuuqva4u3Xrx/vv/8+gwcPZseOHezfv5+2bduyZ88eWrRoweTJk9m/fz8//vgj7dq1o169etx8881ER0fz1ltvVVscNUkSehWNajGKcEc4L69/mRfXvciL614EoFdiL2YOnkmILaT6V6qUeS77HUvNQmDePPjp3/DtK7B1Plgd5lWpXW+BjlebdWbOgtNmpai8DG0TIgC4Z1ArxvZswqo9mRzP9zJvfTp/+PRHfAHzdFd/QNOvTRw/HMjmq61HaRjtYnjHBP62Yi+Hs920T4ykb8tYDA1jOidyMKuQJvVCSW4YhUWKlNVp1Vk+t1OnTrz88suMHTuWgoIClFKMHj26ZHrpMXSA+fPnVzneu+++m4kTJ5KSkoLNZmPOnDk4nU4+/vhj3n33Xex2OwkJCTz88MOsWbOGKVOmYLFYsNvtvP766+f0WZ0vch76OTiSf4SVh1eyP2c/f9v8N/o07MPMwTOrdmrjuTiZZh5I9RWaY+4n9kB4A2h3ufnTrJ+Z7MHcKFSDozluLn91Be0SIujQMJLZy/YA4LRZ6NMylvX7s8gu9NG1STSXd2rI84u34zcMgJKNAECv5vWYcUVHmseF4bKbF0MdzCokKsReUrRMnE4uLLq4yIVFteSTHZ/w2PePcVvybdzf7f7zH4BhwM7FsPED2PU1+PLBFmLWa3eEQZdxMPBhczjnHOV5/ITYrVgtiqM5bnIKfTSNDcNhs3A8z8PhLDfJSZEopTicXYjLZiXP42f9/pO0jA9nw/6TPLHoJ9w+g7hwJ//4bS88foNfzfqeprGhvPubXmj0L07TFCZJ6BcXSei16P++/z8+3vExLw58kWFNh9VeID63Wad99/8ADdnpsO3f5ph857EQGguJnSG0njk+XwvFvw6cKGBN2gme+mIbbm8AAJfDysl8L35DoxSM7dmEdgkRHDxZyI6juSiluLFHYwa2rY/HH+CZL7fh9Rs8dU0nrBbFgRMFHMwqJN/jp0Gki+SkKAB+OJBFYpSL+pHnbwNhGJpCX4Cwat7buBASumFo/IaBo1SZCa8/gN/QcnVyNZMrRWvR1J5T2XZyG4+seASrsjKo8aDaqZRod0Gby8yfYmvfhoW/h13/NZ+7os1TInd9BdFN4YqZ5tWq50njeqE0rhdKp0ZR/OV/u7Aoxd2DWrHrWB4b9p8k3+vng1X7MTQ4rBZaxIeRXejjjnfX4bJb8Ac0Aa3RGgIG5Lp9/Gfr0ZLlKwW/7deCAycK+GLzEVrXD+et8d3ZnZFHn5ZxLNl2jOW7jrM27QR5bj+D29enV/NYYsMdFHjM5JSZ7+Gb7RlMHdmOlvHhAGTmefh4bTqpjaN55stt9GpRj4dGtkdrTfrJQhpGh+ALGNzx7jo27j/JpxP70KZBxC/a7g8YbD2cQ6v64SgUry3Zxc5juTxzbSeiQ3+5BxUwNBYFP6Zn06SeeVaVYRgopch1+8kt2luKCbUXtbvs/29aa3LcfuxWRajDRsDQnMj3EB36y7OWtNbkewKEOqxYLIqAYeAPaBw2C7luP067hYMnC8n3BmgRF0aY04ZhaPYez8cb0DSKCaHAE8BhsxATasdvaAq8AcKdVg6cKCQu3EFU6Ol7iQFDU+g121PgCRAZYsNps+KyW3HYLPgCBnluPwFDE+6yYbUosgt9+AOa2HAHhqHx+A0cNgsuuxWtNUdz3DhsVkLsFgwNoQ4rSincvgBuXwBd9F0UeAM4bRasFgsFXj9un4HdqmgYHYK31DIrQ2uNP6BBgc2iUEqhtcbtC1DgNdcZFWKv1JliZ9PZlh56NTuaf5Tb/3M7aTlpDG0ylGf7P4v9LA9SVrvCLMg7CjmHzOJhx7ZCynVFp0UaMPpF8ybYzfvXSq/9VLluH4W+APVCHdis5h/1sh0ZrNh1HJfdyvCOCby/ch+frEsnJtTOzZc05ZIWsYQ5bby5fA8LfzxMhNPGiOQEPlmXjlLmDonTZsHjN4hw2ejaJAanzcLynccp9AVOi8GioFlcGNd3b0x0iJ1/rj/I6rQTgPkH6zc043o1YcuhHDYeyKJhlAulFIeKLujSWuOyW+nUKIqRyYkU+gK8uXwP+zILcNgsBAxNwDCLrTWKCaFjwyjiwh0cy/Ww8UAWGbke4iOcHM52ExViZ2rferRtHE9YZDTegC5JGFaLAg3xkU7CHDaOZLvxGQYhditagy76PAHqhTnw+A3yPX4cVgshDquZbDQ47RbyPX7CnDZcdnOPydBmz7vA60dhLstqUWgNdqsFq0VR4PVjs1jwF21sitsdMDS+gIFVKQLajDch0oXdqsjzmMkzxG4hu9BvvheF027BXeq7sFst+AMGxZlKKYUCjKLcZbUoAoYumZYY6cLjN8jM9/ziuwwvalNmnhfNz3nPYbXgC2g0GpvFQpjTSr4nQMAw12mzKCJD7OR5/ES6zI2URYHXb+ANGNgsFjz+QMlyio8Z2SyK+AgX2YU+Crz+kvUppUiKDqFeWPnDn1prMjMzyc3NpXnzX5bxliGX88xv+Jm7dS4vrXuJVtGt6NagGx1jOzKs6TDCHeG1Hd7PiodbDq6Dt4aBLvojapACI56EppdWqnhYbQoYmrTMfJrHhv3irJniHnNilAub1cJrS3axKT2bEckJLNl+jOEdExjeMaHkdoC+gMGOo7nkFPoJdZjHB5SCPLefcW+twm/8/Hfy5ys64vYFuKxjAn/+1xaWbs+geVwY13ZN4of0bOxWxZhODWkaG8arX+/EZbeweu8JDmW7AejcOJqxPRqzOyMPp81K/zbx+AIGz365jVyPn4wcDzFhDlIbR1M/wklaZj7928SzZNsxtBHghg6hhFo14U4rIXYr+d4AXr+BoTVun5lMLMo8FdUfMIevDA1hDiuG1uQXJe9Il42CouEuh82CoTVev7kRKH491GlFAXmeAOFOGwHDwGqxEO60kuP2o7XZM3bZrUQULS/cacMXMMjM96Iwe8YF3gAxoQ7yPH48/p9jtFkt+PwGdpuFSJetZAPhDxgYGtz+AP6AxmZVhNitJd+JBiKcNjSQXeAr6UXnuH0ln0G404rTbsUwNIY2j/0EDE2Iw0qky4bCTK7mxkmX/DkoZW4gsgt92K2Wkvc5bRa8fsP8f1a0cSreKNusCn/A3LAW3zu40BvA4zewKIgMseOyW9Cakg1mRb10l8tFo0aNsNt/2SGUhF5L/rX7X3y641O2n9xOvi+fCEcEd3a6k0GNB1HgL6BVdKvzd0ZMRfYsBb/XrOO+5CnI3m/21ruOh6EzwFHNF1HVIbsz8ohw2diTkc/JfG9JDR2gaHfaqPD2gVprfkzPJqA1XRpH18hQnNaaLYdy2J2RR6/msSRElX3MINft40i2m9anDAWV9t3u48SEOmifGAlAjttX7vUExT3wU09DXbfvBDaLhc6NowkYuiRx7jmeT77HT4fESGxWszfutFmq5TMJGJpNB7Mp9Abo2bzeL+7fq7Umt6iXXRUHswo5keclpVFUyRBMZRiG5vMfDtGpURQt4quvIycJvZYZ2mBr5lZe/+F1lqUvK3k9whHBjW1vJNuTTbcG3RjVYlQtRlmKtwA2f2ZWiNz4PtjDoNmlZumB9ldc8L12IYKZJPQLhNaa5QeXc6zgGKG2UL7Y+wVL05diUzb82s+1ra/lipZX0LXBBVT+c993sPmfsH0R5ByExFRoP9qsEBnewLyQyV7OxVS5R+Cjm80SBSOfNfdnC06YZ9cIIc7KOSd0pdQI4BXACryltX66jHmuB2ZgHjP5QWt905mWeTEm9LIcyT9ClDOKl9a9xKc7PsVn+OiX1I8b291I78TeF84BVSMAmz6Fb542L2AqFhZv3iA79SYzse9fBZs/hdzDkL4O8o+B4TfLFNhdZjXJmz755Rk4QohKO6eErpSyAjuAYUA6sAYYq7XeWmqe1sDHwGCt9UmlVH2t9bEzLVcS+ukKfAV8vP1jZm+aTa43lwh7BL0Se9E6pjVJ4UmMbD4Sh/XcLww6Z9588/fB9WY9933fmpUh49rAoQ1gDzWLh0Ummcl+xxfw3V8g4DGHbxp1g/H/KnvZnjzweyAs9vy1R4g65FwTem9ghtZ6eNHzhwC01k+VmudZYIfWutIVbCShl88b8LLy8Er+k/Yf1h9bz4HcAwA0j2rOrR1vJSEsgQZhDap+r9OaoLV5EdPW+eYNshskw+A/mjfwKM2dA4UnYcs/4b8zYMJC8zz40gfCtIZ3RpkbhaF/gl53XRCnTwpxITnXhH4dMEJrfXvR818DvbTW95aaZz5mL74v5rDMDK31l2Us6w7gDoAmTZp027dv31k16GLjM3x8f+h7nlvzHGk5aQAoFNe2uZZJXSZRz1WHxqQLTsArqeDJhvodoNsEsycfFgdHNpm354tvDxk/wYinzXLBQogS5yOh/xvwAdcDjYBlQIrWOqu85UoPveq01mw9sZUCXwFLDizhHz/9A5vFRqg9lJS4FK5rcx39G/XHoi7ws1ByDsG2hbDmLcjY9stpiZ3h9v/BJ+PNA7HJ10LLwRDfFuLant7zF+Iicz6GXGYBq7TW7xQ9/xqYprVeU95yJaGfuz1Ze/hw+4e4/W6+PfgtxwqP0TSyKcOaDmNw48F0jOt4YSd3rc0zZ/KPm+e/52ea91ONbmyO0//vcdjwvtmbLxbVGOo1N98TGmvWgvfmQ1xrs2cfEmOWD7Y6in5s5u/QWAirL6dcijrvXBO6DXM4ZQhwEPOg6E1a6y2l5hmBeaB0vFIqDtgApGqtM8tbriT06uU3/Hyx9wsW7F7A2iNrCegA9Vz1CLGFEGYP4+pWV3NT+5su7ARfloDfLBOcsa3oZzuc2G0m59xD5hCOPcQ888bwV7g4bCHgijSHeyISzUqUrkgIjTMrU0YkgCcHjm6FvCNFG5AW5kYkprl5tyhL0UVEWkNBplmf3hFhno55pjF/IwDaOOt69UJA9Zy2OAp4GXN8/G2t9RNKqceAtVrrz5V5idcLwAggADyhtf7wTMuUhF5zsj3ZLEtfxneHvsPQBul56fyY8SNjWozh6tZXU89VjyYRTS6cUyKrg98LJ/eaZ8kEvOaP4YOAzzxrJj/D7NX78qHgJBzdZO4R+PLNA7b6lDou9lAIr28ODwW8v5zmjDTP6ik8AX53qfeElV+e2Blhrt/vMTcK9hBzYxCRaC7L7/75zlNGwIzd8Bc99ps/obHmfNkHzenRTc2NkK/QbIe3AHwF5nNnZNHGRZvzBXzmtNBYcwOWl2HG74wAi93c0JT+QZu/LXawOcGTa+7p2EPMK4jtLnPjaHcB6vT3a8OMwWIzHztCIWs/nNhrbvgSOpmfceYucGeZjy02c+Pozf954+eKNGP0e8zPwBlpvt8eam5YldX8DCwW87E33/yuAz7zeys8aT43Aubn7i80r5/wFpgbbmeEuWEOiS6KVZvLDY01X8s6YP7fcISZ/7dyDpqF7SISzPhyD5vrcEWbx4EMv/mZhUSbtZJ8hWZnwRlufm+ZuyCxE3T/jXnTmrMgFxZd5LTWzPpxFn/d+NeS15LCk+jfqD8rDq6gcURjpnSfQquYVrUYZS0yAuDOLhoCSjf/yKObmUnCCJhJ/eReMxnlHDLn9eRCaAxENjLnd2dD9oGy9xK0YW40QmPNxJC130wsITFmwvB7zKSZe7goOVnNpGCxmb15S1HSys80h5CKE3nWPvO3PcRMcPZQM3HaQ4s2UkWJNTvd3NDYQ82Lvbx5EJ5gxu/JM2NWllN+lPk74DXjc4SbGxGf24y99G/4Oe7i94EZgxEwX/Pmm0NpMc0g9ygc326uN6qxmaCLk7gzwlxX8bIKs8xpNqf5WvHZUr4Cc35OyV/KaiZWq9P8rFzR5rUSSpkJ1R5i7nnZQ81p3jxAmRsVw2++P+A1N7Y6YG5ArA4zBnsIRCWZMeQeMZcZkWjG7842NxwWm/l5efMhtqX5/rxjZrxh8ebe3uGNcNnjZmG8syAJXQCwL2cfh/IOcazgGG/8+Abpuen0SerDluNbiAuJ44UBL+C0OUkKT6rtUEWwM4yi6xLO8ZaNumhPorhHb7GZibw64vPmltrTKWO9UP40v7v8thXHXDx0V0WS0MVpvAEv+b58YlwxLE9fzt1f3w2Yp0MObTqUB7s/KIldiAuQJHRRofm75lPgK+B44XHe++k9/IafVtGtuKHtDVzT+prauVGHEOI0ktBFlRzJP8J7W99j7dG1bMncQvt67enTsA9jWo6hZXTL2g5PiIuaJHRxVgxt8NH2j/hy75f8kPEDAR3gipZXMKzpMCzKQj1XPTrGdiy39+4JeHh29bN0iO3AtW2uPc/RCxGc5J6i4qxYlIWx7cYytt1YMgszmbt1LnO3zuXz3Z+XzNMprhO/7/F7utTvAsDxwuMsT1/O0YKjfHvwWzZmbAQgy5NFhCOCuVvn8sKAF2hbr21tNEmIoCY9dFElOd4c9mTtQSnFT5k/8damtzhacJSOsR3pGNuRL9O+JMebA0BcSByTukzi6/1f/+LGHqOaj+KZ/s+UuXxddGsvIUTZZMhF1JgCXwGf7PiExWmL2ZO9h7YxbZnWcxoto1uWlPrVWrP5+GaOFhxlzZE1fLz9YxZft5j6ofVPW970b6ez/uh6Hu39KL0Se53v5ghxwZOELi4YB3IOcPm8ywm3hzO4yWCua3MdCWEJRDuj2XR8E7ctvo1QWyjugJuZg2fSv1H/2g5ZiAuKJHRxQVl5eCUL9yzky71f4g78fOm8QpEQlsCHoz/krq/uYl/OPu7qfBd9GvahWVQznFZnLUYtxIVBErq4IJ1wn2DDsQ1kubM46TlJljuLoU2Hklo/laP5R5m2fBprj5r/RyzKQuOIxrSMaknTyKZkujOJccaQEJZAob+QZlHNaBnVkhhXDE6rE5fNVfcKkQlRCZLQRZ21P2c/WzO3sitrF3uy97AraxcHcg5Qz1WPk56T+Axfue91WBw4bU5CrCHmb1sIobZQwh3htIlpQ4PQBoTaQ4mwRxDtisaqrMSHxpPnzWPHyR1kFGaQGJZI44jGNI5oTJQz6jy2XIiyyWmLos5qEtmEJpFNfvFa8ZkwnoCHfF8+LquLvTl72ZO1h1xvLu6AG4/fQ2GgEI/fgzvgxu13U+gvpNBfyPHC46w8tBK/rkS53VIiHBEkhiUS5YwizBbGCc8JjuQfodBXSKg9lKTwJFw2F1ZlNX8sVizKgk3ZUEpxOP8wbr+bRhGNcFldnPCcoEFoA8LsYXgDXhqENsBv+M34Ax7cfjfugBtfwEeMK4a4kDjcfjeH8w8T0AGSwpOwKmtJuwr8BSWPI+wRJWcLNQxriF/7cfvdxLhiiHBEkFGQQbYnmzB7GDaLDY3G0AaGNtBalzy3WWw4rU7yffnYLXZCbCG4bK6SvSCX1UVAB3BanfgMH4fyDuEJeIhyRhEXEofNYsPQBiG2EA7lHeJA7gGindG0j22Py+oiLSeNHG8OIbYQrMpKgc9sg1/7MbRBhCOCcHs43oAXv/YT6YgkyhlVMr9FWcwfLFgsFgp8BWS6M/EFfPgMH9mebDLdmfgNPw3DG+Lxe4gNiaXQX0iON4cwexhhtjBC7aEopTAMs83RrmgiHZEcyjtkxm8PId+bz5GCI0Q6IokPicfQBscKjpHtzSbSEUmMKwZDG1iVlShnFDtP7qTQX0g9Vz3C7GEcyT/C3py9dKjXgUFNBtE0smm1/Z0Uk4Qu6pziROW0OkvG1YtPm6wsX8BHri/X/MP25HDScxJDGxzJP0K43ezBx4fGczj/MAdyD5Cem86B3AMcLThKjieHw/mHiXZGc0niJYTbw8n15nIo/xB53rySZBTQAQJGAEMb+A0/8aHx1HPVY1fWLjx+M+ltPr4Zb8CL3WIvOd3TbrHjsrpw2py4rC5sFhsnPSfJ9mRjs9hICE3Aoix8ve9rNLokyYbaQkseZxRkoNForfnmwDc4rA5cNhdZ7iz82k+ILYQYZwz5/nwCRgClVEliVEqhMJ/7DB+egIdQW2jJRsET8JT7uVqUBafVSaG/sMzpUc4o8rx5BE4tV1yK0+osSdb5vnz0qRUVq8CqrCW3aMwozMBuseMzfCgULpur3Diruo7y2mNRFgxtAOYxovjQeL7Y+wXhjnBJ6EJUF7vVTj2r+Yd+piJkEY4I2sS0qdFYSu9x2JQNazlV+DwBDxZlwW4x69gb2qjycQJfwIfX8BJmDzvreA1t/LwH4XdjtVjxBDxYlZWEMHNj4wl4yCw0e8YKRYG/gMTwRCIdkRT4CtiXs6/k2Ee0M9rslRv+kj2GkngNHx6/B4fVgVVZyfXmctJzErffXbJHYWCUPHZZXcSGxOK0OrFZbITZw0o+o4ARwGqxku3JxmF1EGILwdAGhf5Cc8OhNVaLFb/h54T7BNmebBLCEnBanRT4Cgixh9AgtAH5vnyOFRzDoizUD61PuD2cAn8BJwpPlHwWWZ4s2sa0JcQWQpYni0J/IdHOaELtoWQUZOCyuc768z8TGUMXQog65Exj6HIagBBCBAlJ6EIIESQkoQshRJCQhC6EEEFCEroQQgQJSehCCBEkJKELIUSQkIQuhBBBQhK6EEIECUnoQggRJCShCyFEkJCELoQQQUISuhBCBAlJ6EIIESQqldCVUiOUUtuVUruUUtPOMN+1SimtlCqztKMQQoiaU2FCV0pZgdeAkUAHYKxSqkMZ80UA9wGrqjtIIYQQFatMD70nsEtrvUdr7QU+BK4sY77/A54B3NUYnxBCiEqqTEJPAg6Uep5e9FoJpVRXoLHWeuGZFqSUukMptVYptTYjI6PKwQohhCjfOR8UVUpZgBeBByuaV2s9W2vdXWvdPT4+/lxXLYQQopTKJPSDQONSzxsVvVYsAkgGliql0oBLgM/lwKgQQpxflUnoa4DWSqnmSikHcCPwefFErXW21jpOa91Ma90MWAlcobWWO0ALIcR5VGFC11r7gXuBxcBPwMda6y1KqceUUlfUdIBCCCEqx1aZmbTWi4BFp7z2aDnzDjz3sIQQQlSVXCkqhBBBQhK6EEIECUnoQggRJCShCyFEkJCELoQQQUISuhBCBAlJ6EIIESQkoQshRJCQhC6EEEFCEroQQgQJSehCCBEkJKELIUSQkIQuhBBBQhK6EEIECUnoQggRJCShCyFEkJCELoQQQUISuhBCBAlJ6EIIESQkoQshRJCQhC6EEEFCEroQQgQJSehCCBEkJKELIUSQkIQuhBBBQhK6EEIECUnoQggRJCShCyFEkJCELoQQQUISuhBCBIlKJXSl1Ail1Hal1C6l1LQypj+glNqqlPpRKfW1Uqpp9YcqhBDiTCpM6EopK/AaMBLoAIxVSnU4ZbYNQHetdSfgU+DZ6g5UCCHEmVWmh94T2KW13qO19gIfAleWnkFrvURrXVD0dCXQqHrDFEIIUZHKJPQk4ECp5+lFr5XnN8AX5xKUEEKIqrNV58KUUjcD3YEB5Uy/A7gDoEmTJtW5aiGEuOhVpod+EGhc6nmjotd+QSk1FHgEuEJr7SlrQVrr2Vrr7lrr7vHx8WcTrxBCiHJUJqGvAVorpZorpRzAjcDnpWdQSnUB3sBM5seqP0whhBAVqTCha639wL3AYuAn4GOt9Ral1GNKqSuKZnsOCAc+UUptVEp9Xs7ihBBC1JBKjaFrrRcBi0557dFSj4dWc1xCCCGqSK4UFUKIICEJXQghgoQkdCGECBKS0IUQIkhIQhdCiCAhCV0IIYKEJHQhhAgSktCFECJISEIXQoggIQldCCGChCR0IYQIEpLQhRAiSEhCF0KIICEJXQghgoQkdCGECBKS0IUQIkhIQhdCiCAhCV0IIYJEnUvoAZ+BNnTJc8PQGKWeCyHExapS9xS9kGxZcYjv5+8mun4I7nwfBVletNZYbRYMQxMZFwKA3xvA5wkQ8BtYbRbCop0U5nrxFgaw2i3Y7BZsDgtWu/XnxzYLNof5XBsai1XhCLFhd1pRSoECpcB8AOpMgZ5holJnfOeZncNbL4DFF3+AdXXxogLy+VdOk46x1G8aWe3LrXMJPa5xOO37JJJ9rJDYhuGExzhRFoXfG0ApRU5mIUopbE4rdocVq03h9xnknXDToHkkIeF2/F4Dv8/A7wsQKPXY5wlQmOcj4DNQyuz9ewv9+DwBtAY0aPMf86ccZ9xf0Gfemzjze8/4ViFEHeEKd0hCB2jYKpqGraJrOwxxFnQFG7NzX0HNLr6m1fHwK+ysiJ+d0176GdS5hC7qrpr6T/zzCmp28TWtjodPMLSgrqtzB0WFEEKUTRK6EEIECUnoQggRJCShCyFEkJCELoQQQUISuhBCBAlJ6EIIESQkoQshRJCoVEJXSo1QSm1XSu1SSk0rY7pTKfVR0fRVSqlm1R6pEEKIM6rwSlGllBV4DRgGpANrlFKfa623lprtN8BJrXUrpdSNwDPADTUR8IVGa21e8mwYYBjm5duGUfKaWQPGKHmt9PzaMIrqwpSev9TytC6qLFn2/NowoNT0qs6vtTaf61KxnbmxlfhAKvzAKl5GRQupVBwVz1NxeytezYXSnkqVVaiO9lRHeyu9nOAV0qULzpYtq325lbn0vyewS2u9B0Ap9SFwJVA6oV8JzCh6/CnwF6WU0jVQvCPrs8/I/Nvb5n+Iop+fC2bpc3rdnMYvE2qpx79I2MWvCSFEFSXM+FOtJfQk4ECp5+lAr/Lm0Vr7lVLZQCxwvPRMSqk7gDsAmjRpclYBW2NicLZpU1TKVlFS07bkp/zXUaqMaWW8brEUPbQUPVYoS/H8Fih6rJSlaP6i5xaLOb2M+VXx46L5VdFyzdf45fPi+S0WM6by5i89vQrzl8RWEi+l3l+BStVjqWCeSiyiwrovlYmjWuapzDIqs5rz0J7q+kwqaFDlFlFdsQQna2T1V1qE81ycS2s9G5gN0L1797Pq3kYMHkzE4MHVGpcQQgSDyhwUPQg0LvW8UdFrZc6jlLIBUUBmdQQohBCiciqT0NcArZVSzZVSDuBG4PNT5vkcGF/0+DrgfzUxfi6EEKJ8FQ65FI2J3wssBqzA21rrLUqpx4C1WuvPgb8B7yqldgEnMJO+EEKI86hSY+ha60XAolNee7TUYzfwq+oNTQghRFXIlaJCCBEkJKELIUSQkIQuhBBBQhK6EEIECVVbZxcqpTKAfWf59jhOuQr1InAxthkuznZLmy8OZ9vmplrr+LIm1FpCPxdKqbVa6+61Hcf5dDG2GS7OdkubLw410WYZchFCiCAhCV0IIYJEXU3os2s7gFpwMbYZLs52S5svDtXe5jo5hi6EEOJ0dbWHLoQQ4hSS0IUQIkjUuYRe0Q2rg4VSKk0ptUkptVEptbbotXpKqa+UUjuLfsfUdpznQin1tlLqmFJqc6nXymyjMr1a9L3/qJTqWnuRn71y2jxDKXWw6LveqJQaVWraQ0Vt3q6UGl47UZ8bpVRjpdQSpdRWpdQWpdR9Ra8H7Xd9hjbX7Heti+6bWRd+MMv37gZaAA7gB6BDbcdVQ21NA+JOee1ZYFrR42nAM7Ud5zm2sT/QFdhcURuBUcAXmPdHuwRYVdvxV2ObZwC/L2PeDkX/x51A86L/+9babsNZtDkR6Fr0OALYUdS2oP2uz9DmGv2u61oPveSG1VprL1B8w+qLxZXA34se/x24qvZCOXda62WY9fNLK6+NVwJztWklEK2USjwvgVajctpcniuBD7XWHq31XmAX5t9AnaK1Pqy1Xl/0OBf4CfM+xEH7XZ+hzeWplu+6riX0sm5YfaYPqS7TwH+UUuuKbq4N0EBrfbjo8RGgQe2EVqPKa2Owf/f3Fg0vvF1qKC3o2qyUagZ0AVZxkXzXp7QZavC7rmsJ/WJyqda6KzASuEcp1b/0RG3upwX1OacXQxuLvA60BFKBw8ALtRpNDVFKhQOfAb/TWueUnhas33UZba7R77quJfTK3LA6KGitDxb9PgbMw9z9Olq861n0+1jtRVhjymtj0H73WuujWuuA1toA3uTnXe2gabNSyo6Z2N7XWv+z6OWg/q7LanNNf9d1LaFX5obVdZ5SKkwpFVH8GLgM2Mwvb8Y9HlhQOxHWqPLa+DlwS9EZEJcA2aV21+u0U8aHr8b8rsFs841KKadSqjnQGlh9vuM7V0ophXnf4Z+01i+WmhS033V5ba7x77q2jwafxdHjUZhHjHcDj9R2PDXUxhaYR7x/ALYUtxOIBb4GdgL/BerVdqzn2M5/YO52+jDHDH9TXhsxz3h4reh73wR0r+34q7HN7xa16ceiP+zEUvM/UtTm7cDI2o7/LNt8KeZwyo/AxqKfUcH8XZ+hzTX6Xcul/0IIESTq2pCLEEKIckhCF0KIICEJXQghgoQkdCGECBKS0IUQIkhIQhdCiCAhCV0IIYLE/wPI3wC3fMP+nQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obj1.plot_residuals(init_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkjkH6AjN5pq"
   },
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DBW6HgBINu9v"
   },
   "outputs": [],
   "source": [
    "from ci_vae import ivae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "#import umap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqNZPHbPOkoh",
    "outputId": "35686f32-9d60-4ae3-b7b6-a634658bb1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the code\n"
     ]
    }
   ],
   "source": [
    "print(\"start of the code\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##############################################################   \n",
    "##############################################################\n",
    "model_init=True\n",
    "model_tobe_trained=False\n",
    "\n",
    "model_init=True\n",
    "model_file_address='./bb.pt'\n",
    "save_address1=\"./\"\n",
    "\n",
    "df_XY=pd.read_csv('df_XY.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oDqQbsnOO6d"
   },
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fHFjOqpnigK_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj2 = ivae.IVAE(df_XY = df_XY,\n",
    "               reconst_coef = reconst_coef,\n",
    "               latent_size = 10,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "#obj2.model_initialiaze()\n",
    "\n",
    "obj2.model_load(address=\"bb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOUIzmGTOTyG"
   },
   "source": [
    "## Print the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-H3bybRp484",
    "outputId": "fce40859-ea30-4f75-b4b5-08284301f7cf"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-bdbf25a41d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "for param in obj2.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi3yqTDIOoui"
   },
   "source": [
    "# Make Prediction of All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHcer1BPikHd",
    "outputId": "f72b5c62-5fd7-438d-818f-47f5dddaf3a6"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c64723bb1d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_residuals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bb_residuals.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "\n",
    "    obj2.load_residuals(address='bb_residuals.pkl')\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    obj2.generate_test_results()\n",
    "    print(\"test data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaKllEltPf16"
   },
   "source": [
    "# Comprehensive Checking of The Prediction Values vs. True Values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK8l95VcvpJt",
    "outputId": "470c82e6-2b9a-4903-eaa1-732e46bd84c1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IVAE' object has no attribute 'x_last'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4d1c66f17ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IVAE' object has no attribute 'x_last'"
     ]
    }
   ],
   "source": [
    "print(obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9ZitDT26ZXW",
    "outputId": "de963c21-10fd-4515-b6b5-b9a06eb3bd46"
   },
   "outputs": [],
   "source": [
    "print(obj2.x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ENQ8rtRHInw",
    "outputId": "959af6be-4ab0-4150-e843-b0d2297efc30"
   },
   "outputs": [],
   "source": [
    "(np.abs(obj2.x_pred - obj2.x_last)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJJfodLhQmrD",
    "outputId": "1bc5cb45-60ba-4bc1-c812-fd51c64c41ef"
   },
   "outputs": [],
   "source": [
    "(obj2.x_pred-obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3q3XVFEvsEQ",
    "outputId": "4add0237-b89d-4803-983f-2d35cad6599d"
   },
   "outputs": [],
   "source": [
    "print(obj2.y_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opj3sBJX6cQH",
    "outputId": "daedae9e-1edb-4243-efa3-e54976aaee6c"
   },
   "outputs": [],
   "source": [
    "print(obj2.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "N-n9_o8nQxBa",
    "outputId": "08c51321-bbdc-4302-8c5d-c97c8700b590"
   },
   "outputs": [],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2Yzu9Eiis50",
    "outputId": "d909e107-e05b-41a0-f075-c48541287153"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "    for x, y in obj2.testloader:\n",
    "      x = x.to(device)\n",
    "      print(x.size())\n",
    "      print(x)\n",
    "      # forward\n",
    "      x_hat,y_hat, mu, logvar,z = obj2.model(x)\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(x_hat.cpu().detach().numpy(), columns=obj2.df_XY.drop(columns=['Y']).columns)\n",
    "    print(df_reconstructed.shape)\n",
    "    df_latent=pd.DataFrame(z.cpu().detach().numpy())\n",
    "    \n",
    "    obj2.model.eval()\n",
    "    \n",
    "    df_reconstructed_decoder=pd.DataFrame(obj2.model.decoder(z).cpu().detach().numpy(), columns=obj2.df_XY.drop(columns=['Y']).columns)\n",
    "\n",
    "    df_reconstructed.to_csv('df_reconstructed.csv')\n",
    "    df_latent.to_csv('df_latent.csv')\n",
    "    df_reconstructed_decoder.to_csv('df_reconstructed_decoder.csv')\n",
    "    print(\"Full_data_reconstructed...\")\n",
    "    \n",
    "    print(\"========df_reconstructed========\")\n",
    "    print(df_reconstructed)\n",
    "    print(\"========df_reconstructed_decoder========\")\n",
    "    print(df_reconstructed_decoder)\n",
    "    print(\"========df_Original========\")\n",
    "    print(df_XY)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CciNZOW_Rc0n"
   },
   "source": [
    "# Checking Linear Separability of Data on Lower Dimensioanl Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHqfONTOlqSr",
    "outputId": "6a2fa0ee-b52d-49dd-f55a-f6cdae3ee20e"
   },
   "outputs": [],
   "source": [
    "print(\"regression analysis\")\n",
    "obj2.regression_analysis(obj2.zs,df_XY['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBQlR5KERlL-"
   },
   "source": [
    "# Visualize Data on Lower Dimensional Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SXZtQfoj93-"
   },
   "outputs": [],
   "source": [
    "print(\"calculate tsne_umap_pca\")\n",
    "tsne_mat,umap_mat,pca_mat,Y=obj2.calculate_lower_dimensions(obj2.zs,obj2.y_last,N=100)\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='3d',save_str='tsne3d.pdf')\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='2d',save_str='tsne2d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='3d',save_str='umap3d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='2d',save_str='umap2d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='3d',save_str='pca3d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='2d',save_str='pca2d.pdf')\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ULT6UtRxU6"
   },
   "source": [
    "# Perform Interpolation across all groups (Y) and all features from YY=0 to YY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dXtgbd1iJ0s",
    "outputId": "4710be7c-960f-4184-bdb5-931a5699dd5a"
   },
   "outputs": [],
   "source": [
    "ff = obj2.traversal_all_groups(traversal_step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyAzr8KSAgH"
   },
   "source": [
    "# See the interpolation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWzpnwK0g-8g"
   },
   "outputs": [],
   "source": [
    "with open('results_dict.pkl', 'rb') as f:\n",
    "    ff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "p23aMsKEknL4",
    "outputId": "29141142-bd3d-4f66-c936-9479312bcba9"
   },
   "outputs": [],
   "source": [
    "ff['med']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hMMGIGAslEoz",
    "outputId": "52e9d86d-dcb8-46c1-dfbf-1866da50861b"
   },
   "outputs": [],
   "source": [
    "ff['mean']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "T7sydB3ISVvE",
    "outputId": "36818219-d221-43b4-aa92-fb45eb31fb7a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qHpnnAoxoy_r",
    "outputId": "cc5504d0-001b-4b5d-afc1-d01f408db930"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['mean']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "kR4GHdxyqCxn",
    "outputId": "f9387ed5-758f-41db-bd4a-9043c31ccb4a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['0']['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "l92UJyGZSMYB",
    "outputId": "5632470a-2575-41d5-9877-1335947b8d45"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ff['mean']['0']['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHBvdSESFRV"
   },
   "source": [
    "# Generate Synthetic Data for a Given Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ_3NAgi2-6L",
    "outputId": "a11c8fd5-41de-40d3-dee2-91580cb322a9"
   },
   "outputs": [],
   "source": [
    "bb = obj2.synthetic_single_group(group_id=0,nr_of_synthetic=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E330szfA6BD7",
    "outputId": "a8687ace-e5b6-46d2-904d-e854e532d0d8"
   },
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey_yS1WZ3Os2",
    "outputId": "dbeb2f28-2746-4a69-beeb-a934f19eb8a1"
   },
   "outputs": [],
   "source": [
    "bb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
