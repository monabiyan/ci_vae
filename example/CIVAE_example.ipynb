{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhV9PPiQKgSg"
   },
   "source": [
    "# Generate Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9HIiKcw_PCm5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "#Generate 5 random numbers between 10 and 30\n",
    "np.random.seed(0)\n",
    "n_samples=1000\n",
    "n_features = 5\n",
    "df_XY=pd.DataFrame(data = np.random.normal(0,1, size=(n_samples, n_features)), columns = ['A','B','C','D','E'])\n",
    "df_XY['Y']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY['YY']=list(np.random.randint(2, size=n_samples))\n",
    "df_XY\n",
    "\n",
    "##############################################################   \n",
    "df_XY.shape\n",
    "df_XY.head()\n",
    "df_XY.to_csv('df_XY.csv',index=False)\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "31zCNsOv0BoE",
    "outputId": "b7af009e-8ad8-463e-805d-07a8d5e7e082"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>Y</th>\n",
       "      <th>YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.764052</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.978738</td>\n",
       "      <td>2.240893</td>\n",
       "      <td>1.867558</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.977278</td>\n",
       "      <td>0.950088</td>\n",
       "      <td>-0.151357</td>\n",
       "      <td>-0.103219</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144044</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.443863</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.333674</td>\n",
       "      <td>1.494079</td>\n",
       "      <td>-0.205158</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>-0.854096</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.552990</td>\n",
       "      <td>0.653619</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>-0.742165</td>\n",
       "      <td>2.269755</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.711489</td>\n",
       "      <td>-1.820816</td>\n",
       "      <td>0.163495</td>\n",
       "      <td>-0.813117</td>\n",
       "      <td>-0.605355</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-1.327524</td>\n",
       "      <td>-0.644172</td>\n",
       "      <td>1.908883</td>\n",
       "      <td>-0.563545</td>\n",
       "      <td>1.082473</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-1.951911</td>\n",
       "      <td>2.441216</td>\n",
       "      <td>-0.017285</td>\n",
       "      <td>0.912282</td>\n",
       "      <td>1.239658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.573367</td>\n",
       "      <td>0.424889</td>\n",
       "      <td>-0.271260</td>\n",
       "      <td>-0.683568</td>\n",
       "      <td>-1.537438</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.101374</td>\n",
       "      <td>0.746666</td>\n",
       "      <td>0.929182</td>\n",
       "      <td>0.229418</td>\n",
       "      <td>0.414406</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         B         C         D         E  Y  YY\n",
       "0    1.764052  0.400157  0.978738  2.240893  1.867558  1   1\n",
       "1   -0.977278  0.950088 -0.151357 -0.103219  0.410599  0   0\n",
       "2    0.144044  1.454274  0.761038  0.121675  0.443863  0   0\n",
       "3    0.333674  1.494079 -0.205158  0.313068 -0.854096  1   0\n",
       "4   -2.552990  0.653619  0.864436 -0.742165  2.269755  0   1\n",
       "..        ...       ...       ...       ...       ... ..  ..\n",
       "995  1.711489 -1.820816  0.163495 -0.813117 -0.605355  0   0\n",
       "996 -1.327524 -0.644172  1.908883 -0.563545  1.082473  1   0\n",
       "997 -1.951911  2.441216 -0.017285  0.912282  1.239658  1   1\n",
       "998 -0.573367  0.424889 -0.271260 -0.683568 -1.537438  1   1\n",
       "999 -0.101374  0.746666  0.929182  0.229418  0.414406  0   1\n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77NKQyeKwIj"
   },
   "source": [
    "# Download CI-VAE, other necessary packages and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cAuVLcUNETr7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: df_reconstructed.csv: No such file or directory\n",
      "rm: df_reconstructed_decoder.csv: No such file or directory\n",
      "rm: results_dict.pkl: No such file or directory\n",
      "rm: df_latent.csv: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! rm -rf ci_vae\n",
    "! rm bb.pt\n",
    "! rm bb_residuals.pkl\n",
    "! rm df_reconstructed.csv\n",
    "! rm df_reconstructed_decoder.csv\n",
    "! rm residuals.pdf\n",
    "! rm results_dict.pkl\n",
    "! rm df_latent.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "18S0saDPLp0X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ci_vae'...\n",
      "remote: Enumerating objects: 324, done.\u001b[K\n",
      "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
      "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
      "remote: Total 324 (delta 44), reused 57 (delta 22), pack-reused 245\u001b[K\n",
      "Receiving objects: 100% (324/324), 46.73 MiB | 13.55 MiB/s, done.\n",
      "Resolving deltas: 100% (200/200), done.\n",
      "Requirement already satisfied: umap-learn in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (0.5.3)\n",
      "Requirement already satisfied: tqdm in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (4.50.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.23.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.5.8)\n",
      "Requirement already satisfied: numba>=0.49 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (0.51.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from umap-learn) (1.19.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.22->umap-learn) (2.1.0)\n",
      "Requirement already satisfied: llvmlite>=0.30 in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from pynndescent>=0.5->umap-learn) (0.34.0)\n",
      "Requirement already satisfied: setuptools in /Users/mohsennabian/opt/anaconda3/lib/python3.8/site-packages (from numba>=0.49->umap-learn) (50.3.1.post20201107)\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/monabiyan/ci_vae.git\n",
    "! pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kzwk1I17VAQx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ci_vae import ivae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPVcV9thL8SP"
   },
   "source": [
    "# Set Necessary Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KAVb-irtbIpc"
   },
   "outputs": [],
   "source": [
    "model_init=True\n",
    "model_tobe_trained=True\n",
    "save_address=\"bb\"\n",
    "\n",
    "kl_coef = 0.0001\n",
    "reconst_coef = 1\n",
    "classifier_coef = 0.1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRIfHjpSMKF5"
   },
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ll0w2DunMJei"
   },
   "outputs": [],
   "source": [
    "obj1 = ivae.IVAE(df_XY = df_XY,\n",
    "               latent_size = 2,\n",
    "               reconst_coef = reconst_coef,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "if model_init:\n",
    "    obj1.model_initialiaze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT-AB_8-M-tD"
   },
   "source": [
    "## See The Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJXlsM8Uk2Ry",
    "outputId": "129babee-fc0e-48c5-9262-08404a9b89c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVAE_ARCH(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=10, out_features=4, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(10, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.05, inplace=False)\n",
      "    (4): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.05, inplace=False)\n",
      "    (8): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.05, inplace=False)\n",
      "    (12): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (15): Dropout(p=0.05, inplace=False)\n",
      "    (16): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (17): ReLU()\n",
      "    (18): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (19): Dropout(p=0.05, inplace=False)\n",
      "    (20): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (21): ReLU()\n",
      "    (22): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (23): Dropout(p=0.05, inplace=False)\n",
      "    (24): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (25): ReLU()\n",
      "    (26): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (27): Dropout(p=0.05, inplace=False)\n",
      "    (28): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (29): ReLU()\n",
      "    (30): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (31): Dropout(p=0.05, inplace=False)\n",
      "    (32): Linear(in_features=20, out_features=20, bias=True)\n",
      "    (33): ReLU()\n",
      "    (34): BatchNorm1d(20, eps=1e-05, momentum=0.2, affine=True, track_running_stats=True)\n",
      "    (35): Dropout(p=0.05, inplace=False)\n",
      "    (36): Linear(in_features=20, out_features=5, bias=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): Dropout(p=0.8, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(obj1.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSj9WT_mNHNl"
   },
   "source": [
    "## See the Initialized Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUFyVcu4ldFH",
    "outputId": "5a470060-626d-4602-dcf4-78d601febd3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1255, -0.1332, -0.2631,  0.2878,  0.3824],\n",
      "        [-0.1954, -0.2484, -0.1271, -0.1408, -0.3421],\n",
      "        [-0.1822, -0.1539, -0.4439, -0.1395, -0.3040],\n",
      "        [ 0.1568,  0.4452,  0.4382, -0.0129, -0.0811],\n",
      "        [-0.2041, -0.2300, -0.3295, -0.3451,  0.2595],\n",
      "        [ 0.3771,  0.1424, -0.2702, -0.4012,  0.3346],\n",
      "        [ 0.0979,  0.2576, -0.3247, -0.1336,  0.0614],\n",
      "        [-0.1357,  0.2861,  0.3238, -0.0621,  0.1341],\n",
      "        [-0.4123, -0.3359, -0.4186,  0.2058, -0.2133],\n",
      "        [ 0.3171, -0.0611, -0.0322, -0.2112,  0.1465],\n",
      "        [ 0.1487, -0.2670,  0.3932, -0.1941, -0.1546],\n",
      "        [-0.2165, -0.4302,  0.2742, -0.0267,  0.0773],\n",
      "        [ 0.4455, -0.3335, -0.3291, -0.3714,  0.1650],\n",
      "        [-0.2169,  0.0416,  0.0137,  0.1375,  0.1016],\n",
      "        [-0.0674, -0.0214,  0.2815,  0.3504,  0.1888],\n",
      "        [ 0.0246,  0.2687, -0.4299,  0.2854, -0.0239],\n",
      "        [ 0.0086, -0.1917, -0.1670,  0.4021, -0.2057],\n",
      "        [-0.0078, -0.0151,  0.3559, -0.3285, -0.0320],\n",
      "        [ 0.1314, -0.0928, -0.4169, -0.1393,  0.1681],\n",
      "        [ 0.1968,  0.4003,  0.0372, -0.1870,  0.3113]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0644, -0.3392, -0.1904, -0.4376,  0.0624, -0.2698,  0.2208,  0.1432,\n",
      "        -0.2607, -0.0248,  0.1596, -0.3942,  0.1370,  0.2542, -0.0729,  0.3294,\n",
      "         0.1189,  0.2036, -0.1598, -0.0737], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0254, -0.1235,  0.2196,  0.2173, -0.2104,  0.1939,  0.0220, -0.0051,\n",
      "          0.0570,  0.1137,  0.1580,  0.1821, -0.2157,  0.0803, -0.1656, -0.1441,\n",
      "          0.0395,  0.0679,  0.0321, -0.0113],\n",
      "        [-0.0663, -0.0190, -0.0913, -0.1140,  0.0252,  0.1617,  0.1201,  0.0569,\n",
      "         -0.1646, -0.0185, -0.0734, -0.1935, -0.0495, -0.0969,  0.0940, -0.1708,\n",
      "          0.0704,  0.1744, -0.1885,  0.0201],\n",
      "        [ 0.1998, -0.0595,  0.2107,  0.0580,  0.0693,  0.0969,  0.1436, -0.0855,\n",
      "         -0.0428, -0.0847,  0.0986, -0.0350, -0.0164, -0.0237, -0.0816,  0.0627,\n",
      "         -0.0418,  0.1039, -0.2045, -0.0020],\n",
      "        [-0.1965, -0.0648, -0.0637, -0.1168,  0.1905, -0.1631, -0.0149, -0.0747,\n",
      "          0.0625, -0.0143, -0.1638,  0.1854, -0.0963, -0.1716, -0.1252,  0.1066,\n",
      "         -0.0973, -0.1633,  0.2197, -0.0863],\n",
      "        [ 0.1853,  0.0267, -0.0654,  0.0511,  0.1047, -0.1214,  0.0718, -0.0799,\n",
      "         -0.0075, -0.0545, -0.2195, -0.0081,  0.1056,  0.0143,  0.0594,  0.1774,\n",
      "          0.1713,  0.0457,  0.0803,  0.0743],\n",
      "        [-0.1454, -0.1500, -0.2179,  0.2166, -0.0239, -0.1060, -0.1821, -0.1720,\n",
      "          0.0986, -0.0097,  0.1241,  0.0041, -0.0301,  0.1909,  0.1507,  0.0319,\n",
      "          0.2014, -0.1141, -0.0220,  0.0256],\n",
      "        [-0.2201,  0.2223,  0.1120,  0.0781,  0.2067,  0.0627, -0.1773, -0.0715,\n",
      "          0.0255,  0.0415,  0.2032, -0.1796, -0.0114,  0.2003,  0.2029, -0.1658,\n",
      "          0.0806,  0.0874,  0.2078,  0.1419],\n",
      "        [ 0.1458, -0.2152,  0.0557,  0.1369,  0.1380, -0.1836, -0.1967,  0.0199,\n",
      "          0.1300,  0.0664,  0.2137, -0.2195,  0.0420,  0.1159, -0.1634, -0.0587,\n",
      "          0.0655,  0.1967, -0.0800, -0.0734],\n",
      "        [ 0.0264, -0.1614,  0.2135, -0.2136,  0.0210,  0.1655,  0.0309,  0.1131,\n",
      "         -0.1364, -0.1988,  0.1610, -0.2188,  0.0727, -0.1018, -0.0982, -0.1869,\n",
      "         -0.1363, -0.2105,  0.0253,  0.1135],\n",
      "        [ 0.0914,  0.0247, -0.1705,  0.1222, -0.0837, -0.1079,  0.0355,  0.0799,\n",
      "         -0.0084,  0.0023, -0.0005, -0.0744, -0.0881,  0.0538,  0.0286, -0.0644,\n",
      "          0.0316,  0.2136, -0.0420,  0.1461],\n",
      "        [ 0.2227, -0.0007, -0.1926, -0.0448,  0.1396,  0.1090,  0.1407, -0.0984,\n",
      "         -0.1836, -0.1670,  0.0365, -0.0666,  0.1425, -0.0094,  0.1680,  0.2150,\n",
      "          0.0483, -0.1289,  0.1197,  0.1387],\n",
      "        [-0.1125, -0.1231,  0.0968, -0.1062, -0.0144,  0.0470,  0.0735,  0.0279,\n",
      "          0.0741,  0.0606, -0.0947, -0.1828, -0.1163,  0.1148, -0.1532,  0.1538,\n",
      "         -0.0708,  0.1772,  0.0239, -0.0147],\n",
      "        [ 0.1903,  0.1344, -0.0843, -0.0852, -0.0287, -0.0124,  0.1462,  0.2155,\n",
      "          0.0702,  0.0415,  0.2039, -0.1307, -0.1693, -0.0558,  0.1981, -0.0438,\n",
      "          0.0190,  0.0685, -0.1347,  0.0596],\n",
      "        [ 0.2071,  0.0801, -0.0956,  0.0973,  0.0798,  0.1403, -0.1531, -0.1219,\n",
      "          0.1775, -0.0838, -0.0890,  0.0643,  0.1715, -0.0309, -0.1860, -0.1722,\n",
      "         -0.0372,  0.1821, -0.0091,  0.0054],\n",
      "        [-0.1096,  0.1112, -0.0320, -0.1502,  0.1486, -0.0325, -0.1744, -0.0603,\n",
      "          0.1552, -0.0387, -0.0040, -0.1621, -0.1397, -0.2135,  0.0509, -0.0436,\n",
      "         -0.1650,  0.1845,  0.0302,  0.1641],\n",
      "        [-0.0471, -0.0473,  0.0693,  0.1761,  0.1756, -0.2213, -0.1285, -0.1725,\n",
      "         -0.0379,  0.1913, -0.1912,  0.0415, -0.1801,  0.0834,  0.0329, -0.0215,\n",
      "          0.1707,  0.1679,  0.1593,  0.0850],\n",
      "        [ 0.1294,  0.0052, -0.1275, -0.1959,  0.1872,  0.1945, -0.0396, -0.0030,\n",
      "         -0.0899,  0.1571,  0.2170, -0.0652, -0.1095,  0.1113,  0.1993, -0.0977,\n",
      "         -0.1921,  0.1700, -0.1768, -0.1821],\n",
      "        [ 0.1572, -0.0811, -0.0474,  0.2136,  0.1263,  0.1009,  0.0032,  0.0791,\n",
      "          0.1729, -0.0317, -0.1571,  0.0567, -0.2147,  0.1312,  0.1411,  0.0641,\n",
      "         -0.1967,  0.0961, -0.1790, -0.1256],\n",
      "        [-0.1702,  0.0391, -0.0545, -0.0884, -0.1821,  0.1156,  0.0394,  0.0579,\n",
      "          0.0007,  0.2020,  0.0379, -0.1078, -0.2149, -0.1714, -0.0143,  0.0186,\n",
      "         -0.0984, -0.0550,  0.1673,  0.0766],\n",
      "        [ 0.1523, -0.1873,  0.1421,  0.1505, -0.0295, -0.0885, -0.1975, -0.1563,\n",
      "          0.0506,  0.0187, -0.1462, -0.0288,  0.0980,  0.1438, -0.1719, -0.0770,\n",
      "         -0.0457,  0.1213,  0.2033, -0.0526]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2156, -0.1462, -0.1747, -0.0014,  0.0495, -0.1822, -0.0348, -0.1252,\n",
      "         0.0276,  0.2034,  0.0759,  0.0025,  0.1626,  0.1464,  0.1586,  0.0631,\n",
      "        -0.1890,  0.0694,  0.1962,  0.2234], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6654e-01,  6.8528e-02, -7.5856e-03,  4.8697e-05, -2.0720e-02,\n",
      "         -1.7685e-01, -1.1014e-01,  4.9170e-02, -4.0059e-02, -2.6860e-02,\n",
      "         -8.9478e-02,  1.4593e-01,  6.6905e-02,  1.9536e-01, -1.7452e-01,\n",
      "         -1.8970e-02,  1.4957e-02,  6.6544e-02,  9.9799e-02, -2.0168e-01],\n",
      "        [-2.8511e-02,  1.1238e-02,  1.2639e-01, -2.2333e-01,  1.5379e-01,\n",
      "         -1.4357e-01,  1.6253e-01,  2.6569e-02,  1.9567e-01, -1.8517e-01,\n",
      "          2.1445e-01, -1.4385e-01, -1.3100e-01, -9.2302e-02,  3.0986e-02,\n",
      "         -2.0390e-01, -5.9793e-02,  1.9309e-01,  1.9005e-02, -1.3268e-01],\n",
      "        [-1.5625e-01, -1.5521e-01,  3.3729e-02, -2.6675e-03, -1.3906e-01,\n",
      "          4.3811e-02,  3.8443e-02,  6.2318e-02,  2.1330e-01, -2.2345e-01,\n",
      "          1.7830e-01, -8.6236e-02,  1.6524e-02, -5.7871e-02,  7.7610e-02,\n",
      "          1.2366e-01, -4.9218e-02,  1.2260e-01, -1.5717e-02, -2.0787e-01],\n",
      "        [-9.2980e-02,  1.8759e-01, -1.9091e-01,  3.7170e-02,  1.1819e-01,\n",
      "          9.5529e-02, -6.7980e-02,  3.6432e-02,  2.5062e-02, -2.0285e-02,\n",
      "          8.0137e-03,  2.0724e-01, -1.7450e-01, -1.5498e-01,  2.1192e-01,\n",
      "          5.4097e-02, -7.5641e-02,  1.8358e-01, -2.7977e-02,  2.2068e-01],\n",
      "        [-2.1934e-01, -2.0985e-01, -2.1551e-01, -4.5139e-02,  2.1881e-01,\n",
      "         -1.7552e-01, -6.9967e-02, -5.2223e-02, -1.0534e-01,  9.7009e-02,\n",
      "          6.5622e-02,  9.2552e-02, -9.3523e-02, -1.6128e-01, -1.2732e-01,\n",
      "         -5.2831e-02,  4.0195e-02,  2.1123e-01,  1.7015e-01,  3.9182e-02],\n",
      "        [ 5.0215e-02,  1.2843e-01,  1.3968e-01,  5.1885e-02,  2.1844e-01,\n",
      "         -2.0586e-01,  1.7492e-01, -1.5135e-01,  5.9046e-02, -1.1330e-01,\n",
      "          1.8952e-01,  1.6146e-01, -1.6862e-01,  1.1586e-01,  1.7803e-02,\n",
      "          1.6761e-01, -1.6417e-01, -3.4482e-03,  1.4872e-01, -1.8848e-01],\n",
      "        [-7.7943e-02, -1.1144e-01,  1.7527e-01, -6.2758e-02,  5.1444e-02,\n",
      "          3.1789e-02, -1.5394e-01,  2.4952e-02, -2.2301e-01, -9.8293e-02,\n",
      "          1.5802e-01,  2.0476e-01,  1.5687e-01,  1.0926e-01, -7.9774e-02,\n",
      "          8.0880e-02,  1.4411e-01,  2.5640e-02, -1.0532e-01, -1.7348e-01],\n",
      "        [ 1.4241e-01, -1.6828e-01,  2.2302e-02, -1.9293e-01, -1.7026e-01,\n",
      "          1.3305e-01,  1.4425e-01, -1.4950e-01, -2.1305e-01,  4.2208e-02,\n",
      "         -2.0190e-01,  7.4339e-02,  1.5647e-01, -1.8959e-01,  1.9551e-01,\n",
      "         -1.1356e-01, -1.5670e-01, -3.1099e-02,  1.7703e-01,  2.2103e-02],\n",
      "        [-1.4285e-01,  1.1870e-01,  2.0690e-01,  1.4520e-01,  5.8460e-02,\n",
      "         -6.7699e-02,  6.7382e-02,  1.3265e-01,  4.1957e-02, -1.1163e-01,\n",
      "         -1.7532e-01,  6.9905e-02, -1.9483e-01, -2.1637e-01,  1.1447e-01,\n",
      "         -6.1254e-02, -7.5136e-02,  1.5306e-02, -2.1797e-01,  1.3088e-01],\n",
      "        [ 2.1963e-01, -9.3554e-02, -2.0704e-01,  2.2096e-01, -2.2020e-01,\n",
      "         -1.8346e-01, -9.7318e-02,  6.2245e-02, -6.6252e-02, -1.8763e-01,\n",
      "          1.2785e-01, -1.7716e-01, -2.1366e-01,  1.0950e-02,  9.6418e-02,\n",
      "         -8.7316e-02,  2.2087e-01,  6.6290e-02, -9.7048e-02,  5.6446e-02],\n",
      "        [ 8.8871e-05,  2.1975e-01,  7.7476e-02,  1.9231e-01, -1.9864e-01,\n",
      "          1.4024e-01, -3.1636e-02,  1.3046e-01,  2.1930e-01,  1.0120e-01,\n",
      "         -1.0040e-01, -6.8748e-02,  1.5128e-01,  2.0417e-01, -2.0594e-01,\n",
      "         -4.8621e-03,  2.1024e-01,  1.5091e-02,  3.5209e-03,  2.9825e-04],\n",
      "        [-8.2602e-02, -6.6321e-02, -1.0343e-01,  2.1081e-01, -1.6564e-01,\n",
      "          9.1757e-02,  1.1495e-01, -1.0652e-01, -1.1054e-01, -6.5821e-02,\n",
      "         -1.9049e-01, -2.1462e-01,  7.6444e-02,  1.2937e-01, -8.7926e-02,\n",
      "         -9.2463e-02, -1.7423e-01,  5.1290e-02,  3.3402e-02, -2.5860e-02],\n",
      "        [ 7.8666e-02, -8.6087e-02, -2.6514e-03, -5.3403e-02,  1.4653e-01,\n",
      "         -1.4779e-01,  1.7072e-01,  1.2451e-01,  7.1488e-02, -1.2356e-01,\n",
      "          5.1058e-02,  6.6953e-02, -4.6398e-02,  1.7303e-01, -1.5537e-01,\n",
      "         -3.5098e-02,  1.9856e-02,  1.7283e-01,  6.2378e-02,  1.6930e-01],\n",
      "        [ 1.5455e-01, -1.8607e-01, -3.1732e-02,  1.5025e-01, -2.0774e-01,\n",
      "          7.8630e-02,  9.4160e-02, -1.6130e-01, -1.9206e-01, -9.9362e-02,\n",
      "          1.5849e-01,  1.0831e-01,  1.9687e-01,  2.0315e-01, -1.8494e-01,\n",
      "         -2.1046e-01, -1.9072e-01,  8.8412e-02, -9.3883e-02, -1.1997e-01],\n",
      "        [-1.0260e-01, -1.2474e-01, -1.8319e-01, -1.3866e-01, -1.3860e-01,\n",
      "         -8.4683e-05, -2.0657e-01, -2.0685e-01, -1.6378e-01,  7.3556e-02,\n",
      "          1.7677e-02, -1.7543e-01,  8.7839e-02,  8.2075e-02, -1.0017e-01,\n",
      "         -1.9645e-01,  2.2326e-01,  1.3086e-01, -1.1636e-01, -1.7945e-01],\n",
      "        [ 1.8453e-01,  5.3815e-02,  1.9063e-01, -1.7960e-01, -1.1429e-01,\n",
      "         -1.4523e-02,  2.5603e-02,  7.7348e-02,  8.2344e-02, -1.2626e-01,\n",
      "         -1.3321e-01, -9.9473e-02,  2.9432e-02, -1.7094e-01,  4.5035e-02,\n",
      "          2.2017e-02,  3.6272e-02,  1.3264e-01, -1.1375e-01,  5.5588e-02],\n",
      "        [-1.1514e-01, -1.9208e-01,  9.6650e-02,  1.4542e-01,  1.8699e-01,\n",
      "          2.1578e-01, -1.6727e-02,  1.5905e-01,  1.8239e-01, -6.6300e-02,\n",
      "          3.5001e-02, -4.2237e-02,  8.1007e-02, -1.7039e-01, -2.0119e-01,\n",
      "         -1.0581e-01,  4.2892e-02, -9.1886e-02,  1.1001e-02,  2.0834e-01],\n",
      "        [ 1.7502e-01,  1.4005e-01,  6.0031e-02, -1.1053e-02, -1.7815e-01,\n",
      "         -2.0036e-01, -1.9158e-01, -2.2008e-01, -4.2505e-02,  9.5557e-02,\n",
      "          1.6253e-01,  1.6897e-01,  2.1534e-01, -1.6175e-01, -3.5641e-02,\n",
      "         -1.4008e-01, -7.5404e-02, -1.9587e-01, -1.3445e-01,  4.0914e-02],\n",
      "        [ 1.2899e-02,  2.0762e-01,  3.4233e-02,  2.0445e-02,  8.9330e-02,\n",
      "          7.7611e-02,  1.7794e-01, -9.1589e-03, -1.0824e-01,  3.8276e-02,\n",
      "         -2.0357e-01, -8.2439e-03, -1.5461e-01,  1.8741e-01,  1.6001e-01,\n",
      "         -5.0443e-02, -2.0199e-01,  1.9690e-01,  1.4117e-01,  1.7150e-01],\n",
      "        [-1.5820e-01,  8.5893e-02,  8.2467e-02, -9.3331e-02, -9.1419e-02,\n",
      "         -1.7498e-02, -2.1388e-02, -2.4414e-02,  1.4908e-01,  1.9724e-01,\n",
      "          3.2650e-02, -2.0720e-02, -1.1668e-02, -1.2708e-01,  2.7543e-02,\n",
      "         -2.7953e-02,  7.9319e-02, -1.1970e-01,  2.0790e-01, -4.7411e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0759,  0.1322,  0.0190,  0.0907, -0.0772,  0.1565,  0.1325, -0.0780,\n",
      "         0.1861, -0.0873,  0.0172,  0.0325,  0.0423,  0.1104, -0.2014, -0.0734,\n",
      "        -0.0265,  0.2133,  0.0229, -0.1095], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0506,  0.0874, -0.0072, -0.0750, -0.0288, -0.1834, -0.2216,  0.1511,\n",
      "          0.0151, -0.0076,  0.1477,  0.0784,  0.1140,  0.1348, -0.1496, -0.1050,\n",
      "          0.0289, -0.1132, -0.1878,  0.0244],\n",
      "        [ 0.1800,  0.0709, -0.1569,  0.0173, -0.1834, -0.1056,  0.0022,  0.0363,\n",
      "          0.0100,  0.0126,  0.0367, -0.1124,  0.2026,  0.1349,  0.1996,  0.0167,\n",
      "          0.1569,  0.1714,  0.1883,  0.0797],\n",
      "        [-0.1948, -0.1814, -0.1382, -0.2044,  0.0357,  0.1093, -0.1940,  0.1852,\n",
      "          0.0856, -0.2162,  0.1066, -0.1375,  0.1594, -0.2029, -0.1675,  0.1779,\n",
      "         -0.0648,  0.0757,  0.1415, -0.1779],\n",
      "        [ 0.0713,  0.0318, -0.0202, -0.0421, -0.0289,  0.0258,  0.0360,  0.1751,\n",
      "         -0.0617, -0.0927,  0.1568, -0.0976,  0.0366,  0.1348,  0.0385,  0.1311,\n",
      "          0.2036, -0.1863, -0.0981,  0.2194],\n",
      "        [ 0.1769,  0.2106,  0.0500,  0.0572,  0.1612, -0.1442,  0.0967,  0.0535,\n",
      "         -0.0285,  0.0668, -0.1290, -0.0477,  0.1759,  0.1071, -0.0257, -0.0842,\n",
      "          0.0435, -0.1133, -0.1613, -0.0874],\n",
      "        [-0.0059, -0.1872, -0.0990, -0.0889, -0.0088,  0.0704,  0.1439, -0.2229,\n",
      "         -0.1688,  0.1123,  0.0183,  0.2073,  0.0386,  0.1857, -0.1412,  0.1078,\n",
      "          0.1104,  0.0514, -0.1828,  0.1837],\n",
      "        [ 0.0504, -0.0065, -0.0518, -0.2086,  0.0322, -0.1019, -0.0017,  0.1998,\n",
      "         -0.0612,  0.2206, -0.1545, -0.0129,  0.0142,  0.0990,  0.1938,  0.0407,\n",
      "         -0.1004,  0.1463,  0.0307,  0.0832],\n",
      "        [ 0.0508,  0.0578, -0.2098,  0.0154, -0.1322,  0.0279,  0.1594, -0.1985,\n",
      "         -0.1201,  0.0139,  0.0480, -0.2224,  0.0779,  0.1985, -0.0274, -0.0960,\n",
      "          0.1546, -0.1420,  0.0039, -0.1321],\n",
      "        [-0.0606,  0.2131, -0.0898, -0.0316, -0.1340, -0.1088, -0.0593, -0.0214,\n",
      "         -0.1103,  0.2004,  0.1680, -0.0593, -0.0961, -0.1460, -0.1442,  0.2130,\n",
      "         -0.0299, -0.2062, -0.2154, -0.1360],\n",
      "        [-0.0004, -0.0321,  0.0270,  0.1753, -0.0391, -0.0160,  0.1444,  0.1636,\n",
      "         -0.0070,  0.1577, -0.1808,  0.1762,  0.0679,  0.0655,  0.1933, -0.1612,\n",
      "         -0.1497,  0.0541,  0.0491,  0.2095],\n",
      "        [ 0.0422,  0.1963,  0.0568,  0.1673,  0.1177,  0.0971, -0.0894, -0.2214,\n",
      "         -0.1987, -0.0887, -0.0841,  0.1283,  0.0882, -0.1325,  0.2013,  0.1488,\n",
      "          0.1696, -0.1826,  0.1246,  0.1029],\n",
      "        [-0.0753,  0.0754,  0.0224,  0.1824,  0.0772, -0.0998,  0.0816, -0.1339,\n",
      "          0.1891,  0.0937, -0.0865, -0.0338, -0.1109,  0.0689,  0.0899, -0.0656,\n",
      "          0.0485,  0.1177,  0.0800,  0.1080],\n",
      "        [-0.1812,  0.0261, -0.1978, -0.0801, -0.0007,  0.1027,  0.0123,  0.1270,\n",
      "          0.0141, -0.0084, -0.1431, -0.1368,  0.0604,  0.0973,  0.1303, -0.1143,\n",
      "         -0.0987,  0.1056,  0.1393, -0.0402],\n",
      "        [-0.0247, -0.0828,  0.0876, -0.1694,  0.0060, -0.0539,  0.1508,  0.0624,\n",
      "         -0.0217, -0.0714, -0.0555, -0.0067,  0.0150, -0.1450,  0.0404,  0.0470,\n",
      "         -0.2171, -0.0391, -0.1374,  0.0004],\n",
      "        [ 0.0557, -0.1996, -0.0295, -0.1419, -0.0250, -0.2152, -0.2068,  0.1214,\n",
      "          0.1977, -0.1526,  0.1063, -0.1138, -0.0853, -0.1649,  0.2143,  0.0136,\n",
      "         -0.0112, -0.0460, -0.1362, -0.1773],\n",
      "        [ 0.1692,  0.1498,  0.2162,  0.0570,  0.0555,  0.1109, -0.0400,  0.1722,\n",
      "          0.2066,  0.1628, -0.1115, -0.1786, -0.0623,  0.0211,  0.1596,  0.0242,\n",
      "          0.1926, -0.0809,  0.1459,  0.1835],\n",
      "        [-0.1149, -0.1422,  0.0573,  0.0200, -0.0022, -0.1758, -0.1188, -0.0849,\n",
      "          0.0520, -0.0458,  0.1089, -0.1127,  0.0600, -0.1562,  0.2040,  0.1110,\n",
      "         -0.0805, -0.1746, -0.1787,  0.2151],\n",
      "        [ 0.1509, -0.1439,  0.1041,  0.1303,  0.2158, -0.2002,  0.0890,  0.1448,\n",
      "          0.2186, -0.0861,  0.1531, -0.1739, -0.1479,  0.1819,  0.1319, -0.0754,\n",
      "         -0.2145,  0.1155,  0.0996, -0.0764],\n",
      "        [ 0.0300,  0.0177, -0.1349,  0.1768,  0.0680,  0.1992, -0.1225,  0.1326,\n",
      "          0.1529,  0.1426, -0.0910, -0.1232,  0.2002, -0.0365, -0.0847,  0.1366,\n",
      "         -0.0964, -0.1053,  0.0993,  0.0870],\n",
      "        [ 0.1676, -0.0862, -0.0912,  0.0228,  0.0460,  0.1655, -0.0791, -0.1596,\n",
      "         -0.1134, -0.2176, -0.0007,  0.0105, -0.1048,  0.1660, -0.0025,  0.0709,\n",
      "          0.2027,  0.2217,  0.0525,  0.1276]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0294, -0.0671,  0.0157, -0.0602, -0.1469, -0.1042,  0.0846, -0.1936,\n",
      "        -0.1481, -0.1027,  0.0781, -0.0855,  0.0831, -0.1890, -0.2015,  0.1946,\n",
      "        -0.0198, -0.0670,  0.1543, -0.2207], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1294, -0.0570, -0.0858, -0.2094, -0.1099,  0.0316,  0.0440,  0.1355,\n",
      "         -0.0024, -0.1916,  0.1585,  0.2037,  0.1383, -0.1029,  0.1017, -0.1216,\n",
      "          0.2075, -0.2226, -0.0616,  0.0073],\n",
      "        [ 0.1883,  0.1468, -0.0326, -0.2136, -0.0860,  0.1297,  0.0443,  0.1248,\n",
      "          0.1751,  0.0276, -0.0537,  0.0566,  0.1874, -0.0438,  0.1171,  0.2231,\n",
      "          0.1400, -0.1709,  0.1924,  0.1801],\n",
      "        [-0.0198,  0.0692, -0.0379,  0.2141,  0.0476,  0.0185, -0.1530,  0.1952,\n",
      "          0.0033, -0.0904, -0.1779,  0.0245, -0.0588, -0.1116, -0.1239,  0.0988,\n",
      "         -0.0641,  0.0894, -0.0395, -0.0076],\n",
      "        [ 0.1552,  0.0650, -0.0825,  0.1906,  0.1440, -0.1463, -0.0169,  0.1626,\n",
      "          0.2006, -0.1338,  0.0130, -0.1830, -0.1639, -0.1731, -0.0891,  0.1675,\n",
      "          0.0246, -0.0573, -0.0722, -0.1050],\n",
      "        [ 0.1219,  0.2177,  0.0726,  0.1171, -0.0580,  0.1588, -0.0044,  0.0247,\n",
      "         -0.1208, -0.1608, -0.0768,  0.0716,  0.0393, -0.1017, -0.0234,  0.2196,\n",
      "          0.1500, -0.0496, -0.2075,  0.0659],\n",
      "        [ 0.1320,  0.1431, -0.2226,  0.0697, -0.1123, -0.2190,  0.1547, -0.0497,\n",
      "         -0.1020,  0.0253,  0.0242,  0.0070, -0.0831, -0.0837, -0.1936, -0.1277,\n",
      "         -0.0680,  0.1451,  0.0960,  0.0645],\n",
      "        [ 0.0030,  0.1700, -0.0971, -0.0350,  0.2142,  0.0641,  0.1730,  0.0550,\n",
      "         -0.1568, -0.2168, -0.1428, -0.0579,  0.2134, -0.0954, -0.1262, -0.2163,\n",
      "         -0.0107,  0.2078,  0.1190,  0.0675],\n",
      "        [-0.0562, -0.1805,  0.0392, -0.0119, -0.0813, -0.1723, -0.1264, -0.0490,\n",
      "         -0.1658,  0.1504,  0.0846,  0.1644, -0.1183, -0.1932, -0.0133,  0.0222,\n",
      "         -0.0482, -0.1825, -0.1313,  0.2227],\n",
      "        [-0.0711,  0.1762,  0.0393, -0.0368, -0.1973, -0.1859, -0.0930, -0.0769,\n",
      "          0.0611,  0.1313, -0.0913, -0.1099, -0.0024, -0.0847,  0.0058, -0.0576,\n",
      "         -0.0244,  0.1904,  0.0161,  0.2215],\n",
      "        [ 0.0142,  0.2084,  0.0345, -0.0734, -0.2234,  0.1857,  0.0711,  0.2044,\n",
      "          0.1011,  0.1200, -0.0580, -0.0535, -0.0289,  0.0901, -0.2081,  0.1209,\n",
      "         -0.0200,  0.0990,  0.1313, -0.1865],\n",
      "        [-0.1359,  0.0801, -0.2215,  0.0453,  0.0650,  0.2140,  0.2051, -0.0118,\n",
      "          0.1187, -0.0232, -0.1535,  0.0993,  0.0178, -0.0900, -0.2119, -0.0351,\n",
      "          0.1942,  0.1778, -0.0182, -0.0083],\n",
      "        [ 0.1212, -0.1911,  0.0964,  0.0718, -0.0093,  0.0997,  0.0952,  0.2131,\n",
      "          0.0680, -0.0706, -0.0931, -0.0416,  0.1255, -0.0626,  0.2153, -0.1437,\n",
      "         -0.1008,  0.0523, -0.0045, -0.0426],\n",
      "        [-0.1171, -0.0598,  0.1779, -0.0717,  0.0575, -0.1807,  0.0971, -0.2192,\n",
      "          0.0169,  0.1607, -0.0853,  0.1624, -0.0286,  0.0070,  0.1009, -0.0641,\n",
      "         -0.2094, -0.0697, -0.2133, -0.1791],\n",
      "        [-0.0835,  0.0536, -0.0191,  0.0780,  0.0747, -0.0008,  0.0578, -0.0939,\n",
      "         -0.1737, -0.1465, -0.0080, -0.2101, -0.0102,  0.1580, -0.0881, -0.0498,\n",
      "          0.0846,  0.0711, -0.0457, -0.1528],\n",
      "        [-0.1643, -0.2157, -0.2141,  0.1545,  0.2063, -0.0990,  0.0039,  0.2001,\n",
      "         -0.1179,  0.2134,  0.0798,  0.0088,  0.0546,  0.1578,  0.0719,  0.0519,\n",
      "          0.1379, -0.0835,  0.0069,  0.0561],\n",
      "        [-0.2123, -0.1163,  0.1133, -0.1709,  0.1481,  0.0487, -0.0793, -0.0195,\n",
      "         -0.0515, -0.1988, -0.0420,  0.0943,  0.1070,  0.1792, -0.0373,  0.0989,\n",
      "         -0.1591, -0.0150, -0.1111, -0.1208],\n",
      "        [-0.0811, -0.0109, -0.1862,  0.1573,  0.0531, -0.2113, -0.0878, -0.0047,\n",
      "          0.0863,  0.1363, -0.1350, -0.0397, -0.1843, -0.0632, -0.0104, -0.2201,\n",
      "         -0.2215, -0.1591,  0.0598, -0.0478],\n",
      "        [-0.0615, -0.1898,  0.2037,  0.1800,  0.1935, -0.0754,  0.0002, -0.1230,\n",
      "         -0.0479, -0.0231,  0.0020,  0.0215, -0.2187,  0.0425, -0.0659, -0.0289,\n",
      "          0.0596,  0.2205, -0.0881,  0.1360],\n",
      "        [-0.0299,  0.1366,  0.0041, -0.1055,  0.1943,  0.1895,  0.2210, -0.0910,\n",
      "         -0.0288, -0.0029, -0.0067,  0.0737, -0.1325,  0.0400,  0.1548, -0.0690,\n",
      "          0.1761, -0.0361,  0.0636, -0.0453],\n",
      "        [ 0.0843, -0.0495,  0.1185, -0.2153, -0.1162,  0.0662,  0.2119, -0.0408,\n",
      "          0.0440,  0.1002,  0.0303,  0.1903, -0.0997, -0.0862, -0.1840,  0.0985,\n",
      "          0.1025, -0.1262, -0.0753, -0.1012]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0840, -0.1653,  0.1171,  0.1648, -0.0105, -0.1455, -0.1311,  0.0982,\n",
      "         0.0464,  0.1954,  0.0378,  0.1689,  0.1989, -0.0623,  0.0959,  0.1431,\n",
      "         0.0398, -0.0742, -0.0047, -0.0086], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1622, -0.0215,  0.1595, -0.2112, -0.0125,  0.0102, -0.1051,  0.1039,\n",
      "         -0.1472,  0.1433, -0.0405,  0.0066, -0.1397,  0.0696,  0.0116,  0.0102,\n",
      "          0.0738,  0.0586,  0.1797, -0.2153],\n",
      "        [ 0.2222, -0.0469,  0.0853,  0.0702,  0.2189, -0.1760,  0.1178, -0.0470,\n",
      "         -0.1682, -0.1099, -0.0500, -0.1101,  0.1817, -0.0942,  0.0192, -0.0762,\n",
      "         -0.0541, -0.1031, -0.2233, -0.0307],\n",
      "        [-0.0411, -0.1090, -0.0176, -0.0942, -0.0947, -0.2230, -0.1776,  0.0654,\n",
      "         -0.0162, -0.1066, -0.0751, -0.0754,  0.0331, -0.0234, -0.0231, -0.0935,\n",
      "         -0.0106,  0.0466,  0.1586, -0.0275],\n",
      "        [ 0.1490, -0.2149,  0.0092,  0.0030, -0.0276,  0.1941,  0.0603,  0.1432,\n",
      "         -0.1394, -0.1537,  0.0287,  0.1605,  0.0061,  0.0727,  0.1755, -0.0406,\n",
      "         -0.1828,  0.0815, -0.0430, -0.0163],\n",
      "        [-0.0663, -0.1471,  0.0458,  0.2062, -0.0361,  0.0440, -0.0827,  0.0877,\n",
      "          0.0223,  0.1287, -0.1549,  0.0571, -0.0916,  0.0833,  0.1718,  0.1659,\n",
      "         -0.1823,  0.2156, -0.1680, -0.1007],\n",
      "        [-0.1916, -0.0266,  0.0638,  0.1736, -0.0192, -0.1915,  0.0315, -0.2035,\n",
      "          0.0463,  0.1510, -0.0349, -0.1409, -0.2093, -0.1851, -0.1114, -0.0240,\n",
      "          0.1790, -0.2119,  0.0487, -0.1025],\n",
      "        [ 0.1150, -0.0429, -0.1968, -0.0420, -0.1835,  0.0213, -0.0468,  0.0660,\n",
      "          0.1780,  0.0486,  0.0876,  0.2057,  0.1023, -0.0250,  0.1541, -0.1833,\n",
      "         -0.1985,  0.0777,  0.1562, -0.1691],\n",
      "        [-0.1197, -0.0646, -0.0837,  0.1446,  0.0805, -0.0765,  0.1127,  0.0823,\n",
      "          0.1723, -0.0923, -0.0354,  0.0312, -0.2170, -0.2134,  0.2216,  0.0040,\n",
      "         -0.1533,  0.2232, -0.0739,  0.0651],\n",
      "        [-0.1309, -0.0593,  0.2057,  0.0933, -0.0281,  0.1935, -0.1812, -0.1250,\n",
      "          0.0466,  0.0204,  0.0339, -0.1344,  0.1865,  0.0886, -0.1619,  0.1078,\n",
      "         -0.0632,  0.2014, -0.0442,  0.0628],\n",
      "        [-0.1007,  0.2028, -0.1803,  0.0746, -0.1076,  0.0485, -0.0167, -0.0102,\n",
      "          0.1072, -0.1965,  0.1182, -0.0295,  0.0318, -0.1022,  0.1799,  0.0751,\n",
      "         -0.0948,  0.1664, -0.0574, -0.1279],\n",
      "        [-0.1025,  0.2167, -0.1420, -0.0948,  0.1420,  0.0176,  0.1940, -0.0347,\n",
      "         -0.1403, -0.1608, -0.2126,  0.1535,  0.1194,  0.0238, -0.1833,  0.1825,\n",
      "         -0.0655,  0.0170,  0.0105,  0.1767],\n",
      "        [ 0.0013, -0.1257,  0.0497, -0.0521, -0.1736,  0.1817,  0.0745,  0.1466,\n",
      "         -0.2043,  0.0775, -0.2179, -0.1102, -0.0055, -0.2090,  0.0004,  0.0318,\n",
      "          0.0868, -0.1812, -0.0355, -0.0490],\n",
      "        [-0.0332, -0.0073,  0.0010,  0.1345,  0.1954,  0.0614,  0.1382,  0.0422,\n",
      "          0.0939,  0.1495,  0.1765, -0.0009,  0.1794, -0.0433, -0.0564,  0.0834,\n",
      "          0.1691,  0.0058,  0.1077,  0.0810],\n",
      "        [ 0.1711,  0.0458, -0.0708, -0.0302, -0.1980, -0.0087, -0.1481, -0.0170,\n",
      "          0.0836, -0.0822,  0.1123, -0.0414,  0.0355,  0.1584,  0.0757, -0.0443,\n",
      "          0.0407, -0.0030, -0.1677,  0.1142],\n",
      "        [-0.2192,  0.1144,  0.0316, -0.2192,  0.0585,  0.1387,  0.0246,  0.0623,\n",
      "          0.0883,  0.0222,  0.1351, -0.1495, -0.1246, -0.0958, -0.0200, -0.2080,\n",
      "         -0.0886,  0.2051, -0.0071, -0.1939],\n",
      "        [ 0.1986, -0.0206, -0.1875,  0.1499, -0.0377, -0.1869, -0.0894, -0.2068,\n",
      "         -0.1869,  0.1537,  0.0182,  0.1825,  0.0643,  0.1215, -0.1616,  0.1268,\n",
      "          0.1765, -0.2210, -0.1217, -0.0889],\n",
      "        [-0.1671,  0.1428,  0.0582, -0.1029,  0.1637,  0.1616, -0.0787,  0.1741,\n",
      "         -0.2236, -0.0728,  0.0121,  0.1237,  0.1240, -0.0147,  0.0828, -0.0921,\n",
      "         -0.1075,  0.1774, -0.1269, -0.1301],\n",
      "        [ 0.0133,  0.1580,  0.0989,  0.2030,  0.0868, -0.1697,  0.1364, -0.0927,\n",
      "          0.0724, -0.2049, -0.0371,  0.0511,  0.0895,  0.0585, -0.1753,  0.1527,\n",
      "         -0.0408, -0.0505,  0.0933,  0.2192],\n",
      "        [ 0.0701,  0.0450,  0.0391,  0.1846, -0.1611, -0.1967, -0.1106, -0.1918,\n",
      "          0.2129, -0.1294, -0.1146, -0.1745, -0.0606,  0.1118,  0.1785, -0.2060,\n",
      "          0.0215,  0.0996,  0.1834, -0.1299],\n",
      "        [-0.1649,  0.1634, -0.2149,  0.1205, -0.1608, -0.2230,  0.1809, -0.1440,\n",
      "         -0.0919,  0.0823,  0.1827, -0.1346, -0.0712, -0.2213,  0.0683, -0.0053,\n",
      "          0.1867, -0.1253,  0.0174,  0.1465]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0976, -0.0562,  0.0861, -0.2098,  0.0164, -0.1910,  0.0768, -0.0639,\n",
      "        -0.0127,  0.0026, -0.1914,  0.1059, -0.1987, -0.1392,  0.0838, -0.1983,\n",
      "         0.1867, -0.1865, -0.2156,  0.0560], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.6377e-01,  7.4653e-02, -1.1569e-01, -1.7416e-01,  1.1844e-02,\n",
      "          8.2795e-02, -2.1991e-01, -2.1427e-01,  7.3495e-02, -6.1270e-02,\n",
      "         -1.0789e-01, -3.5584e-02,  1.7912e-01,  4.5703e-02,  1.7014e-01,\n",
      "          2.7155e-02,  1.4987e-01, -1.3921e-01,  9.3370e-02, -4.5493e-02],\n",
      "        [-1.2729e-01,  7.1050e-02, -4.1854e-02, -1.2739e-01, -1.0364e-01,\n",
      "         -9.3272e-02,  6.1132e-03,  1.0954e-01,  5.1501e-02, -1.5402e-01,\n",
      "          5.5185e-02,  1.9504e-02, -7.4384e-02,  1.3551e-01,  2.7441e-02,\n",
      "          5.9387e-03,  1.8713e-01,  9.4671e-02,  1.2831e-01, -3.7321e-02],\n",
      "        [-1.2524e-01,  3.2689e-02,  4.7451e-02,  1.0237e-01,  2.1341e-01,\n",
      "         -9.1025e-02, -4.1973e-02, -3.0828e-02, -1.0587e-01, -1.9125e-01,\n",
      "         -3.0127e-02,  2.0183e-01, -7.0426e-02,  1.3851e-01,  1.5119e-01,\n",
      "         -2.0225e-01,  4.3236e-02,  1.7809e-01, -3.3633e-02, -1.3703e-01],\n",
      "        [-1.4165e-01,  1.6319e-02,  2.0923e-01,  5.2251e-02,  2.0119e-01,\n",
      "         -8.6841e-02,  6.6162e-02, -1.7873e-01, -1.6881e-01,  2.1801e-02,\n",
      "         -2.8153e-02,  2.1130e-01,  7.6320e-02, -1.0342e-02, -2.0258e-01,\n",
      "         -1.3800e-01,  2.2039e-01,  1.6812e-01, -1.3631e-01,  4.4911e-02],\n",
      "        [-1.0249e-01,  1.0043e-01,  8.4321e-02, -1.4217e-01, -9.5707e-02,\n",
      "         -1.0996e-01,  2.0293e-01,  8.8254e-02,  2.1687e-01,  1.4676e-01,\n",
      "         -1.6944e-01, -1.9938e-02,  1.8929e-01, -6.1239e-02,  2.1336e-01,\n",
      "          1.0568e-01, -1.6878e-01,  1.5281e-01,  2.2086e-01, -8.3093e-02],\n",
      "        [-4.0305e-02, -1.2966e-01, -1.1219e-01,  1.2398e-01,  1.6589e-01,\n",
      "          1.0325e-01,  9.1025e-02, -5.9113e-02,  6.7020e-02, -1.4018e-01,\n",
      "         -7.1642e-02,  8.7771e-02,  1.2972e-01,  1.0175e-01, -2.0563e-01,\n",
      "          1.0325e-01, -1.2203e-01, -8.1234e-02,  1.9722e-01,  8.7998e-02],\n",
      "        [-2.0456e-01,  9.2459e-02, -1.8978e-01,  3.4382e-02,  2.0636e-01,\n",
      "         -1.6415e-02,  1.4803e-01, -8.6159e-02,  9.0317e-02,  5.6005e-02,\n",
      "         -7.8011e-02, -1.5277e-01,  1.5963e-01, -1.2895e-01, -7.5178e-02,\n",
      "          9.0411e-02, -9.5163e-02, -6.1919e-02, -1.1670e-01,  2.4711e-02],\n",
      "        [-8.1778e-03,  1.4452e-02, -1.6450e-01, -1.3345e-01,  1.2292e-01,\n",
      "         -8.6540e-02,  1.6455e-01,  1.1636e-01,  1.4259e-01,  8.1045e-02,\n",
      "          1.4306e-01,  2.2767e-02,  9.9463e-03,  5.0125e-02, -5.8301e-02,\n",
      "         -5.6879e-02,  5.2865e-04, -1.9317e-01,  2.0553e-01, -4.2670e-02],\n",
      "        [ 1.3811e-02,  2.0831e-01, -1.3468e-01, -2.8668e-02,  1.1087e-01,\n",
      "          1.1693e-01,  1.4080e-01, -9.0603e-02, -8.8850e-02, -9.8454e-02,\n",
      "         -4.0564e-02, -4.5617e-02,  1.1356e-01,  1.1002e-01, -1.0438e-01,\n",
      "          1.6509e-01, -4.8279e-02, -1.4247e-01,  5.8916e-02,  3.3823e-02],\n",
      "        [ 9.8315e-02, -1.9469e-01,  2.1615e-01, -1.2730e-01, -2.0352e-01,\n",
      "          2.1888e-01,  1.3642e-01, -1.8112e-01, -1.4731e-01, -3.7319e-02,\n",
      "         -1.6480e-01, -3.0568e-02, -4.9872e-02, -1.8334e-01, -1.6172e-02,\n",
      "          8.8659e-02,  1.2065e-01,  2.0645e-01, -1.7372e-01,  1.1635e-01],\n",
      "        [-1.6307e-02, -2.2001e-02, -8.9830e-03, -1.0182e-01, -9.8158e-03,\n",
      "         -1.0925e-01, -9.1777e-02, -1.0767e-01, -9.0977e-02, -5.9465e-02,\n",
      "          1.5616e-01,  2.0540e-01, -1.4747e-01, -2.0136e-01, -2.1917e-01,\n",
      "         -1.6427e-02, -1.8891e-01, -2.9328e-02,  1.8333e-01, -1.2578e-01],\n",
      "        [ 8.0426e-02, -1.4858e-01,  1.1120e-01, -6.5642e-02,  7.5159e-02,\n",
      "          1.2329e-02,  1.3843e-01, -1.0785e-01, -2.8549e-02,  9.0018e-02,\n",
      "         -1.3247e-01, -9.0938e-02, -1.9721e-01,  1.3407e-01, -9.9585e-02,\n",
      "          1.1073e-02,  2.2114e-01,  1.3453e-01, -1.8665e-01,  2.2123e-01],\n",
      "        [-1.7984e-01, -2.1419e-01, -1.1101e-02,  1.0926e-01, -2.1241e-01,\n",
      "          1.9342e-01, -1.4000e-01,  2.0250e-01,  1.1008e-01, -1.8270e-04,\n",
      "          2.5577e-02,  4.1046e-02, -4.1529e-02,  1.8927e-01, -8.4536e-02,\n",
      "          2.0152e-01,  4.9852e-02, -8.6333e-02,  2.2289e-01, -1.8653e-01],\n",
      "        [-2.2307e-03, -1.8504e-01, -1.2275e-01,  3.7036e-03,  2.0402e-01,\n",
      "          8.0061e-02, -1.9679e-01,  1.3840e-01,  3.7156e-02,  4.6610e-02,\n",
      "          3.6496e-02, -1.4409e-01,  1.4943e-01, -1.0771e-01, -1.4895e-02,\n",
      "         -1.7893e-01, -6.5227e-02,  2.1971e-01, -1.1257e-02, -1.8436e-01],\n",
      "        [ 2.2279e-01, -7.0924e-02, -1.4577e-01,  2.1688e-01,  1.1911e-02,\n",
      "          4.6526e-02,  1.3870e-01, -1.7470e-02,  1.2234e-01, -1.6276e-01,\n",
      "          6.5521e-02, -9.2023e-02,  2.1657e-01,  1.9641e-01,  1.3583e-01,\n",
      "         -1.3385e-01,  1.4707e-01,  1.1573e-01, -3.7838e-02,  4.5901e-02],\n",
      "        [-1.1118e-01,  6.6796e-02, -1.1136e-01,  1.1290e-01,  5.3508e-02,\n",
      "         -5.5573e-02,  1.2383e-01,  1.9528e-01, -5.3292e-02,  2.2326e-01,\n",
      "          1.1351e-01,  8.9640e-02,  2.1195e-01, -4.9429e-02,  1.1004e-01,\n",
      "         -1.3485e-02, -1.4049e-01,  1.4613e-01, -1.4263e-01, -6.6632e-02],\n",
      "        [-4.7651e-02, -1.7558e-01, -1.4779e-01,  1.7736e-01,  7.5658e-02,\n",
      "          1.2242e-01, -1.7817e-01,  3.5539e-02, -1.1617e-01, -2.0169e-02,\n",
      "          5.1780e-02, -6.4297e-02,  8.3625e-02,  1.5521e-01,  1.0433e-01,\n",
      "          1.4105e-01,  1.7550e-02, -1.2851e-02,  8.7799e-02,  1.1283e-01],\n",
      "        [-1.1471e-02, -2.8657e-03,  3.4017e-02, -1.3359e-02,  2.5269e-02,\n",
      "          1.3979e-01, -1.4121e-01, -1.8965e-01, -2.2301e-01, -1.0114e-01,\n",
      "          1.1815e-01, -1.8602e-01, -1.4301e-01, -6.7218e-03,  1.7499e-01,\n",
      "          1.8519e-01, -6.3410e-02,  5.3078e-02,  7.2209e-02,  4.2678e-02],\n",
      "        [ 9.7669e-02,  1.7897e-01,  1.9028e-01, -3.4162e-02, -1.3123e-01,\n",
      "         -1.5318e-01, -2.2069e-01, -1.9438e-03, -1.6442e-01, -1.0695e-01,\n",
      "          1.8026e-01,  1.8756e-01, -1.0257e-02, -1.2748e-01,  1.7281e-01,\n",
      "         -1.3726e-01,  8.9181e-02, -9.4155e-02, -3.8139e-02, -1.7367e-01],\n",
      "        [-2.1483e-01,  1.4280e-01,  1.8324e-01,  1.6012e-01,  1.1364e-01,\n",
      "         -2.7284e-02,  1.8302e-02,  6.1133e-02,  1.4988e-01, -1.3479e-01,\n",
      "         -1.5366e-01,  1.3350e-01, -1.5467e-02,  2.3513e-02,  7.8823e-02,\n",
      "         -8.2944e-02,  7.8582e-02,  5.9242e-02, -1.6394e-01, -1.5141e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1800,  0.1914,  0.1415,  0.1479, -0.1731, -0.0213,  0.0379, -0.2103,\n",
      "        -0.1267,  0.0347, -0.0941,  0.1064,  0.0308, -0.1863,  0.0954, -0.1394,\n",
      "         0.1836,  0.1541,  0.1534, -0.1117], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0283,  0.0918, -0.0046,  0.0098,  0.1716,  0.0473, -0.0725,  0.1742,\n",
      "         -0.0919,  0.2128,  0.1391,  0.0466,  0.0373, -0.0378, -0.0340, -0.0087,\n",
      "          0.1531,  0.1286,  0.1191, -0.0081],\n",
      "        [ 0.1990,  0.0373,  0.2031,  0.1296,  0.1540,  0.1470, -0.0844, -0.1450,\n",
      "         -0.0952,  0.0721,  0.0898,  0.1928, -0.0220,  0.1670, -0.2152,  0.1611,\n",
      "         -0.1865, -0.1448, -0.1686,  0.1619],\n",
      "        [-0.1389, -0.1660,  0.1080, -0.1190,  0.1277, -0.1663, -0.0353,  0.2202,\n",
      "          0.0242, -0.1670, -0.1499, -0.0872, -0.1400,  0.1507, -0.0535,  0.1661,\n",
      "         -0.1113,  0.0351, -0.1863, -0.1458],\n",
      "        [ 0.1642, -0.0095,  0.2161, -0.2006,  0.1187, -0.0410, -0.0637,  0.2193,\n",
      "         -0.0083,  0.2035,  0.1736, -0.1793, -0.1848, -0.0335, -0.0089, -0.0687,\n",
      "         -0.0967, -0.2013, -0.0124, -0.0786],\n",
      "        [ 0.1569, -0.1117, -0.1703, -0.1832, -0.0257,  0.0977,  0.2225,  0.0272,\n",
      "          0.1411,  0.0008, -0.1730, -0.0823, -0.0625, -0.1927, -0.0477, -0.1097,\n",
      "         -0.1751,  0.0114,  0.1729,  0.0428],\n",
      "        [ 0.1657,  0.1560,  0.0568, -0.0405,  0.0038,  0.1880,  0.0053,  0.1446,\n",
      "         -0.0588, -0.0266, -0.1024, -0.1906, -0.1862, -0.2092, -0.0004,  0.0973,\n",
      "         -0.1508,  0.2027,  0.1631,  0.1923],\n",
      "        [-0.1630, -0.1334, -0.1369, -0.0914,  0.1148, -0.2028, -0.1109, -0.0276,\n",
      "          0.0160,  0.2026, -0.0430,  0.1950,  0.1337, -0.2016, -0.0690,  0.0057,\n",
      "         -0.0249,  0.1158, -0.2169, -0.0219],\n",
      "        [-0.1266, -0.1990,  0.0299, -0.0151, -0.0670, -0.1360,  0.0290, -0.0495,\n",
      "         -0.1548,  0.1418, -0.0271,  0.0708, -0.1662,  0.0543, -0.1882, -0.0982,\n",
      "         -0.0290, -0.1168,  0.0044, -0.0869],\n",
      "        [-0.0064, -0.1308, -0.1002, -0.1990,  0.1688, -0.0038,  0.1999,  0.2134,\n",
      "          0.1634,  0.0106,  0.0350, -0.0603,  0.1534, -0.1196, -0.0950,  0.1440,\n",
      "          0.0531, -0.0301,  0.0009, -0.1740],\n",
      "        [-0.0489,  0.0937,  0.1108,  0.0683, -0.1798,  0.0452,  0.1631, -0.0249,\n",
      "          0.0625,  0.0629, -0.1916, -0.0747,  0.0406, -0.1383,  0.0386,  0.1581,\n",
      "          0.0614, -0.1846,  0.1170, -0.0995],\n",
      "        [-0.1292,  0.1360,  0.1218, -0.0247, -0.1788, -0.1060,  0.1021, -0.0629,\n",
      "         -0.1302,  0.1938,  0.0242, -0.0159,  0.1503, -0.0114, -0.1087, -0.0584,\n",
      "         -0.0875,  0.0484, -0.0326,  0.0560],\n",
      "        [-0.1020,  0.0154, -0.1487,  0.0099,  0.0271, -0.2108,  0.1419,  0.1540,\n",
      "          0.0368, -0.0210, -0.0384,  0.2056, -0.0149, -0.0538,  0.1315,  0.1399,\n",
      "         -0.1752,  0.2183, -0.2195, -0.1774],\n",
      "        [-0.1932,  0.0837, -0.0015, -0.0675,  0.1656, -0.1014, -0.0521, -0.1204,\n",
      "         -0.0671, -0.0495,  0.0242,  0.0771, -0.0543, -0.1719, -0.1665, -0.1569,\n",
      "          0.0794,  0.0362, -0.2233,  0.0831],\n",
      "        [-0.0084, -0.1474, -0.0561, -0.1718,  0.1424, -0.0737,  0.0372,  0.1510,\n",
      "         -0.0978, -0.1559, -0.0034,  0.1628,  0.1857,  0.1229, -0.0762,  0.2232,\n",
      "          0.1347,  0.1234, -0.1122,  0.1098],\n",
      "        [-0.0900,  0.0193, -0.0693, -0.1826,  0.0565,  0.0012, -0.0717,  0.2109,\n",
      "          0.0393, -0.0987, -0.0870,  0.2143,  0.0241,  0.1049, -0.0006, -0.1691,\n",
      "         -0.1847,  0.2219, -0.0687, -0.2156],\n",
      "        [-0.0048, -0.1937, -0.0572, -0.1802,  0.1704, -0.2144,  0.1155, -0.1721,\n",
      "         -0.1360, -0.0691, -0.0465,  0.0564,  0.1202,  0.2076, -0.0924, -0.1720,\n",
      "          0.1777,  0.1113, -0.1360,  0.1742],\n",
      "        [-0.2204,  0.1999,  0.2059, -0.1422,  0.0540, -0.2092, -0.2235, -0.0283,\n",
      "         -0.1249,  0.2075, -0.0121, -0.0093, -0.1250,  0.2043, -0.0513,  0.0537,\n",
      "         -0.0151,  0.1129,  0.0192, -0.0044],\n",
      "        [ 0.1955, -0.1609, -0.0418,  0.2141,  0.1621, -0.1756,  0.0612,  0.1053,\n",
      "          0.0309, -0.1154,  0.2002,  0.1129,  0.0170,  0.2068, -0.1163,  0.1681,\n",
      "          0.2036, -0.1319, -0.0242,  0.0965],\n",
      "        [ 0.0124,  0.1377, -0.1206, -0.1819,  0.0642, -0.0576, -0.0883,  0.0911,\n",
      "         -0.2227, -0.0725,  0.0814,  0.0726,  0.0267,  0.1274, -0.1520,  0.1357,\n",
      "          0.0988, -0.0020, -0.1808, -0.1644],\n",
      "        [ 0.1200,  0.2100, -0.1901, -0.0562, -0.0532, -0.2040, -0.2174,  0.1542,\n",
      "         -0.0440,  0.1336,  0.0700, -0.0322,  0.0652,  0.0563,  0.0487,  0.0457,\n",
      "         -0.2133, -0.1654, -0.1747, -0.1487]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1735, -0.1367,  0.0728, -0.1502, -0.0129,  0.1617, -0.1116, -0.1126,\n",
      "         0.2119,  0.0086,  0.1093,  0.0750, -0.0198, -0.2231,  0.1887,  0.2038,\n",
      "        -0.0216,  0.1258,  0.2123, -0.1537], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0881, -0.0431, -0.1585,  0.0928, -0.0109,  0.0226, -0.0446,  0.0898,\n",
      "         -0.0309,  0.0579, -0.2162, -0.2230, -0.0942, -0.0033, -0.0345,  0.2023,\n",
      "          0.1703, -0.0251, -0.1815,  0.1399],\n",
      "        [-0.0805,  0.1231, -0.1051, -0.1174, -0.1759,  0.0910, -0.0347,  0.1149,\n",
      "          0.1122,  0.2070, -0.1744,  0.1387, -0.1151,  0.1188,  0.0277, -0.2036,\n",
      "          0.1489,  0.0700,  0.0629,  0.1158],\n",
      "        [ 0.1109,  0.2006, -0.1510,  0.0695, -0.0945,  0.1004,  0.0210,  0.0858,\n",
      "         -0.0198, -0.1210,  0.2126,  0.1991,  0.0470, -0.1648,  0.0586,  0.2094,\n",
      "          0.1305,  0.0632,  0.0108,  0.1075],\n",
      "        [ 0.1858, -0.0708, -0.0896,  0.0377, -0.0252,  0.1188, -0.0295, -0.1977,\n",
      "          0.1064,  0.0411, -0.0518, -0.2102,  0.0579, -0.2190, -0.1086, -0.1948,\n",
      "         -0.1933,  0.1435, -0.0974,  0.0887],\n",
      "        [-0.0100,  0.0637, -0.1296,  0.0189, -0.0468,  0.2006, -0.1223, -0.1376,\n",
      "          0.1512,  0.0720,  0.0842, -0.1846, -0.0199,  0.1150,  0.1781,  0.1902,\n",
      "         -0.1666,  0.0230,  0.0747, -0.0034],\n",
      "        [-0.1011, -0.0883,  0.1338, -0.1458, -0.0457,  0.1716, -0.0950,  0.1192,\n",
      "          0.2045,  0.2139,  0.0609, -0.0543, -0.1958, -0.0896, -0.0935,  0.0137,\n",
      "         -0.1931,  0.0851,  0.1344, -0.1934],\n",
      "        [ 0.2148,  0.0512, -0.0309, -0.2205,  0.1338,  0.1645,  0.1557,  0.0982,\n",
      "         -0.1700, -0.1212,  0.2064, -0.1805, -0.1957, -0.1897, -0.0570,  0.0698,\n",
      "         -0.1152,  0.0543, -0.1894,  0.2214],\n",
      "        [ 0.1566,  0.1205, -0.2088,  0.1064,  0.0400, -0.1975, -0.0764,  0.2168,\n",
      "          0.1773,  0.0931, -0.1302, -0.0313,  0.0323, -0.0157, -0.1196,  0.0567,\n",
      "          0.1029,  0.1565, -0.1610,  0.1051],\n",
      "        [ 0.0557, -0.2054,  0.0125, -0.1288,  0.0469,  0.0396, -0.1155, -0.0877,\n",
      "          0.0557,  0.0603, -0.0296, -0.2200,  0.1320,  0.1098, -0.1602,  0.2047,\n",
      "          0.0012,  0.1538, -0.1630, -0.0903],\n",
      "        [ 0.1136, -0.1776, -0.2022, -0.0129,  0.0034, -0.0449, -0.0503, -0.0432,\n",
      "         -0.1895, -0.0471, -0.0856,  0.2098, -0.1331, -0.1696, -0.1388,  0.1359,\n",
      "         -0.1572,  0.0143,  0.1193,  0.1156]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0032,  0.1759,  0.1626,  0.2002, -0.1470,  0.1121,  0.0114, -0.1158,\n",
      "        -0.1414,  0.0895], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0982, -0.1175,  0.2125, -0.0518,  0.2587,  0.2315, -0.1218,  0.2978,\n",
      "          0.2308, -0.0730],\n",
      "        [-0.2390, -0.2455, -0.1838,  0.0491,  0.2972,  0.2369,  0.1924,  0.1194,\n",
      "         -0.0573, -0.1805],\n",
      "        [-0.2645, -0.2643,  0.0857,  0.2401,  0.1463,  0.1694,  0.1984,  0.1186,\n",
      "         -0.3094, -0.0368],\n",
      "        [ 0.2686, -0.1221, -0.0677,  0.1713, -0.0703, -0.2671,  0.0547,  0.0544,\n",
      "          0.1696, -0.3134]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1549,  0.0867,  0.2418, -0.0513], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2214, -0.6870],\n",
      "        [ 0.5083,  0.0315],\n",
      "        [-0.4100, -0.3378],\n",
      "        [ 0.1125, -0.1580],\n",
      "        [ 0.4664,  0.1557],\n",
      "        [ 0.2745,  0.3667],\n",
      "        [ 0.3864,  0.3302],\n",
      "        [ 0.2668,  0.5416],\n",
      "        [-0.1817, -0.5001],\n",
      "        [-0.6380, -0.3038]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.6947, -0.3123, -0.4414,  0.5354,  0.0630,  0.7039,  0.0885,  0.6972,\n",
      "         0.2379,  0.6153], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.5300e-01,  2.0929e-01, -2.4092e-01,  1.7415e-02,  2.5791e-01,\n",
      "         -1.4120e-01,  2.7389e-01, -1.7932e-01,  4.8486e-02,  1.6930e-01],\n",
      "        [-2.9610e-01, -2.8556e-01,  3.2376e-02,  1.9751e-01, -9.6546e-02,\n",
      "          2.8489e-01, -7.6742e-02, -2.4316e-01,  5.0526e-02,  1.3556e-02],\n",
      "        [ 2.4089e-01, -1.5205e-01,  1.7450e-01, -1.6924e-01, -7.0551e-02,\n",
      "          2.9763e-01,  1.2638e-01, -2.5764e-01,  3.0000e-02, -5.0744e-03],\n",
      "        [-2.1715e-01,  1.1224e-01, -1.6651e-01,  6.3811e-02, -1.0761e-01,\n",
      "          2.8156e-01,  8.6364e-02, -1.2446e-01, -1.5942e-01,  2.7557e-01],\n",
      "        [ 2.3551e-01,  3.2621e-02,  1.5568e-01,  1.7023e-01, -2.1477e-02,\n",
      "          1.7811e-01,  1.5543e-01, -1.5282e-03,  1.4560e-01,  1.9026e-01],\n",
      "        [-9.3535e-02, -1.8086e-01, -3.1265e-03, -3.2492e-02, -2.4309e-01,\n",
      "         -7.4445e-02,  2.9623e-01, -7.0795e-03, -2.8574e-01,  2.3517e-01],\n",
      "        [-6.3028e-02,  3.1388e-01, -1.2916e-01, -1.4695e-01,  1.7106e-01,\n",
      "         -7.6992e-02, -3.1592e-01, -8.8879e-02, -1.6382e-01,  8.6458e-02],\n",
      "        [-2.4944e-01,  3.4266e-02, -9.6421e-03,  2.3625e-01,  2.6654e-01,\n",
      "          1.6024e-01,  2.8661e-01, -2.3362e-01, -1.7707e-01,  2.3311e-01],\n",
      "        [ 2.8710e-01,  1.2533e-01, -2.3911e-02,  2.4045e-01, -1.1379e-01,\n",
      "         -1.5318e-01, -1.3934e-01, -7.2586e-02,  1.8231e-01,  1.6468e-01],\n",
      "        [ 1.6025e-01, -1.3623e-03,  2.5515e-01, -7.7816e-02, -2.1227e-01,\n",
      "          3.0427e-01, -8.7796e-02, -1.8881e-01, -3.7332e-02, -2.1745e-02],\n",
      "        [-3.2731e-02, -2.1626e-01, -2.6083e-01, -1.7953e-01,  2.4110e-02,\n",
      "         -2.5946e-01,  2.3876e-01,  2.4611e-01, -1.5819e-01,  2.4242e-01],\n",
      "        [-1.4459e-02, -2.1380e-01,  2.5659e-01,  2.8801e-01,  3.1292e-01,\n",
      "          1.4313e-01,  2.2620e-01, -1.9833e-01, -1.5185e-01,  1.4101e-02],\n",
      "        [ 2.4147e-01,  7.8484e-02, -1.5615e-02, -5.7755e-02,  5.8905e-03,\n",
      "         -1.1437e-01,  1.8289e-01, -8.9259e-02, -9.2155e-02,  1.3301e-01],\n",
      "        [ 1.2485e-01,  1.1428e-01,  2.5766e-01,  2.6329e-01, -3.0802e-01,\n",
      "         -1.9103e-01, -2.9368e-01,  1.1273e-01, -2.6539e-01,  8.2619e-02],\n",
      "        [-8.5356e-02, -1.2826e-01,  1.9191e-01, -1.9986e-01, -2.5290e-01,\n",
      "          1.4820e-01, -1.7595e-01, -1.1260e-01, -1.5155e-01, -1.9545e-01],\n",
      "        [ 1.6445e-01,  2.2961e-01, -2.0114e-02,  1.5960e-01,  1.5631e-01,\n",
      "         -1.3990e-01, -2.5678e-01,  2.5684e-04,  2.9533e-01,  1.2393e-01],\n",
      "        [ 1.8483e-01, -9.1643e-02, -8.5102e-02, -2.1204e-01,  2.9547e-02,\n",
      "          1.7808e-01,  1.1628e-01,  2.5174e-01, -1.9356e-01, -1.1095e-01],\n",
      "        [ 3.5663e-02, -2.0695e-01,  1.6226e-01, -2.8250e-01, -2.4367e-01,\n",
      "          6.1825e-03,  6.9227e-02,  1.9141e-01,  1.9425e-01,  2.0236e-01],\n",
      "        [ 2.3478e-01, -1.6359e-01,  2.4204e-01,  1.3542e-01,  2.5407e-01,\n",
      "         -2.0657e-01, -2.0379e-01,  2.5996e-01, -2.3776e-02, -2.1280e-01],\n",
      "        [-7.3131e-02, -1.7844e-02,  4.8886e-02, -1.5507e-01,  4.0525e-02,\n",
      "          2.0372e-01, -4.7962e-02, -2.7408e-01,  1.4186e-01,  3.8509e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1264, -0.3037, -0.2699,  0.0935,  0.2266,  0.2191, -0.0326, -0.2841,\n",
      "        -0.1391,  0.1841, -0.0287, -0.0692,  0.1922, -0.1260, -0.0347,  0.2273,\n",
      "        -0.0281,  0.1329,  0.0705,  0.1537], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0849,  0.1270,  0.0510,  0.1617,  0.0681, -0.1585, -0.2005, -0.1632,\n",
      "          0.1121,  0.1128, -0.2134,  0.0247, -0.1271,  0.1680, -0.1203,  0.0245,\n",
      "         -0.1944,  0.1733,  0.0666,  0.1622],\n",
      "        [-0.1084,  0.1159, -0.1415,  0.0808, -0.0454, -0.1036, -0.0459, -0.0905,\n",
      "          0.1960, -0.0434, -0.0933,  0.2170,  0.0826,  0.1232, -0.0829, -0.1437,\n",
      "         -0.0230, -0.1058, -0.1684, -0.0463],\n",
      "        [-0.1083, -0.1210,  0.1524, -0.1750,  0.1789,  0.0750, -0.0132,  0.0287,\n",
      "          0.1950,  0.1078, -0.0248, -0.0564, -0.1965,  0.0464,  0.1551,  0.0163,\n",
      "          0.2191,  0.0676,  0.1505, -0.1010],\n",
      "        [-0.1241, -0.2229, -0.1976, -0.1806,  0.1240,  0.0133, -0.0823, -0.1374,\n",
      "          0.2032,  0.2055,  0.1205,  0.0619, -0.0392, -0.1201,  0.0848, -0.2110,\n",
      "          0.0333, -0.0750,  0.1307, -0.0257],\n",
      "        [-0.0744, -0.0447, -0.0505,  0.2112,  0.0785, -0.1513,  0.1876, -0.0008,\n",
      "         -0.0807, -0.1389, -0.1045, -0.1381, -0.0085,  0.2135,  0.0692, -0.0883,\n",
      "          0.0469, -0.1073, -0.1599, -0.1919],\n",
      "        [-0.1878, -0.1746,  0.0368, -0.0378, -0.0694,  0.1468,  0.2173,  0.1244,\n",
      "         -0.2168, -0.0411, -0.0171, -0.0086, -0.0917, -0.2117, -0.0143, -0.0323,\n",
      "          0.0965,  0.0661, -0.1577,  0.0118],\n",
      "        [ 0.1152, -0.2084,  0.0731,  0.2062,  0.0392, -0.0573, -0.1242, -0.2153,\n",
      "          0.1713, -0.1410, -0.0552, -0.1049,  0.1926, -0.0911,  0.0916, -0.0162,\n",
      "         -0.0071, -0.0255, -0.1068, -0.0207],\n",
      "        [-0.0132,  0.1453,  0.0306, -0.1115,  0.1658, -0.0559, -0.2129, -0.1545,\n",
      "         -0.0980, -0.1533,  0.1919,  0.0214, -0.0815, -0.1014,  0.1169, -0.0104,\n",
      "         -0.0941,  0.1014,  0.0816,  0.0908],\n",
      "        [-0.2093, -0.0169,  0.1355,  0.1063, -0.1278,  0.1705,  0.1087,  0.0903,\n",
      "         -0.0535,  0.1423, -0.2016, -0.1010, -0.1384, -0.0760, -0.0029, -0.2126,\n",
      "         -0.1072,  0.2086, -0.1391, -0.1202],\n",
      "        [-0.0067,  0.0325,  0.1811, -0.1045,  0.1568,  0.2206,  0.1062,  0.1417,\n",
      "         -0.0491, -0.2164,  0.1633, -0.2171,  0.2063,  0.0880, -0.1197, -0.1582,\n",
      "          0.1085,  0.0855,  0.1088, -0.1204],\n",
      "        [-0.1375, -0.1331,  0.1531,  0.1384, -0.0573, -0.1826, -0.0398, -0.1736,\n",
      "         -0.0592, -0.0670, -0.1619,  0.0360,  0.1272, -0.0197, -0.1090, -0.1949,\n",
      "          0.0027,  0.0349, -0.0941,  0.0109],\n",
      "        [-0.0349,  0.0704,  0.1476, -0.0277, -0.1289,  0.1840,  0.0139, -0.0883,\n",
      "         -0.0957, -0.2043,  0.1368,  0.1105, -0.0800, -0.1775, -0.1593, -0.1922,\n",
      "         -0.0731,  0.1427,  0.1203, -0.0215],\n",
      "        [ 0.1706,  0.0070,  0.0351,  0.1524,  0.1612, -0.0899,  0.0054, -0.1029,\n",
      "         -0.0914, -0.0605, -0.1885, -0.0586, -0.0966,  0.0463, -0.1713,  0.1853,\n",
      "         -0.1345,  0.0301, -0.1049, -0.0139],\n",
      "        [ 0.0558,  0.0248, -0.0347,  0.0614, -0.0550,  0.1752, -0.1314,  0.0523,\n",
      "         -0.1238,  0.2054, -0.1067,  0.0749,  0.1136,  0.0341, -0.1394,  0.1020,\n",
      "          0.0703,  0.0150,  0.1693, -0.1767],\n",
      "        [-0.1463,  0.0453,  0.1940,  0.0706,  0.1129,  0.1108, -0.1679,  0.1074,\n",
      "         -0.1683, -0.0573, -0.1448,  0.0071, -0.0583,  0.0283, -0.1984, -0.1746,\n",
      "          0.1155, -0.0424,  0.1336,  0.0796],\n",
      "        [-0.2063, -0.0689,  0.2070,  0.1210, -0.2133, -0.0008, -0.1654, -0.1774,\n",
      "         -0.1815,  0.0994, -0.2004, -0.0540, -0.2004,  0.1523, -0.1753, -0.1421,\n",
      "          0.0638, -0.2038,  0.2001,  0.1667],\n",
      "        [ 0.0535,  0.0827, -0.0592,  0.0211, -0.0462,  0.2047,  0.1174,  0.0603,\n",
      "         -0.1188, -0.1163,  0.0553,  0.0171, -0.2120, -0.1455, -0.0799,  0.0018,\n",
      "          0.0788,  0.0203,  0.1298,  0.0956],\n",
      "        [ 0.1383,  0.1502,  0.1081, -0.1810, -0.0365,  0.1094,  0.1579,  0.1510,\n",
      "          0.2038,  0.0616, -0.0367, -0.1077,  0.2071,  0.1188,  0.1939,  0.1538,\n",
      "          0.0845,  0.0034,  0.1116,  0.0561],\n",
      "        [-0.1718,  0.2178, -0.0818, -0.2030,  0.2210, -0.1578,  0.1168, -0.1335,\n",
      "          0.1962, -0.1512,  0.0232, -0.1969,  0.0318, -0.1353, -0.2149,  0.0479,\n",
      "         -0.0198,  0.1054,  0.2208,  0.0601],\n",
      "        [-0.0879,  0.2190,  0.1226,  0.1582, -0.1003, -0.1361, -0.2185, -0.1464,\n",
      "         -0.0156,  0.0436, -0.0537, -0.0886,  0.1529,  0.1187, -0.1374, -0.1256,\n",
      "         -0.0970, -0.0848, -0.1587, -0.1284]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0523, -0.1132,  0.1999, -0.0424, -0.1403, -0.0503, -0.0662,  0.1012,\n",
      "         0.0813, -0.0883,  0.1987,  0.1707, -0.1659,  0.0207,  0.1241,  0.0694,\n",
      "        -0.1803,  0.0778, -0.0613,  0.1159], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1500,  0.2196, -0.0381, -0.0389, -0.0664,  0.1696,  0.0929,  0.1947,\n",
      "         -0.0688,  0.0671,  0.1800,  0.1399,  0.1491, -0.1395, -0.0423, -0.0797,\n",
      "          0.0968,  0.0450, -0.1530,  0.1044],\n",
      "        [ 0.1201,  0.1092, -0.1324, -0.0455,  0.0348,  0.1682, -0.1942, -0.0563,\n",
      "          0.0515, -0.1151, -0.0251, -0.0031, -0.1352,  0.1531,  0.1605,  0.1534,\n",
      "          0.1685,  0.1111, -0.0194,  0.2216],\n",
      "        [ 0.1882,  0.1445, -0.1510, -0.1214,  0.1951, -0.1057,  0.0354, -0.1859,\n",
      "         -0.0073, -0.1415, -0.1154, -0.0200,  0.0559, -0.0914, -0.0969,  0.0329,\n",
      "          0.0833, -0.2201,  0.1164,  0.0045],\n",
      "        [-0.1135,  0.1667,  0.0363, -0.0886, -0.1675, -0.2052, -0.0562, -0.2122,\n",
      "          0.0610, -0.0051, -0.0159, -0.1405, -0.1683, -0.0124,  0.0598, -0.1422,\n",
      "         -0.1133, -0.0297,  0.2090,  0.0086],\n",
      "        [-0.0433, -0.1917,  0.0229, -0.0117, -0.0479,  0.0662, -0.0162,  0.1081,\n",
      "          0.1865, -0.0477,  0.0670, -0.1379, -0.0245,  0.0489,  0.1017,  0.1350,\n",
      "         -0.1126,  0.1690, -0.0162,  0.1053],\n",
      "        [-0.1300, -0.0926, -0.0093, -0.1916, -0.1889, -0.1106,  0.2160, -0.0569,\n",
      "         -0.0157, -0.1717,  0.1003,  0.2064, -0.1842,  0.1914, -0.0874,  0.0603,\n",
      "         -0.0285, -0.1129,  0.0168,  0.0612],\n",
      "        [-0.2136,  0.1864, -0.1281,  0.0155, -0.0965,  0.1209,  0.2104,  0.1595,\n",
      "         -0.2002,  0.0452, -0.0619, -0.0666,  0.0225, -0.0401,  0.1146,  0.1455,\n",
      "          0.2012, -0.0704, -0.0052,  0.0367],\n",
      "        [ 0.1927, -0.1692,  0.2024,  0.2059, -0.1944, -0.1148,  0.2193, -0.0527,\n",
      "         -0.0159,  0.0986,  0.1993,  0.1610,  0.1928,  0.1078, -0.2091,  0.2057,\n",
      "          0.2102,  0.0494, -0.0581,  0.0020],\n",
      "        [-0.1958, -0.1425, -0.1002, -0.0830,  0.0916,  0.0829, -0.1185,  0.2079,\n",
      "          0.0494, -0.1285, -0.0274, -0.2058,  0.1741, -0.0853, -0.0396, -0.2209,\n",
      "         -0.0602, -0.2221,  0.0395, -0.1767],\n",
      "        [-0.0167, -0.0590, -0.0598,  0.1726, -0.2055,  0.0088,  0.2028, -0.0064,\n",
      "          0.1348, -0.0642,  0.0570, -0.0964,  0.2196, -0.1600,  0.0297, -0.0172,\n",
      "          0.1655, -0.0807,  0.2057,  0.0563],\n",
      "        [-0.0264, -0.0131, -0.1008, -0.2014, -0.2229,  0.1495, -0.2052,  0.1784,\n",
      "         -0.0426, -0.1916,  0.0587,  0.0710, -0.0149, -0.1597,  0.1145, -0.0402,\n",
      "          0.1014,  0.0603, -0.1155,  0.0359],\n",
      "        [-0.0019, -0.2063, -0.2064, -0.1429,  0.0954, -0.1433, -0.1686,  0.0797,\n",
      "         -0.1917,  0.0400, -0.0789, -0.1858, -0.2099, -0.0250,  0.0366,  0.2176,\n",
      "         -0.1524, -0.0759, -0.1267, -0.0987],\n",
      "        [ 0.1325, -0.0987, -0.1629,  0.0220, -0.0747,  0.0668,  0.1154, -0.1641,\n",
      "         -0.2058, -0.0955,  0.0763,  0.0329,  0.1404, -0.1981,  0.2180, -0.2134,\n",
      "         -0.1945,  0.0135,  0.0566,  0.1322],\n",
      "        [ 0.1588,  0.1920, -0.1741,  0.1642, -0.1875, -0.0200,  0.2005, -0.0950,\n",
      "         -0.1551,  0.1539, -0.1528,  0.1792, -0.0678, -0.0085,  0.1370,  0.2124,\n",
      "         -0.0075, -0.1389, -0.2101, -0.2077],\n",
      "        [-0.1985,  0.1365, -0.1575,  0.1526,  0.1858,  0.0737, -0.0216, -0.0121,\n",
      "         -0.0584, -0.2161, -0.1785,  0.0491, -0.0287, -0.2128,  0.0235,  0.1361,\n",
      "         -0.1866, -0.0715,  0.0870,  0.0699],\n",
      "        [ 0.1659,  0.0567,  0.1006, -0.0595,  0.0749,  0.2234,  0.1894, -0.1377,\n",
      "          0.0229,  0.0190, -0.1805,  0.0195, -0.1625, -0.1365,  0.0599,  0.1745,\n",
      "         -0.1651, -0.1055, -0.2078,  0.2199],\n",
      "        [ 0.0394,  0.1221, -0.1754,  0.0066,  0.0152,  0.1407, -0.1982, -0.0484,\n",
      "          0.1357,  0.0716,  0.0566, -0.2085, -0.1214,  0.1510,  0.0397,  0.1387,\n",
      "         -0.1881, -0.1287,  0.1311,  0.2119],\n",
      "        [-0.0696, -0.1571, -0.1550, -0.0230, -0.0156,  0.1182,  0.2133,  0.0319,\n",
      "          0.2063,  0.2183, -0.0131,  0.1542, -0.1496,  0.2012,  0.0370,  0.0099,\n",
      "         -0.1675, -0.1588,  0.1755, -0.1246],\n",
      "        [-0.1393,  0.1856, -0.0498, -0.0112,  0.1653,  0.0160,  0.2047, -0.1028,\n",
      "         -0.0520, -0.0794,  0.2137, -0.0541, -0.1809, -0.1984,  0.1877,  0.0597,\n",
      "          0.1998,  0.1869,  0.0244,  0.0258],\n",
      "        [ 0.2029,  0.0522, -0.1826, -0.2002,  0.0714,  0.1903, -0.0152, -0.1252,\n",
      "          0.0982,  0.2160,  0.0794,  0.0972,  0.0161, -0.1504,  0.2072, -0.0512,\n",
      "          0.1995,  0.1570, -0.0625,  0.1058]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1289,  0.2031, -0.2106,  0.1048, -0.1832,  0.2107,  0.0010, -0.0388,\n",
      "        -0.1743, -0.0075,  0.1780, -0.1194,  0.0227, -0.0488,  0.1018,  0.0707,\n",
      "        -0.1954, -0.1409,  0.1576, -0.0631], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0453, -0.0369, -0.2108,  0.0578, -0.1156,  0.1609, -0.0909,  0.1260,\n",
      "          0.1571, -0.1147, -0.0383, -0.0226, -0.1596, -0.0075, -0.1661,  0.1790,\n",
      "          0.1444,  0.1683, -0.1572, -0.0100],\n",
      "        [-0.1335, -0.1533,  0.0566,  0.0287,  0.0755, -0.1565, -0.0296, -0.2191,\n",
      "         -0.2155, -0.1154, -0.1831,  0.1072, -0.1191, -0.1443,  0.1768,  0.1248,\n",
      "          0.1532,  0.2151, -0.1845,  0.1507],\n",
      "        [-0.0796, -0.0745, -0.1922, -0.0681,  0.2095, -0.0834, -0.0287,  0.1837,\n",
      "         -0.1694, -0.2106,  0.1084, -0.0948, -0.2154,  0.0934,  0.1960,  0.1896,\n",
      "          0.1517,  0.1024,  0.2229,  0.0518],\n",
      "        [-0.1400,  0.1674, -0.0571,  0.1529, -0.1200,  0.0163, -0.1299,  0.0071,\n",
      "         -0.0254,  0.1009,  0.0096, -0.0829, -0.0462,  0.1013,  0.0941,  0.0541,\n",
      "         -0.1937,  0.0981,  0.0014, -0.0555],\n",
      "        [-0.1094, -0.1048,  0.2147, -0.1955,  0.1808, -0.1796, -0.0726,  0.1863,\n",
      "         -0.0610, -0.1312,  0.1978,  0.1449,  0.0463, -0.0085,  0.1373, -0.1133,\n",
      "          0.0264,  0.0811, -0.1841, -0.1711],\n",
      "        [-0.2153,  0.0061, -0.1378,  0.1087, -0.0618,  0.0320, -0.0474,  0.0482,\n",
      "         -0.0742,  0.1993, -0.1432,  0.1533, -0.1415, -0.0305, -0.0622,  0.0576,\n",
      "         -0.1362,  0.0301,  0.1059, -0.1881],\n",
      "        [ 0.1714,  0.1175,  0.1305, -0.1150,  0.0871, -0.2088, -0.0634,  0.0191,\n",
      "         -0.0677,  0.0468, -0.1701,  0.1235, -0.1608,  0.1694,  0.0814,  0.1182,\n",
      "         -0.0559, -0.0123,  0.0764,  0.2120],\n",
      "        [ 0.1440,  0.1943,  0.1768,  0.1287, -0.0217,  0.0733,  0.0198,  0.0132,\n",
      "         -0.0663,  0.2002,  0.1489,  0.0662, -0.0316, -0.1934,  0.0931, -0.1603,\n",
      "          0.2074, -0.0330, -0.0529, -0.0012],\n",
      "        [-0.1661,  0.0172, -0.1140, -0.2200,  0.1715,  0.0611, -0.0435,  0.0243,\n",
      "         -0.1530,  0.2201, -0.0828,  0.1284,  0.0756, -0.1321, -0.0843, -0.0263,\n",
      "         -0.2019, -0.0088, -0.0393, -0.1646],\n",
      "        [-0.0897, -0.1486, -0.1731,  0.0536, -0.2107,  0.0506,  0.1566,  0.0527,\n",
      "         -0.1321, -0.1620,  0.0755,  0.1762,  0.0890, -0.0498, -0.2049,  0.0108,\n",
      "          0.0079,  0.1172,  0.0120,  0.2029],\n",
      "        [-0.1191, -0.2161, -0.1762,  0.2058, -0.0496,  0.0640, -0.0768,  0.1088,\n",
      "          0.1495,  0.2204, -0.1529, -0.0831,  0.0822, -0.1478, -0.0688,  0.1535,\n",
      "         -0.2048,  0.1395, -0.1812, -0.0739],\n",
      "        [ 0.2056, -0.1221,  0.1400,  0.2157,  0.0298,  0.1595, -0.2102, -0.0041,\n",
      "          0.1800, -0.0885,  0.1284,  0.0669, -0.0871,  0.0423, -0.1457, -0.0695,\n",
      "         -0.1349, -0.0177, -0.0727,  0.0138],\n",
      "        [-0.0343,  0.1926,  0.2120, -0.1456,  0.1226,  0.0265, -0.1944, -0.1192,\n",
      "         -0.1797,  0.1083,  0.0978,  0.2073, -0.1311,  0.0841, -0.1983,  0.2010,\n",
      "         -0.1132, -0.0654,  0.0192, -0.1147],\n",
      "        [ 0.0365,  0.0266, -0.0209,  0.0989,  0.1262,  0.0261, -0.1290, -0.1879,\n",
      "          0.1002,  0.0005, -0.0945,  0.1942,  0.2028, -0.1003, -0.0204, -0.0414,\n",
      "          0.0789, -0.2132, -0.0560,  0.0474],\n",
      "        [ 0.0596,  0.1197,  0.0659,  0.1364,  0.0610, -0.0647,  0.0390,  0.1532,\n",
      "         -0.0137, -0.1680, -0.1193,  0.0690,  0.2214, -0.0358, -0.1976, -0.1886,\n",
      "         -0.1777,  0.1520,  0.0389, -0.1442],\n",
      "        [-0.0252, -0.1507,  0.0632, -0.1923, -0.1754,  0.1359,  0.1246,  0.1721,\n",
      "         -0.1525,  0.1164,  0.1583, -0.0411,  0.1960, -0.1989, -0.2222, -0.0281,\n",
      "          0.1214,  0.0959, -0.0641, -0.0577],\n",
      "        [-0.1071,  0.0511, -0.0725,  0.1982, -0.1250, -0.0765,  0.0681,  0.0763,\n",
      "         -0.0533, -0.1290, -0.0321,  0.1070,  0.1698, -0.0144,  0.0378,  0.0764,\n",
      "          0.0971, -0.2066,  0.0607, -0.1709],\n",
      "        [-0.1570, -0.1189, -0.0092, -0.1982,  0.2233, -0.2030,  0.0160,  0.1365,\n",
      "          0.0872,  0.1702,  0.0880, -0.0492,  0.0050,  0.2073,  0.1436, -0.1793,\n",
      "          0.1638,  0.1235,  0.0168, -0.0022],\n",
      "        [-0.0163, -0.0459,  0.1762,  0.0638, -0.0849,  0.2124,  0.0456, -0.1749,\n",
      "          0.0802, -0.0981,  0.1546,  0.1516,  0.2227,  0.0819, -0.2213,  0.0572,\n",
      "          0.0050,  0.0568, -0.1179,  0.1211],\n",
      "        [ 0.0229,  0.0454,  0.1732,  0.1800,  0.1141,  0.2218, -0.1061,  0.1353,\n",
      "          0.0264,  0.1351, -0.1162,  0.0292, -0.1190,  0.0233, -0.2182, -0.0984,\n",
      "          0.0590, -0.0023,  0.2108, -0.0486]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1796, -0.0998,  0.1489, -0.1392,  0.1134,  0.0881,  0.1772,  0.1427,\n",
      "         0.0998,  0.1495,  0.1508, -0.0761, -0.1502, -0.1464, -0.0699, -0.1487,\n",
      "        -0.1879,  0.1613, -0.0438,  0.1881], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0453, -0.1286,  0.1550,  0.2219, -0.0359,  0.1569, -0.1502,  0.1358,\n",
      "          0.1591,  0.1843,  0.0058,  0.0819,  0.0985, -0.0921,  0.2055, -0.0160,\n",
      "          0.2053, -0.1763, -0.0758, -0.2169],\n",
      "        [ 0.2140, -0.2077,  0.1282,  0.0883, -0.0837,  0.1026,  0.1831,  0.0475,\n",
      "         -0.0780, -0.2120, -0.0634, -0.0009,  0.0713, -0.1716,  0.1783,  0.0495,\n",
      "         -0.0256, -0.1150,  0.0628,  0.1284],\n",
      "        [ 0.0301, -0.0294,  0.0552, -0.1791, -0.1791, -0.0489, -0.1338, -0.0075,\n",
      "          0.1679,  0.1440, -0.0501, -0.2189,  0.1437, -0.0063,  0.1594, -0.0456,\n",
      "          0.0172,  0.2123,  0.0116, -0.0701],\n",
      "        [-0.0215,  0.2121,  0.0799, -0.0284, -0.0988,  0.1598, -0.1612,  0.0906,\n",
      "         -0.1829,  0.1326,  0.1290,  0.1678,  0.0279,  0.0519, -0.0803,  0.0367,\n",
      "          0.0917,  0.0229,  0.0036, -0.1565],\n",
      "        [-0.0235,  0.1837, -0.2107, -0.1207,  0.0015, -0.1703, -0.0269,  0.1588,\n",
      "         -0.1961,  0.1853, -0.0941, -0.1148, -0.0224, -0.1602,  0.1909, -0.1089,\n",
      "          0.1984, -0.0138,  0.1920, -0.1279],\n",
      "        [-0.0625,  0.1898,  0.0625,  0.0823,  0.1705,  0.0911,  0.2224, -0.1943,\n",
      "         -0.1784,  0.1292,  0.1333,  0.0567, -0.1065, -0.2113,  0.1576, -0.0371,\n",
      "         -0.0635, -0.0285, -0.0342,  0.0710],\n",
      "        [ 0.1819, -0.1008,  0.0926,  0.0247,  0.0908, -0.1631,  0.1866, -0.0574,\n",
      "         -0.1346,  0.1699, -0.1697,  0.2062,  0.0958, -0.0259, -0.1965,  0.0716,\n",
      "          0.0801, -0.1661, -0.1756,  0.0906],\n",
      "        [ 0.0577, -0.1720, -0.0385,  0.2235,  0.1252, -0.1369,  0.0952,  0.1094,\n",
      "          0.1932, -0.2180,  0.1807,  0.0738,  0.2144,  0.0801,  0.0144,  0.0027,\n",
      "         -0.2146,  0.1998, -0.1393,  0.0113],\n",
      "        [-0.0555, -0.0161,  0.1961, -0.1999, -0.1089,  0.1765,  0.0813, -0.0202,\n",
      "          0.1563,  0.0700,  0.0016, -0.0469,  0.0954,  0.1378,  0.2132, -0.0909,\n",
      "         -0.1935, -0.0859, -0.1439, -0.1420],\n",
      "        [-0.0212,  0.2202,  0.2217, -0.0876,  0.0900,  0.2186, -0.0797, -0.1422,\n",
      "         -0.1831,  0.1367, -0.0947,  0.1658,  0.0868,  0.0390, -0.0795, -0.0776,\n",
      "          0.1601, -0.0358,  0.0049, -0.0050],\n",
      "        [-0.1041,  0.0837, -0.1637, -0.1932,  0.1701, -0.0761, -0.1923, -0.2142,\n",
      "         -0.0033,  0.0090,  0.1865,  0.0517, -0.1084,  0.0230, -0.0694, -0.1865,\n",
      "         -0.1564,  0.0579,  0.0640,  0.1699],\n",
      "        [ 0.1675, -0.0995, -0.1354, -0.1842, -0.0223,  0.1678,  0.2163,  0.1374,\n",
      "         -0.1351,  0.1259, -0.1909,  0.0002, -0.1927, -0.0896, -0.2130, -0.0449,\n",
      "          0.1680, -0.0071,  0.2151,  0.0139],\n",
      "        [-0.0536, -0.0536, -0.0533, -0.2134,  0.1825,  0.2157, -0.1446, -0.0471,\n",
      "          0.0616,  0.1324, -0.0840, -0.1438, -0.1443,  0.1856,  0.1838, -0.0817,\n",
      "         -0.0757, -0.0214, -0.0827, -0.1469],\n",
      "        [-0.0872, -0.0523,  0.2189, -0.1371, -0.2102, -0.2187, -0.0640, -0.0845,\n",
      "         -0.0472,  0.1083, -0.0782,  0.2002,  0.0055, -0.1411, -0.0226, -0.1826,\n",
      "         -0.0942, -0.0230,  0.1186,  0.0643],\n",
      "        [-0.0754,  0.0370,  0.0785,  0.0095,  0.1912,  0.1870,  0.1915, -0.0261,\n",
      "          0.0530,  0.1639,  0.1253,  0.1480, -0.1123,  0.2061, -0.1101,  0.1640,\n",
      "          0.1608,  0.0056,  0.0825, -0.1228],\n",
      "        [ 0.1927, -0.0380,  0.1060,  0.1839,  0.2002,  0.1400, -0.1281,  0.1369,\n",
      "         -0.1307,  0.1718,  0.0144, -0.1629,  0.1288, -0.0176,  0.1240, -0.2224,\n",
      "         -0.1658,  0.1372,  0.1893,  0.1318],\n",
      "        [-0.2061, -0.2094,  0.1732, -0.0613,  0.0506,  0.0461,  0.2186, -0.0398,\n",
      "          0.0185,  0.2131, -0.1848,  0.0410, -0.0666,  0.2122, -0.0011,  0.0010,\n",
      "         -0.1599, -0.0177,  0.1374, -0.0630],\n",
      "        [-0.0884,  0.1436,  0.1696, -0.1212,  0.1955, -0.0149, -0.0835, -0.1551,\n",
      "          0.0166,  0.1937,  0.1459,  0.1968, -0.0240, -0.1437, -0.0589,  0.0448,\n",
      "         -0.1468, -0.1525, -0.1834,  0.0165],\n",
      "        [ 0.0396,  0.2085, -0.1763, -0.0291, -0.0063,  0.1949, -0.1547,  0.0945,\n",
      "         -0.1462,  0.1778, -0.1693,  0.0202, -0.1060, -0.0639, -0.0967,  0.0415,\n",
      "          0.0625, -0.1880,  0.0744, -0.0653],\n",
      "        [-0.0468,  0.0056,  0.0990,  0.1062,  0.0234,  0.0246, -0.2051, -0.2036,\n",
      "          0.1925,  0.2045, -0.0613,  0.0895,  0.1312, -0.0180, -0.1664,  0.0303,\n",
      "          0.0026, -0.1376, -0.1644, -0.1848]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1368, -0.0645, -0.0987, -0.0687, -0.0845, -0.1467,  0.0593, -0.1340,\n",
      "        -0.1072,  0.1980,  0.1169,  0.1237,  0.1501,  0.0921,  0.1431,  0.1688,\n",
      "        -0.1859,  0.0578, -0.0544, -0.2129], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0672,  0.0953, -0.2073,  0.1876,  0.1756,  0.0464,  0.0302, -0.1348,\n",
      "          0.1830, -0.1214,  0.2190, -0.1131, -0.1017, -0.0790, -0.0312,  0.1393,\n",
      "          0.2202,  0.2124, -0.0748,  0.1658],\n",
      "        [-0.0524, -0.2100, -0.1216,  0.1385, -0.0412,  0.1218,  0.1189,  0.1531,\n",
      "         -0.1745, -0.2036, -0.2233, -0.1580,  0.1258,  0.0952, -0.1842, -0.0646,\n",
      "         -0.1820,  0.0731,  0.0302, -0.1456],\n",
      "        [ 0.0232,  0.0093, -0.0102,  0.2207, -0.1060,  0.1389,  0.1132,  0.0341,\n",
      "         -0.0084, -0.0281,  0.1710,  0.0986,  0.0057, -0.1591,  0.1507, -0.1415,\n",
      "         -0.0081, -0.1788,  0.1079,  0.1875],\n",
      "        [-0.0172, -0.0597,  0.0721, -0.0565,  0.1047,  0.0811,  0.2082, -0.0294,\n",
      "         -0.1838, -0.0923, -0.1470,  0.0867, -0.1968, -0.0101, -0.2210, -0.0831,\n",
      "          0.0431, -0.0913,  0.0632, -0.1962],\n",
      "        [-0.0598, -0.2158, -0.2029, -0.1446,  0.2102, -0.0908, -0.0228, -0.0920,\n",
      "         -0.0118,  0.0079,  0.1821, -0.1682, -0.2188,  0.1834, -0.1402,  0.0701,\n",
      "         -0.0892,  0.2148, -0.1012,  0.2211],\n",
      "        [-0.0256,  0.0773,  0.1785, -0.0370, -0.1253, -0.1735,  0.2087, -0.1726,\n",
      "          0.2161, -0.1702, -0.2147,  0.0206, -0.0187,  0.1975, -0.0738,  0.2135,\n",
      "          0.1654,  0.1933, -0.0656, -0.0168],\n",
      "        [-0.2155,  0.0178,  0.1648,  0.0898, -0.1283, -0.0906, -0.0884, -0.0009,\n",
      "          0.2080, -0.2100, -0.0598,  0.2078, -0.1175,  0.0437, -0.0125, -0.0744,\n",
      "          0.1216, -0.0071, -0.2174, -0.1982],\n",
      "        [-0.1579,  0.1026,  0.1378, -0.1739, -0.1227,  0.0076,  0.1344,  0.1639,\n",
      "         -0.1347, -0.1466, -0.1304, -0.1691,  0.1251,  0.0540,  0.2127, -0.0942,\n",
      "          0.0689,  0.1802,  0.0854, -0.1328],\n",
      "        [-0.2028, -0.1152,  0.2197, -0.1538,  0.0535, -0.1728, -0.0298,  0.1347,\n",
      "          0.1841, -0.0389,  0.0999,  0.0238, -0.0489, -0.1009,  0.0609, -0.1451,\n",
      "         -0.1782,  0.1850,  0.0329,  0.1080],\n",
      "        [ 0.2049, -0.1498,  0.0209, -0.0729,  0.1618, -0.0602, -0.0855,  0.1507,\n",
      "          0.0715,  0.1025,  0.0550,  0.2014, -0.0424, -0.2057, -0.1587,  0.0526,\n",
      "          0.1536,  0.2064,  0.0967, -0.2086],\n",
      "        [-0.1495,  0.0418,  0.1136, -0.0183,  0.0697,  0.0894, -0.0465,  0.2150,\n",
      "         -0.0203,  0.1392,  0.1052,  0.0674,  0.1527,  0.0149, -0.1865, -0.0885,\n",
      "         -0.0251, -0.1421, -0.2171, -0.0677],\n",
      "        [ 0.0822, -0.1111, -0.1861, -0.1922,  0.1924,  0.0459,  0.1750, -0.0403,\n",
      "          0.0548, -0.0046,  0.2134, -0.0494,  0.2083, -0.2043,  0.0422,  0.1423,\n",
      "          0.1107,  0.0477,  0.1057, -0.1377],\n",
      "        [-0.1181, -0.0805, -0.2030, -0.0360, -0.0689,  0.2091, -0.2200,  0.0826,\n",
      "          0.1856,  0.2024, -0.1246, -0.1357, -0.1144,  0.0074,  0.1789, -0.1629,\n",
      "          0.2161,  0.1701,  0.1890,  0.1595],\n",
      "        [ 0.0943, -0.1615,  0.1723,  0.0455,  0.0426, -0.1156, -0.1381,  0.0075,\n",
      "          0.1177,  0.1451, -0.0637,  0.1768, -0.1925,  0.0454,  0.0465, -0.1578,\n",
      "          0.1740, -0.2167,  0.1228,  0.0632],\n",
      "        [ 0.1588,  0.1170, -0.1478, -0.0469,  0.0444,  0.1361,  0.0535,  0.1361,\n",
      "          0.2002,  0.1282,  0.1857,  0.1990, -0.0479,  0.0239, -0.1854,  0.0050,\n",
      "         -0.1898,  0.1534, -0.0325, -0.1611],\n",
      "        [-0.1245,  0.0734,  0.1837, -0.1002, -0.0579, -0.0532, -0.0186,  0.1002,\n",
      "          0.1650,  0.1942,  0.0861, -0.0773,  0.0398,  0.1970, -0.0880,  0.0812,\n",
      "         -0.0317,  0.2074, -0.1200,  0.0256],\n",
      "        [-0.1449, -0.0497, -0.0816, -0.0677,  0.0063,  0.0547,  0.1253,  0.2138,\n",
      "          0.1614, -0.0466,  0.2079,  0.1422, -0.0823,  0.0146,  0.2181, -0.1041,\n",
      "          0.1380,  0.2104,  0.0132,  0.0809],\n",
      "        [ 0.0372, -0.1283, -0.0681,  0.0126, -0.0464,  0.2128, -0.0189,  0.1642,\n",
      "          0.1939,  0.1170,  0.1811,  0.0923,  0.1093,  0.1601,  0.1961, -0.1054,\n",
      "          0.2152,  0.1005, -0.1608,  0.1006],\n",
      "        [ 0.1727, -0.1707,  0.0732,  0.0998,  0.1340,  0.1461,  0.0093,  0.0438,\n",
      "         -0.2199,  0.0676, -0.1037,  0.1227, -0.1742, -0.0240,  0.1527, -0.1509,\n",
      "         -0.0721, -0.0204,  0.2159, -0.0994],\n",
      "        [ 0.0215, -0.0549,  0.0288, -0.2205, -0.0327,  0.2209, -0.2134, -0.0973,\n",
      "          0.1064, -0.1339,  0.1580, -0.1908, -0.0106,  0.2008,  0.0299, -0.1901,\n",
      "         -0.0409,  0.1859, -0.1665, -0.1902]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1583, -0.1323,  0.1312, -0.0792,  0.1465, -0.1616, -0.1676,  0.1417,\n",
      "         0.1206,  0.2131,  0.1110,  0.1249,  0.0376, -0.0179,  0.2126,  0.0379,\n",
      "         0.1360,  0.1810, -0.0471,  0.1959], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1244, -0.1054, -0.2142,  0.0728,  0.2143, -0.0760,  0.0604,  0.1346,\n",
      "          0.0716,  0.0460, -0.0287,  0.0103, -0.0818,  0.0045,  0.0621,  0.1984,\n",
      "         -0.1419, -0.0728,  0.1695,  0.1101],\n",
      "        [ 0.2024, -0.2163, -0.0228, -0.0040,  0.1036, -0.1512, -0.2027,  0.1825,\n",
      "         -0.0011, -0.1275, -0.0177,  0.1528,  0.0738, -0.0054,  0.1984,  0.1739,\n",
      "         -0.1910,  0.0615, -0.0323, -0.0734],\n",
      "        [ 0.0108, -0.0305,  0.0743, -0.0757, -0.1735, -0.1818,  0.1426, -0.0132,\n",
      "          0.1366,  0.1177, -0.0731,  0.0479, -0.1254, -0.1064,  0.1929, -0.0126,\n",
      "          0.1511, -0.1578, -0.2080, -0.0823],\n",
      "        [ 0.0615,  0.0870, -0.2053,  0.1722, -0.1694, -0.1431, -0.0359,  0.1248,\n",
      "          0.1471, -0.1313,  0.1898,  0.0425, -0.0324, -0.1864,  0.0526,  0.0504,\n",
      "          0.1270, -0.1550,  0.0390,  0.1377],\n",
      "        [-0.0446,  0.1799,  0.0109, -0.1417, -0.1440,  0.2116,  0.2199,  0.0038,\n",
      "         -0.1182,  0.0711, -0.0184,  0.0922, -0.2186,  0.1877,  0.1458,  0.0708,\n",
      "         -0.2202,  0.1862, -0.0369, -0.0925],\n",
      "        [ 0.1688, -0.1492,  0.1056,  0.1103, -0.2055,  0.0928, -0.0852,  0.1547,\n",
      "         -0.0986,  0.2058,  0.1068,  0.1447,  0.0845, -0.1041,  0.1308, -0.1576,\n",
      "         -0.0500,  0.2137,  0.1539,  0.0186],\n",
      "        [ 0.1975,  0.0411,  0.0044, -0.1820, -0.0714,  0.1066,  0.0911,  0.0113,\n",
      "         -0.1140,  0.1139,  0.1237,  0.0144,  0.1637,  0.1500,  0.1414, -0.1903,\n",
      "          0.0238, -0.0103,  0.1260,  0.2075],\n",
      "        [ 0.0557, -0.1959,  0.0679, -0.1140,  0.0235, -0.0610, -0.0613,  0.1369,\n",
      "          0.1858, -0.1510, -0.0052, -0.2193, -0.1180, -0.1507, -0.1746, -0.1923,\n",
      "         -0.1986, -0.1815, -0.1083, -0.0035],\n",
      "        [-0.1092, -0.2024,  0.1028, -0.0669, -0.0584, -0.0525, -0.0190, -0.1212,\n",
      "         -0.1122,  0.0901, -0.1814, -0.0785,  0.0422,  0.0414,  0.0759, -0.2015,\n",
      "         -0.0682, -0.0110,  0.1915, -0.0580],\n",
      "        [-0.0741, -0.0775, -0.1247, -0.2177, -0.0008,  0.2033, -0.0716, -0.0316,\n",
      "          0.1270,  0.0093, -0.0240,  0.2142, -0.1405, -0.1019,  0.0195,  0.1082,\n",
      "         -0.0588,  0.0494, -0.1062,  0.1142],\n",
      "        [-0.0321,  0.0941, -0.0820,  0.0933,  0.0245, -0.1566, -0.0399, -0.1815,\n",
      "          0.0643,  0.1175,  0.0126,  0.0790, -0.0103,  0.0667, -0.2187, -0.1004,\n",
      "         -0.1713,  0.1522,  0.1642,  0.0883],\n",
      "        [ 0.0123,  0.1468,  0.1273,  0.1781, -0.1008,  0.0788,  0.1690, -0.0985,\n",
      "          0.1096, -0.0664, -0.2229,  0.0649,  0.0734,  0.1266,  0.0252, -0.0412,\n",
      "         -0.2165, -0.0188,  0.0268,  0.2057],\n",
      "        [-0.2037, -0.1924, -0.2117, -0.0835,  0.1149, -0.0643, -0.0178, -0.0888,\n",
      "         -0.1513,  0.0327,  0.1774,  0.1462,  0.0173,  0.0071,  0.1987,  0.1608,\n",
      "         -0.0256,  0.1117, -0.2067, -0.0528],\n",
      "        [ 0.1242, -0.1915,  0.0385, -0.0484, -0.0746, -0.2106, -0.1062, -0.0642,\n",
      "          0.1203,  0.0119, -0.1155, -0.1980,  0.0573,  0.1463,  0.2023,  0.1125,\n",
      "         -0.0232,  0.0500,  0.1213,  0.0901],\n",
      "        [ 0.0545,  0.1806, -0.1670,  0.1569,  0.0971,  0.0166, -0.1577, -0.0986,\n",
      "         -0.1216,  0.1060, -0.0158,  0.1243,  0.1441, -0.0019, -0.1430,  0.0523,\n",
      "         -0.0025,  0.1918,  0.0523,  0.2101],\n",
      "        [ 0.1995, -0.1773,  0.0303,  0.0054,  0.1485,  0.0645, -0.0562,  0.1980,\n",
      "         -0.2197, -0.0987,  0.1595, -0.0540, -0.1917,  0.1754,  0.1897, -0.1625,\n",
      "          0.0871, -0.0064, -0.1100, -0.1132],\n",
      "        [ 0.0794,  0.0064, -0.1509,  0.2149,  0.1575, -0.2054,  0.1523, -0.1019,\n",
      "          0.0089,  0.0956, -0.0849,  0.1609, -0.1918, -0.2105,  0.1427,  0.2115,\n",
      "          0.0102,  0.2154, -0.0078,  0.0388],\n",
      "        [ 0.0628,  0.2034,  0.1590, -0.2100, -0.1257, -0.2033, -0.0058,  0.1038,\n",
      "          0.0741, -0.1123,  0.1540, -0.1160, -0.0848,  0.1068,  0.1738,  0.1183,\n",
      "          0.1169,  0.0633, -0.1634, -0.0659],\n",
      "        [ 0.2124,  0.1196,  0.0226, -0.0770,  0.2113, -0.1334, -0.1701, -0.1605,\n",
      "         -0.0418, -0.2141, -0.2005, -0.1386,  0.1770, -0.2022,  0.1918, -0.1340,\n",
      "         -0.0171, -0.0385, -0.1543,  0.1600],\n",
      "        [-0.0520, -0.1533, -0.1716, -0.0026, -0.1417,  0.0957,  0.0911, -0.2154,\n",
      "         -0.1436,  0.1195,  0.1755,  0.1731, -0.0155, -0.0732, -0.1193, -0.1637,\n",
      "         -0.1491, -0.2159,  0.0325, -0.1234]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1171, -0.0704, -0.0761,  0.0702, -0.1893,  0.1273,  0.1145,  0.0477,\n",
      "         0.2147, -0.1767, -0.0043, -0.0449,  0.0228,  0.0772, -0.1773, -0.1881,\n",
      "         0.1881,  0.1197, -0.0206,  0.1710], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 2.4061e-02, -2.0006e-01, -1.0221e-01,  4.1105e-02,  7.1014e-02,\n",
      "          2.2024e-01, -1.9169e-01, -7.9832e-02,  1.3167e-01, -1.6941e-01,\n",
      "         -1.9645e-01, -2.1718e-01,  8.7820e-02, -5.5547e-02,  4.7386e-02,\n",
      "          1.5990e-01, -1.5445e-01,  2.0958e-01,  1.2670e-01,  1.4069e-02],\n",
      "        [-6.1118e-02, -1.6004e-01,  1.1641e-01,  1.7563e-01,  1.0540e-01,\n",
      "         -8.0306e-02, -1.3429e-02, -8.2749e-02,  1.5112e-01, -9.8876e-02,\n",
      "         -1.7550e-01, -1.9365e-01,  3.8639e-02,  5.0212e-02,  2.1962e-01,\n",
      "          8.5950e-02, -1.5008e-01,  2.0541e-01, -1.1087e-01, -1.8256e-02],\n",
      "        [ 5.2712e-02, -9.5520e-02, -1.5279e-01,  4.5508e-02,  1.8009e-01,\n",
      "         -1.0829e-01, -1.2020e-01, -6.3870e-02, -9.1152e-02,  1.5960e-01,\n",
      "          1.5155e-01,  1.5929e-01, -4.3693e-02, -4.0138e-03,  9.4721e-02,\n",
      "          7.6645e-02, -4.7344e-02, -2.2261e-01,  1.5645e-01,  8.1569e-02],\n",
      "        [-1.8573e-01,  8.0028e-02,  1.6859e-02, -1.5504e-01,  1.6016e-01,\n",
      "         -1.9458e-01,  2.1691e-01,  1.0521e-02, -1.0672e-01,  6.1637e-02,\n",
      "          2.1495e-01, -4.0436e-02,  1.2094e-01, -2.3740e-02, -1.9601e-01,\n",
      "          1.0164e-01, -1.5895e-01, -2.7534e-02, -7.2598e-03, -1.3361e-01],\n",
      "        [ 1.5625e-01, -2.2158e-01,  2.0888e-01, -2.0901e-01,  4.0068e-02,\n",
      "         -9.3854e-02, -1.1794e-02, -1.7533e-01, -1.9259e-01, -1.9341e-01,\n",
      "          7.2907e-02,  1.3785e-01,  8.8325e-02, -2.1162e-01, -2.0990e-01,\n",
      "          1.7992e-01,  2.1410e-01, -1.4709e-01,  9.5912e-03, -1.2766e-01],\n",
      "        [-7.3393e-02, -1.0276e-01,  1.0271e-01,  1.8122e-01,  1.3262e-01,\n",
      "          9.0187e-02, -1.0522e-01, -1.1794e-01, -1.5950e-01, -1.5373e-01,\n",
      "         -4.9542e-02,  1.9906e-01,  2.2215e-01,  7.0985e-02,  3.3946e-02,\n",
      "         -2.0618e-01, -1.2511e-01,  8.7887e-02,  1.9822e-02, -1.3345e-02],\n",
      "        [-5.0228e-02, -1.3899e-01, -7.4671e-02,  9.9750e-02, -2.1079e-01,\n",
      "          5.0914e-03, -1.0455e-01, -5.6548e-02,  1.5673e-01, -2.7171e-02,\n",
      "         -1.7774e-01,  1.8913e-01, -3.6407e-02,  2.2235e-01, -7.3405e-02,\n",
      "          4.2960e-02,  2.8486e-03,  1.2124e-01, -1.0099e-01,  1.7065e-01],\n",
      "        [ 2.4455e-02, -1.1416e-01,  4.6538e-02, -1.2966e-01,  1.9053e-01,\n",
      "          7.4804e-02, -1.7928e-02, -2.1709e-01, -1.5939e-01, -2.2231e-01,\n",
      "          9.0704e-02, -1.3503e-01,  6.2825e-02,  2.0622e-01, -5.7326e-02,\n",
      "         -7.3765e-03,  1.3163e-02,  9.0754e-02, -2.8462e-02, -1.2358e-01],\n",
      "        [-1.9104e-01,  5.3152e-05,  1.3142e-01, -1.6653e-02, -1.3547e-01,\n",
      "          4.7268e-02, -7.5458e-02,  1.6570e-01,  1.3934e-01, -5.8741e-02,\n",
      "         -1.2523e-01,  1.7082e-03,  4.4848e-02,  1.2198e-01, -7.1863e-02,\n",
      "          1.9797e-03,  8.0884e-02, -5.4238e-02,  4.1719e-02,  1.9682e-01],\n",
      "        [-3.8825e-02, -9.2799e-02,  8.0834e-03,  1.8880e-01, -2.1977e-01,\n",
      "          7.4178e-03, -8.1281e-03,  4.7926e-02, -1.8127e-01, -1.8088e-01,\n",
      "         -4.2402e-02,  1.7167e-03,  4.1135e-02, -4.9161e-02, -1.4294e-01,\n",
      "         -2.1113e-01, -1.1551e-01, -1.6679e-01,  2.0973e-01, -8.5744e-02],\n",
      "        [ 1.6461e-02,  6.8699e-02, -1.5043e-01,  1.1989e-01,  1.3760e-01,\n",
      "          8.5002e-02, -1.8679e-01, -3.2779e-02, -1.0719e-01, -1.2200e-01,\n",
      "         -1.0797e-01,  1.7857e-02, -1.6273e-01, -1.9479e-01,  7.3782e-02,\n",
      "          9.2781e-02,  7.8435e-02, -6.0404e-03, -7.3929e-02, -4.9899e-02],\n",
      "        [-4.5388e-02, -1.2993e-01, -9.5395e-02, -3.3230e-03,  3.1361e-02,\n",
      "         -7.8596e-02,  1.1310e-01,  1.4280e-01, -2.1495e-01,  5.4219e-02,\n",
      "         -1.8761e-01, -1.0710e-01,  3.5356e-02, -1.6470e-01, -1.9597e-01,\n",
      "         -1.2302e-01,  1.8938e-01, -8.7966e-02,  1.6874e-02, -1.1113e-02],\n",
      "        [ 1.8641e-01,  9.7054e-04,  6.3941e-02, -1.0797e-01, -1.5361e-01,\n",
      "          5.4679e-02, -6.4424e-02, -1.9181e-01, -5.4847e-02, -1.3684e-01,\n",
      "         -1.7628e-01, -9.5016e-02, -1.6197e-02, -8.1784e-02,  1.1182e-01,\n",
      "         -4.9592e-02, -1.4093e-01,  1.7072e-01, -5.3761e-02,  3.9980e-02],\n",
      "        [-1.6967e-01, -1.4977e-01,  1.3577e-01,  1.9827e-01,  9.0416e-02,\n",
      "         -9.6073e-02, -8.1038e-02, -2.8769e-02, -5.9069e-02, -4.7701e-03,\n",
      "         -1.8451e-01, -1.5240e-01,  4.0654e-02,  1.1522e-01,  2.1608e-01,\n",
      "         -1.3241e-01,  6.6489e-02,  2.0368e-01,  1.6639e-01, -1.5933e-01],\n",
      "        [ 9.4333e-02,  1.2339e-01,  9.6362e-03,  1.6903e-01,  1.2277e-01,\n",
      "          1.2213e-01, -1.0063e-01, -1.3288e-02, -2.0273e-01,  9.2158e-02,\n",
      "         -8.5143e-02, -2.1910e-01,  4.2218e-02, -1.4213e-01,  1.0050e-01,\n",
      "         -7.5974e-03, -2.2068e-01,  1.1300e-01,  9.8962e-02,  1.5272e-01],\n",
      "        [ 2.0129e-01, -2.7656e-02, -1.4740e-01,  2.3772e-03,  2.0535e-01,\n",
      "         -6.8359e-02, -1.9259e-01,  3.2130e-02, -2.1811e-01, -2.0004e-01,\n",
      "          1.0452e-01, -1.5169e-01, -4.8720e-02,  4.3552e-02,  1.9387e-02,\n",
      "          1.8871e-01, -1.6576e-01, -9.4159e-02,  6.2629e-02,  6.6025e-02],\n",
      "        [ 1.1749e-01,  2.2607e-02,  4.7694e-02,  1.4138e-01,  2.0846e-01,\n",
      "          6.5402e-02, -1.5992e-01,  1.4492e-01, -1.6834e-01,  1.0426e-01,\n",
      "          2.1501e-01, -1.7199e-01, -4.5172e-02,  5.8214e-03, -1.5687e-01,\n",
      "         -1.6032e-01,  4.2933e-02,  1.4302e-01,  9.7590e-02, -5.7108e-02],\n",
      "        [-1.4315e-01, -1.1918e-02,  1.2232e-01,  2.0827e-01, -4.8011e-02,\n",
      "          7.6350e-02, -1.6607e-01,  7.9421e-02,  1.4501e-01, -2.0045e-01,\n",
      "         -2.7997e-02,  1.7503e-01,  1.0188e-01,  2.1390e-03, -2.4512e-02,\n",
      "          1.0724e-01, -9.4480e-02,  7.1611e-02, -9.7713e-02, -1.4816e-01],\n",
      "        [ 5.8245e-02,  1.5382e-01, -1.2571e-01,  2.0225e-01, -4.3209e-02,\n",
      "         -1.6791e-01,  2.1438e-01, -2.1795e-01, -8.3313e-02, -1.4831e-02,\n",
      "         -1.4361e-01,  4.5635e-03,  6.7845e-02,  6.7782e-02,  3.5864e-02,\n",
      "         -1.2658e-01, -1.9886e-01,  8.6517e-02, -1.5308e-02, -1.6088e-01],\n",
      "        [ 2.4097e-02,  1.9386e-01, -1.2453e-01, -2.3576e-02,  9.7730e-02,\n",
      "         -1.5104e-01,  6.7127e-02, -1.1423e-01, -1.6820e-02,  1.7680e-01,\n",
      "          3.8279e-03, -3.3486e-02,  1.7983e-02, -3.2160e-02,  1.9429e-01,\n",
      "         -2.7462e-02, -8.7376e-03, -1.5894e-01, -8.5582e-02, -5.3193e-02]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0340, -0.1817,  0.0905, -0.0048,  0.1606,  0.1927,  0.1478, -0.0537,\n",
      "         0.2191,  0.0991,  0.0292,  0.0520,  0.1881, -0.0394, -0.2109, -0.1193,\n",
      "         0.0112, -0.1210, -0.1450, -0.1870], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0753,  0.2147,  0.0320,  0.1006, -0.1838,  0.1245, -0.1137, -0.1634,\n",
      "          0.0871,  0.1209,  0.0948,  0.2133,  0.1877,  0.1748,  0.1069,  0.0151,\n",
      "          0.0793,  0.0269, -0.1243,  0.1859],\n",
      "        [-0.0228, -0.0507,  0.1266, -0.2113,  0.1302, -0.0443, -0.1130,  0.0815,\n",
      "         -0.1849,  0.1546,  0.2002,  0.0657, -0.1962,  0.0039,  0.1641,  0.0059,\n",
      "         -0.0562,  0.1035, -0.2103,  0.1927],\n",
      "        [-0.1304,  0.0468, -0.1694,  0.1097,  0.0361, -0.2050,  0.0215,  0.1335,\n",
      "         -0.0682,  0.0590, -0.1135,  0.1964,  0.2036,  0.1336, -0.1261, -0.2180,\n",
      "         -0.0234,  0.1257,  0.0980,  0.1982],\n",
      "        [ 0.0403,  0.1482, -0.1874,  0.0154,  0.2149,  0.1138, -0.1066, -0.0596,\n",
      "          0.1265,  0.0368, -0.1708,  0.0714, -0.2184,  0.1063,  0.0783, -0.1371,\n",
      "         -0.1820,  0.0648, -0.1463, -0.1637],\n",
      "        [ 0.1594,  0.2129, -0.0421, -0.1637,  0.2021,  0.0048,  0.0070, -0.1664,\n",
      "          0.0179, -0.0897,  0.1859,  0.0009,  0.0202, -0.1964, -0.0368, -0.0306,\n",
      "         -0.0059,  0.1427, -0.0247, -0.1404]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0445, -0.2162,  0.0724, -0.0205, -0.0591], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3420, -0.3370],\n",
      "        [-0.2704,  0.1828]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.6008, -0.3217], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in obj1.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59iLYeT-NRsG"
   },
   "source": [
    "# RUN for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ekz8oACnb3tA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:378: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n",
      "/Users/mohsennabian/Desktop/ci_vae/example/ci_vae/ivae.py:406: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(torch.reshape(y, (-1,)), dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 total_train_loss: 1.002071 Total_test_loss: 0.878473 Total_BCE_test_loss: 0.793575 Total_KLD_test_loss: 0.000006 Total_CEP_test_loss: 0.084893\n",
      "====> Epoch: 2 total_train_loss: 0.939039 Total_test_loss: 0.871801 Total_BCE_test_loss: 0.790666 Total_KLD_test_loss: 0.000002 Total_CEP_test_loss: 0.081133\n",
      "====> Epoch: 3 total_train_loss: 0.921352 Total_test_loss: 0.871313 Total_BCE_test_loss: 0.789073 Total_KLD_test_loss: 0.000003 Total_CEP_test_loss: 0.082236\n",
      "====> Epoch: 4 total_train_loss: 0.901816 Total_test_loss: 0.864607 Total_BCE_test_loss: 0.788611 Total_KLD_test_loss: 0.000008 Total_CEP_test_loss: 0.075989\n",
      "====> Epoch: 5 total_train_loss: 0.890170 Total_test_loss: 0.864015 Total_BCE_test_loss: 0.788591 Total_KLD_test_loss: 0.000012 Total_CEP_test_loss: 0.075412\n",
      "====> Epoch: 6 total_train_loss: 0.889733 Total_test_loss: 0.864211 Total_BCE_test_loss: 0.789090 Total_KLD_test_loss: 0.000019 Total_CEP_test_loss: 0.075102\n",
      "====> Epoch: 7 total_train_loss: 0.885155 Total_test_loss: 0.862018 Total_BCE_test_loss: 0.787319 Total_KLD_test_loss: 0.000035 Total_CEP_test_loss: 0.074664\n",
      "====> Epoch: 8 total_train_loss: 0.878832 Total_test_loss: 0.857396 Total_BCE_test_loss: 0.783683 Total_KLD_test_loss: 0.000074 Total_CEP_test_loss: 0.073639\n",
      "====> Epoch: 9 total_train_loss: 0.869550 Total_test_loss: 0.856772 Total_BCE_test_loss: 0.781762 Total_KLD_test_loss: 0.000130 Total_CEP_test_loss: 0.074879\n",
      "====> Epoch: 10 total_train_loss: 0.869194 Total_test_loss: 0.849514 Total_BCE_test_loss: 0.774912 Total_KLD_test_loss: 0.000175 Total_CEP_test_loss: 0.074427\n",
      "====> Epoch: 11 total_train_loss: 0.866314 Total_test_loss: 0.843344 Total_BCE_test_loss: 0.770392 Total_KLD_test_loss: 0.000200 Total_CEP_test_loss: 0.072753\n",
      "====> Epoch: 12 total_train_loss: 0.864888 Total_test_loss: 0.839045 Total_BCE_test_loss: 0.766907 Total_KLD_test_loss: 0.000217 Total_CEP_test_loss: 0.071921\n",
      "====> Epoch: 13 total_train_loss: 0.852822 Total_test_loss: 0.834356 Total_BCE_test_loss: 0.762290 Total_KLD_test_loss: 0.000241 Total_CEP_test_loss: 0.071826\n",
      "====> Epoch: 14 total_train_loss: 0.853359 Total_test_loss: 0.827013 Total_BCE_test_loss: 0.755719 Total_KLD_test_loss: 0.000266 Total_CEP_test_loss: 0.071028\n",
      "====> Epoch: 15 total_train_loss: 0.846902 Total_test_loss: 0.820153 Total_BCE_test_loss: 0.749249 Total_KLD_test_loss: 0.000309 Total_CEP_test_loss: 0.070595\n",
      "====> Epoch: 16 total_train_loss: 0.842665 Total_test_loss: 0.816941 Total_BCE_test_loss: 0.746204 Total_KLD_test_loss: 0.000353 Total_CEP_test_loss: 0.070385\n",
      "====> Epoch: 17 total_train_loss: 0.834204 Total_test_loss: 0.810518 Total_BCE_test_loss: 0.739936 Total_KLD_test_loss: 0.000408 Total_CEP_test_loss: 0.070174\n",
      "====> Epoch: 18 total_train_loss: 0.829977 Total_test_loss: 0.808777 Total_BCE_test_loss: 0.738321 Total_KLD_test_loss: 0.000447 Total_CEP_test_loss: 0.070008\n",
      "====> Epoch: 19 total_train_loss: 0.823587 Total_test_loss: 0.805509 Total_BCE_test_loss: 0.735206 Total_KLD_test_loss: 0.000525 Total_CEP_test_loss: 0.069778\n",
      "====> Epoch: 20 total_train_loss: 0.823884 Total_test_loss: 0.801131 Total_BCE_test_loss: 0.730770 Total_KLD_test_loss: 0.000548 Total_CEP_test_loss: 0.069812\n",
      "====> Epoch: 21 total_train_loss: 0.816192 Total_test_loss: 0.795909 Total_BCE_test_loss: 0.725681 Total_KLD_test_loss: 0.000555 Total_CEP_test_loss: 0.069673\n",
      "====> Epoch: 22 total_train_loss: 0.812680 Total_test_loss: 0.788680 Total_BCE_test_loss: 0.718469 Total_KLD_test_loss: 0.000593 Total_CEP_test_loss: 0.069617\n",
      "====> Epoch: 23 total_train_loss: 0.809432 Total_test_loss: 0.786829 Total_BCE_test_loss: 0.716687 Total_KLD_test_loss: 0.000626 Total_CEP_test_loss: 0.069516\n",
      "====> Epoch: 24 total_train_loss: 0.803727 Total_test_loss: 0.782291 Total_BCE_test_loss: 0.712089 Total_KLD_test_loss: 0.000658 Total_CEP_test_loss: 0.069544\n",
      "====> Epoch: 25 total_train_loss: 0.801029 Total_test_loss: 0.777870 Total_BCE_test_loss: 0.707604 Total_KLD_test_loss: 0.000697 Total_CEP_test_loss: 0.069570\n",
      "====> Epoch: 26 total_train_loss: 0.799353 Total_test_loss: 0.775978 Total_BCE_test_loss: 0.705719 Total_KLD_test_loss: 0.000773 Total_CEP_test_loss: 0.069486\n",
      "====> Epoch: 27 total_train_loss: 0.789933 Total_test_loss: 0.767858 Total_BCE_test_loss: 0.697543 Total_KLD_test_loss: 0.000849 Total_CEP_test_loss: 0.069466\n",
      "====> Epoch: 28 total_train_loss: 0.792444 Total_test_loss: 0.759900 Total_BCE_test_loss: 0.689613 Total_KLD_test_loss: 0.000916 Total_CEP_test_loss: 0.069371\n",
      "====> Epoch: 29 total_train_loss: 0.783975 Total_test_loss: 0.753008 Total_BCE_test_loss: 0.682683 Total_KLD_test_loss: 0.000991 Total_CEP_test_loss: 0.069334\n",
      "====> Epoch: 30 total_train_loss: 0.778413 Total_test_loss: 0.745239 Total_BCE_test_loss: 0.674875 Total_KLD_test_loss: 0.001054 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 31 total_train_loss: 0.771602 Total_test_loss: 0.741875 Total_BCE_test_loss: 0.671505 Total_KLD_test_loss: 0.001129 Total_CEP_test_loss: 0.069241\n",
      "====> Epoch: 32 total_train_loss: 0.761147 Total_test_loss: 0.735914 Total_BCE_test_loss: 0.665419 Total_KLD_test_loss: 0.001193 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 33 total_train_loss: 0.762908 Total_test_loss: 0.731545 Total_BCE_test_loss: 0.661014 Total_KLD_test_loss: 0.001221 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 34 total_train_loss: 0.755990 Total_test_loss: 0.726615 Total_BCE_test_loss: 0.655870 Total_KLD_test_loss: 0.001301 Total_CEP_test_loss: 0.069444\n",
      "====> Epoch: 35 total_train_loss: 0.748494 Total_test_loss: 0.716760 Total_BCE_test_loss: 0.646065 Total_KLD_test_loss: 0.001377 Total_CEP_test_loss: 0.069317\n",
      "====> Epoch: 36 total_train_loss: 0.749910 Total_test_loss: 0.711766 Total_BCE_test_loss: 0.641029 Total_KLD_test_loss: 0.001439 Total_CEP_test_loss: 0.069299\n",
      "====> Epoch: 37 total_train_loss: 0.739918 Total_test_loss: 0.704305 Total_BCE_test_loss: 0.633523 Total_KLD_test_loss: 0.001497 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 38 total_train_loss: 0.735539 Total_test_loss: 0.703801 Total_BCE_test_loss: 0.632872 Total_KLD_test_loss: 0.001573 Total_CEP_test_loss: 0.069356\n",
      "====> Epoch: 39 total_train_loss: 0.730335 Total_test_loss: 0.704237 Total_BCE_test_loss: 0.633234 Total_KLD_test_loss: 0.001653 Total_CEP_test_loss: 0.069350\n",
      "====> Epoch: 40 total_train_loss: 0.731971 Total_test_loss: 0.700464 Total_BCE_test_loss: 0.629394 Total_KLD_test_loss: 0.001715 Total_CEP_test_loss: 0.069355\n",
      "====> Epoch: 41 total_train_loss: 0.727399 Total_test_loss: 0.693205 Total_BCE_test_loss: 0.622097 Total_KLD_test_loss: 0.001773 Total_CEP_test_loss: 0.069335\n",
      "====> Epoch: 42 total_train_loss: 0.724023 Total_test_loss: 0.688373 Total_BCE_test_loss: 0.617217 Total_KLD_test_loss: 0.001855 Total_CEP_test_loss: 0.069301\n",
      "====> Epoch: 43 total_train_loss: 0.716978 Total_test_loss: 0.684338 Total_BCE_test_loss: 0.613164 Total_KLD_test_loss: 0.001892 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 44 total_train_loss: 0.716072 Total_test_loss: 0.684674 Total_BCE_test_loss: 0.613425 Total_KLD_test_loss: 0.001945 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 45 total_train_loss: 0.715857 Total_test_loss: 0.683419 Total_BCE_test_loss: 0.612107 Total_KLD_test_loss: 0.001998 Total_CEP_test_loss: 0.069314\n",
      "====> Epoch: 46 total_train_loss: 0.714036 Total_test_loss: 0.680554 Total_BCE_test_loss: 0.609201 Total_KLD_test_loss: 0.002045 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 47 total_train_loss: 0.713609 Total_test_loss: 0.675008 Total_BCE_test_loss: 0.603612 Total_KLD_test_loss: 0.002086 Total_CEP_test_loss: 0.069310\n",
      "====> Epoch: 48 total_train_loss: 0.708220 Total_test_loss: 0.668298 Total_BCE_test_loss: 0.596839 Total_KLD_test_loss: 0.002156 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 49 total_train_loss: 0.708001 Total_test_loss: 0.668774 Total_BCE_test_loss: 0.597294 Total_KLD_test_loss: 0.002220 Total_CEP_test_loss: 0.069261\n",
      "====> Epoch: 50 total_train_loss: 0.699848 Total_test_loss: 0.664979 Total_BCE_test_loss: 0.593425 Total_KLD_test_loss: 0.002280 Total_CEP_test_loss: 0.069274\n",
      "====> Epoch: 51 total_train_loss: 0.706583 Total_test_loss: 0.665625 Total_BCE_test_loss: 0.593953 Total_KLD_test_loss: 0.002361 Total_CEP_test_loss: 0.069311\n",
      "====> Epoch: 52 total_train_loss: 0.699692 Total_test_loss: 0.664933 Total_BCE_test_loss: 0.593186 Total_KLD_test_loss: 0.002425 Total_CEP_test_loss: 0.069322\n",
      "====> Epoch: 53 total_train_loss: 0.702693 Total_test_loss: 0.660435 Total_BCE_test_loss: 0.588615 Total_KLD_test_loss: 0.002461 Total_CEP_test_loss: 0.069359\n",
      "====> Epoch: 54 total_train_loss: 0.694160 Total_test_loss: 0.662917 Total_BCE_test_loss: 0.591008 Total_KLD_test_loss: 0.002532 Total_CEP_test_loss: 0.069377\n",
      "====> Epoch: 55 total_train_loss: 0.694360 Total_test_loss: 0.656125 Total_BCE_test_loss: 0.584165 Total_KLD_test_loss: 0.002554 Total_CEP_test_loss: 0.069406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 56 total_train_loss: 0.690416 Total_test_loss: 0.654416 Total_BCE_test_loss: 0.582443 Total_KLD_test_loss: 0.002576 Total_CEP_test_loss: 0.069396\n",
      "====> Epoch: 57 total_train_loss: 0.689695 Total_test_loss: 0.649959 Total_BCE_test_loss: 0.578014 Total_KLD_test_loss: 0.002582 Total_CEP_test_loss: 0.069363\n",
      "====> Epoch: 58 total_train_loss: 0.685463 Total_test_loss: 0.648722 Total_BCE_test_loss: 0.576813 Total_KLD_test_loss: 0.002629 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 59 total_train_loss: 0.688819 Total_test_loss: 0.649671 Total_BCE_test_loss: 0.577731 Total_KLD_test_loss: 0.002642 Total_CEP_test_loss: 0.069298\n",
      "====> Epoch: 60 total_train_loss: 0.682691 Total_test_loss: 0.644206 Total_BCE_test_loss: 0.572233 Total_KLD_test_loss: 0.002671 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 61 total_train_loss: 0.679948 Total_test_loss: 0.642990 Total_BCE_test_loss: 0.570933 Total_KLD_test_loss: 0.002736 Total_CEP_test_loss: 0.069321\n",
      "====> Epoch: 62 total_train_loss: 0.675042 Total_test_loss: 0.641548 Total_BCE_test_loss: 0.569477 Total_KLD_test_loss: 0.002785 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 63 total_train_loss: 0.675146 Total_test_loss: 0.641073 Total_BCE_test_loss: 0.568982 Total_KLD_test_loss: 0.002857 Total_CEP_test_loss: 0.069234\n",
      "====> Epoch: 64 total_train_loss: 0.676732 Total_test_loss: 0.635028 Total_BCE_test_loss: 0.562873 Total_KLD_test_loss: 0.002914 Total_CEP_test_loss: 0.069240\n",
      "====> Epoch: 65 total_train_loss: 0.674982 Total_test_loss: 0.636203 Total_BCE_test_loss: 0.563925 Total_KLD_test_loss: 0.003012 Total_CEP_test_loss: 0.069266\n",
      "====> Epoch: 66 total_train_loss: 0.671508 Total_test_loss: 0.633895 Total_BCE_test_loss: 0.561477 Total_KLD_test_loss: 0.003085 Total_CEP_test_loss: 0.069333\n",
      "====> Epoch: 67 total_train_loss: 0.672565 Total_test_loss: 0.632671 Total_BCE_test_loss: 0.560231 Total_KLD_test_loss: 0.003139 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 68 total_train_loss: 0.669661 Total_test_loss: 0.634948 Total_BCE_test_loss: 0.562550 Total_KLD_test_loss: 0.003148 Total_CEP_test_loss: 0.069250\n",
      "====> Epoch: 69 total_train_loss: 0.670781 Total_test_loss: 0.634916 Total_BCE_test_loss: 0.562503 Total_KLD_test_loss: 0.003196 Total_CEP_test_loss: 0.069217\n",
      "====> Epoch: 70 total_train_loss: 0.672446 Total_test_loss: 0.630532 Total_BCE_test_loss: 0.558137 Total_KLD_test_loss: 0.003193 Total_CEP_test_loss: 0.069202\n",
      "====> Epoch: 71 total_train_loss: 0.669652 Total_test_loss: 0.628840 Total_BCE_test_loss: 0.556372 Total_KLD_test_loss: 0.003224 Total_CEP_test_loss: 0.069243\n",
      "====> Epoch: 72 total_train_loss: 0.670302 Total_test_loss: 0.628186 Total_BCE_test_loss: 0.555567 Total_KLD_test_loss: 0.003254 Total_CEP_test_loss: 0.069364\n",
      "====> Epoch: 73 total_train_loss: 0.669831 Total_test_loss: 0.628438 Total_BCE_test_loss: 0.555713 Total_KLD_test_loss: 0.003214 Total_CEP_test_loss: 0.069512\n",
      "====> Epoch: 74 total_train_loss: 0.663118 Total_test_loss: 0.626994 Total_BCE_test_loss: 0.554288 Total_KLD_test_loss: 0.003148 Total_CEP_test_loss: 0.069559\n",
      "====> Epoch: 75 total_train_loss: 0.667250 Total_test_loss: 0.627567 Total_BCE_test_loss: 0.554977 Total_KLD_test_loss: 0.003124 Total_CEP_test_loss: 0.069465\n",
      "====> Epoch: 76 total_train_loss: 0.662356 Total_test_loss: 0.623646 Total_BCE_test_loss: 0.551194 Total_KLD_test_loss: 0.003150 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 77 total_train_loss: 0.662217 Total_test_loss: 0.622781 Total_BCE_test_loss: 0.550338 Total_KLD_test_loss: 0.003197 Total_CEP_test_loss: 0.069246\n",
      "====> Epoch: 78 total_train_loss: 0.660724 Total_test_loss: 0.624231 Total_BCE_test_loss: 0.551768 Total_KLD_test_loss: 0.003199 Total_CEP_test_loss: 0.069263\n",
      "====> Epoch: 79 total_train_loss: 0.664731 Total_test_loss: 0.619825 Total_BCE_test_loss: 0.547352 Total_KLD_test_loss: 0.003198 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 80 total_train_loss: 0.663069 Total_test_loss: 0.621345 Total_BCE_test_loss: 0.548831 Total_KLD_test_loss: 0.003245 Total_CEP_test_loss: 0.069269\n",
      "====> Epoch: 81 total_train_loss: 0.659550 Total_test_loss: 0.618772 Total_BCE_test_loss: 0.546259 Total_KLD_test_loss: 0.003267 Total_CEP_test_loss: 0.069246\n",
      "====> Epoch: 82 total_train_loss: 0.661393 Total_test_loss: 0.618488 Total_BCE_test_loss: 0.545957 Total_KLD_test_loss: 0.003256 Total_CEP_test_loss: 0.069275\n",
      "====> Epoch: 83 total_train_loss: 0.657438 Total_test_loss: 0.617331 Total_BCE_test_loss: 0.544713 Total_KLD_test_loss: 0.003323 Total_CEP_test_loss: 0.069295\n",
      "====> Epoch: 84 total_train_loss: 0.657337 Total_test_loss: 0.614857 Total_BCE_test_loss: 0.542118 Total_KLD_test_loss: 0.003364 Total_CEP_test_loss: 0.069375\n",
      "====> Epoch: 85 total_train_loss: 0.654977 Total_test_loss: 0.613289 Total_BCE_test_loss: 0.540395 Total_KLD_test_loss: 0.003385 Total_CEP_test_loss: 0.069510\n",
      "====> Epoch: 86 total_train_loss: 0.662845 Total_test_loss: 0.614171 Total_BCE_test_loss: 0.541340 Total_KLD_test_loss: 0.003383 Total_CEP_test_loss: 0.069447\n",
      "====> Epoch: 87 total_train_loss: 0.658558 Total_test_loss: 0.611364 Total_BCE_test_loss: 0.538621 Total_KLD_test_loss: 0.003398 Total_CEP_test_loss: 0.069345\n",
      "====> Epoch: 88 total_train_loss: 0.654484 Total_test_loss: 0.610934 Total_BCE_test_loss: 0.538257 Total_KLD_test_loss: 0.003415 Total_CEP_test_loss: 0.069262\n",
      "====> Epoch: 89 total_train_loss: 0.653021 Total_test_loss: 0.611070 Total_BCE_test_loss: 0.538372 Total_KLD_test_loss: 0.003417 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 90 total_train_loss: 0.654098 Total_test_loss: 0.611034 Total_BCE_test_loss: 0.538288 Total_KLD_test_loss: 0.003442 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 91 total_train_loss: 0.653043 Total_test_loss: 0.614004 Total_BCE_test_loss: 0.541281 Total_KLD_test_loss: 0.003407 Total_CEP_test_loss: 0.069317\n",
      "====> Epoch: 92 total_train_loss: 0.656876 Total_test_loss: 0.613682 Total_BCE_test_loss: 0.540942 Total_KLD_test_loss: 0.003420 Total_CEP_test_loss: 0.069320\n",
      "====> Epoch: 93 total_train_loss: 0.654458 Total_test_loss: 0.614782 Total_BCE_test_loss: 0.542013 Total_KLD_test_loss: 0.003433 Total_CEP_test_loss: 0.069336\n",
      "====> Epoch: 94 total_train_loss: 0.653079 Total_test_loss: 0.615569 Total_BCE_test_loss: 0.542849 Total_KLD_test_loss: 0.003400 Total_CEP_test_loss: 0.069320\n",
      "====> Epoch: 95 total_train_loss: 0.657978 Total_test_loss: 0.608658 Total_BCE_test_loss: 0.535928 Total_KLD_test_loss: 0.003378 Total_CEP_test_loss: 0.069353\n",
      "====> Epoch: 96 total_train_loss: 0.650336 Total_test_loss: 0.604585 Total_BCE_test_loss: 0.531842 Total_KLD_test_loss: 0.003412 Total_CEP_test_loss: 0.069331\n",
      "====> Epoch: 97 total_train_loss: 0.647768 Total_test_loss: 0.601355 Total_BCE_test_loss: 0.528625 Total_KLD_test_loss: 0.003433 Total_CEP_test_loss: 0.069297\n",
      "====> Epoch: 98 total_train_loss: 0.645132 Total_test_loss: 0.600624 Total_BCE_test_loss: 0.527858 Total_KLD_test_loss: 0.003466 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 99 total_train_loss: 0.648750 Total_test_loss: 0.601876 Total_BCE_test_loss: 0.529064 Total_KLD_test_loss: 0.003509 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 100 total_train_loss: 0.646579 Total_test_loss: 0.603053 Total_BCE_test_loss: 0.530266 Total_KLD_test_loss: 0.003516 Total_CEP_test_loss: 0.069270\n",
      "0.001\n",
      "0.001\n",
      "model saved\n",
      "0.001\n",
      "0.0005\n",
      "====> Epoch: 1 total_train_loss: 0.645491 Total_test_loss: 0.600290 Total_BCE_test_loss: 0.527516 Total_KLD_test_loss: 0.003493 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 2 total_train_loss: 0.646496 Total_test_loss: 0.598085 Total_BCE_test_loss: 0.525349 Total_KLD_test_loss: 0.003455 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 3 total_train_loss: 0.645697 Total_test_loss: 0.597749 Total_BCE_test_loss: 0.525013 Total_KLD_test_loss: 0.003452 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 4 total_train_loss: 0.643137 Total_test_loss: 0.597968 Total_BCE_test_loss: 0.525240 Total_KLD_test_loss: 0.003442 Total_CEP_test_loss: 0.069286\n",
      "====> Epoch: 5 total_train_loss: 0.647497 Total_test_loss: 0.598035 Total_BCE_test_loss: 0.525328 Total_KLD_test_loss: 0.003426 Total_CEP_test_loss: 0.069282\n",
      "====> Epoch: 6 total_train_loss: 0.646855 Total_test_loss: 0.597554 Total_BCE_test_loss: 0.524818 Total_KLD_test_loss: 0.003447 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 7 total_train_loss: 0.644081 Total_test_loss: 0.597134 Total_BCE_test_loss: 0.524418 Total_KLD_test_loss: 0.003439 Total_CEP_test_loss: 0.069278\n",
      "====> Epoch: 8 total_train_loss: 0.648710 Total_test_loss: 0.596574 Total_BCE_test_loss: 0.523856 Total_KLD_test_loss: 0.003437 Total_CEP_test_loss: 0.069281\n",
      "====> Epoch: 9 total_train_loss: 0.642958 Total_test_loss: 0.595664 Total_BCE_test_loss: 0.522932 Total_KLD_test_loss: 0.003452 Total_CEP_test_loss: 0.069280\n",
      "====> Epoch: 10 total_train_loss: 0.646082 Total_test_loss: 0.596430 Total_BCE_test_loss: 0.523685 Total_KLD_test_loss: 0.003460 Total_CEP_test_loss: 0.069285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 11 total_train_loss: 0.641997 Total_test_loss: 0.594950 Total_BCE_test_loss: 0.522203 Total_KLD_test_loss: 0.003464 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 12 total_train_loss: 0.640105 Total_test_loss: 0.596204 Total_BCE_test_loss: 0.523485 Total_KLD_test_loss: 0.003431 Total_CEP_test_loss: 0.069287\n",
      "====> Epoch: 13 total_train_loss: 0.645811 Total_test_loss: 0.595430 Total_BCE_test_loss: 0.522720 Total_KLD_test_loss: 0.003421 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 14 total_train_loss: 0.644112 Total_test_loss: 0.594886 Total_BCE_test_loss: 0.522173 Total_KLD_test_loss: 0.003422 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 15 total_train_loss: 0.643866 Total_test_loss: 0.594227 Total_BCE_test_loss: 0.521503 Total_KLD_test_loss: 0.003428 Total_CEP_test_loss: 0.069295\n",
      "====> Epoch: 16 total_train_loss: 0.641863 Total_test_loss: 0.594581 Total_BCE_test_loss: 0.521847 Total_KLD_test_loss: 0.003433 Total_CEP_test_loss: 0.069301\n",
      "====> Epoch: 17 total_train_loss: 0.643761 Total_test_loss: 0.594821 Total_BCE_test_loss: 0.522084 Total_KLD_test_loss: 0.003433 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 18 total_train_loss: 0.646036 Total_test_loss: 0.594673 Total_BCE_test_loss: 0.521933 Total_KLD_test_loss: 0.003434 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 19 total_train_loss: 0.642276 Total_test_loss: 0.594474 Total_BCE_test_loss: 0.521743 Total_KLD_test_loss: 0.003426 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 20 total_train_loss: 0.644877 Total_test_loss: 0.594604 Total_BCE_test_loss: 0.521887 Total_KLD_test_loss: 0.003409 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 21 total_train_loss: 0.640278 Total_test_loss: 0.594969 Total_BCE_test_loss: 0.522240 Total_KLD_test_loss: 0.003423 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 22 total_train_loss: 0.640822 Total_test_loss: 0.594219 Total_BCE_test_loss: 0.521473 Total_KLD_test_loss: 0.003438 Total_CEP_test_loss: 0.069308\n",
      "====> Epoch: 23 total_train_loss: 0.642249 Total_test_loss: 0.594295 Total_BCE_test_loss: 0.521546 Total_KLD_test_loss: 0.003445 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 24 total_train_loss: 0.639735 Total_test_loss: 0.594279 Total_BCE_test_loss: 0.521553 Total_KLD_test_loss: 0.003421 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 25 total_train_loss: 0.643584 Total_test_loss: 0.595141 Total_BCE_test_loss: 0.522400 Total_KLD_test_loss: 0.003435 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 26 total_train_loss: 0.635821 Total_test_loss: 0.594714 Total_BCE_test_loss: 0.522018 Total_KLD_test_loss: 0.003400 Total_CEP_test_loss: 0.069296\n",
      "====> Epoch: 27 total_train_loss: 0.639300 Total_test_loss: 0.594243 Total_BCE_test_loss: 0.521532 Total_KLD_test_loss: 0.003417 Total_CEP_test_loss: 0.069294\n",
      "====> Epoch: 28 total_train_loss: 0.637311 Total_test_loss: 0.594196 Total_BCE_test_loss: 0.521481 Total_KLD_test_loss: 0.003423 Total_CEP_test_loss: 0.069292\n",
      "====> Epoch: 29 total_train_loss: 0.637938 Total_test_loss: 0.592948 Total_BCE_test_loss: 0.520227 Total_KLD_test_loss: 0.003426 Total_CEP_test_loss: 0.069295\n",
      "====> Epoch: 30 total_train_loss: 0.642283 Total_test_loss: 0.592398 Total_BCE_test_loss: 0.519661 Total_KLD_test_loss: 0.003450 Total_CEP_test_loss: 0.069288\n",
      "====> Epoch: 31 total_train_loss: 0.638946 Total_test_loss: 0.592045 Total_BCE_test_loss: 0.519279 Total_KLD_test_loss: 0.003481 Total_CEP_test_loss: 0.069284\n",
      "====> Epoch: 32 total_train_loss: 0.638486 Total_test_loss: 0.592538 Total_BCE_test_loss: 0.519780 Total_KLD_test_loss: 0.003474 Total_CEP_test_loss: 0.069285\n",
      "====> Epoch: 33 total_train_loss: 0.638895 Total_test_loss: 0.592663 Total_BCE_test_loss: 0.519875 Total_KLD_test_loss: 0.003499 Total_CEP_test_loss: 0.069290\n",
      "====> Epoch: 34 total_train_loss: 0.635271 Total_test_loss: 0.591662 Total_BCE_test_loss: 0.518860 Total_KLD_test_loss: 0.003513 Total_CEP_test_loss: 0.069289\n",
      "====> Epoch: 35 total_train_loss: 0.639387 Total_test_loss: 0.592127 Total_BCE_test_loss: 0.519340 Total_KLD_test_loss: 0.003496 Total_CEP_test_loss: 0.069291\n",
      "====> Epoch: 36 total_train_loss: 0.637042 Total_test_loss: 0.592213 Total_BCE_test_loss: 0.519466 Total_KLD_test_loss: 0.003453 Total_CEP_test_loss: 0.069293\n",
      "====> Epoch: 37 total_train_loss: 0.638067 Total_test_loss: 0.591100 Total_BCE_test_loss: 0.518353 Total_KLD_test_loss: 0.003454 Total_CEP_test_loss: 0.069293\n",
      "====> Epoch: 38 total_train_loss: 0.645984 Total_test_loss: 0.590927 Total_BCE_test_loss: 0.518172 Total_KLD_test_loss: 0.003459 Total_CEP_test_loss: 0.069296\n",
      "====> Epoch: 39 total_train_loss: 0.638824 Total_test_loss: 0.589906 Total_BCE_test_loss: 0.517123 Total_KLD_test_loss: 0.003483 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 40 total_train_loss: 0.639502 Total_test_loss: 0.590226 Total_BCE_test_loss: 0.517435 Total_KLD_test_loss: 0.003493 Total_CEP_test_loss: 0.069299\n",
      "====> Epoch: 41 total_train_loss: 0.641081 Total_test_loss: 0.590911 Total_BCE_test_loss: 0.518140 Total_KLD_test_loss: 0.003473 Total_CEP_test_loss: 0.069297\n",
      "====> Epoch: 42 total_train_loss: 0.637496 Total_test_loss: 0.591309 Total_BCE_test_loss: 0.518538 Total_KLD_test_loss: 0.003471 Total_CEP_test_loss: 0.069300\n",
      "====> Epoch: 43 total_train_loss: 0.637740 Total_test_loss: 0.591073 Total_BCE_test_loss: 0.518266 Total_KLD_test_loss: 0.003505 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 44 total_train_loss: 0.637796 Total_test_loss: 0.589733 Total_BCE_test_loss: 0.516933 Total_KLD_test_loss: 0.003498 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 45 total_train_loss: 0.642499 Total_test_loss: 0.589516 Total_BCE_test_loss: 0.516705 Total_KLD_test_loss: 0.003505 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 46 total_train_loss: 0.640218 Total_test_loss: 0.591067 Total_BCE_test_loss: 0.518266 Total_KLD_test_loss: 0.003495 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 47 total_train_loss: 0.636564 Total_test_loss: 0.590573 Total_BCE_test_loss: 0.517760 Total_KLD_test_loss: 0.003506 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 48 total_train_loss: 0.639654 Total_test_loss: 0.590455 Total_BCE_test_loss: 0.517640 Total_KLD_test_loss: 0.003509 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 49 total_train_loss: 0.638410 Total_test_loss: 0.589644 Total_BCE_test_loss: 0.516814 Total_KLD_test_loss: 0.003523 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 50 total_train_loss: 0.640463 Total_test_loss: 0.590800 Total_BCE_test_loss: 0.517987 Total_KLD_test_loss: 0.003504 Total_CEP_test_loss: 0.069308\n",
      "model saved\n",
      "1e-05\n",
      "====> Epoch: 1 total_train_loss: 0.634392 Total_test_loss: 0.590965 Total_BCE_test_loss: 0.518153 Total_KLD_test_loss: 0.003506 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 2 total_train_loss: 0.639129 Total_test_loss: 0.590316 Total_BCE_test_loss: 0.517525 Total_KLD_test_loss: 0.003484 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 3 total_train_loss: 0.635729 Total_test_loss: 0.590161 Total_BCE_test_loss: 0.517337 Total_KLD_test_loss: 0.003516 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 4 total_train_loss: 0.634362 Total_test_loss: 0.591126 Total_BCE_test_loss: 0.518340 Total_KLD_test_loss: 0.003479 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 5 total_train_loss: 0.632692 Total_test_loss: 0.591296 Total_BCE_test_loss: 0.518514 Total_KLD_test_loss: 0.003474 Total_CEP_test_loss: 0.069308\n",
      "====> Epoch: 6 total_train_loss: 0.638073 Total_test_loss: 0.590424 Total_BCE_test_loss: 0.517623 Total_KLD_test_loss: 0.003493 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 7 total_train_loss: 0.638732 Total_test_loss: 0.590707 Total_BCE_test_loss: 0.517942 Total_KLD_test_loss: 0.003459 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 8 total_train_loss: 0.635605 Total_test_loss: 0.591126 Total_BCE_test_loss: 0.518345 Total_KLD_test_loss: 0.003475 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 9 total_train_loss: 0.639873 Total_test_loss: 0.590972 Total_BCE_test_loss: 0.518200 Total_KLD_test_loss: 0.003467 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 10 total_train_loss: 0.638648 Total_test_loss: 0.591067 Total_BCE_test_loss: 0.518267 Total_KLD_test_loss: 0.003494 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 11 total_train_loss: 0.635092 Total_test_loss: 0.590533 Total_BCE_test_loss: 0.517750 Total_KLD_test_loss: 0.003476 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 12 total_train_loss: 0.639040 Total_test_loss: 0.590146 Total_BCE_test_loss: 0.517332 Total_KLD_test_loss: 0.003507 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 13 total_train_loss: 0.637252 Total_test_loss: 0.590610 Total_BCE_test_loss: 0.517761 Total_KLD_test_loss: 0.003543 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 14 total_train_loss: 0.641696 Total_test_loss: 0.590549 Total_BCE_test_loss: 0.517700 Total_KLD_test_loss: 0.003543 Total_CEP_test_loss: 0.069305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 15 total_train_loss: 0.634835 Total_test_loss: 0.589464 Total_BCE_test_loss: 0.516645 Total_KLD_test_loss: 0.003514 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 16 total_train_loss: 0.637767 Total_test_loss: 0.589715 Total_BCE_test_loss: 0.516857 Total_KLD_test_loss: 0.003551 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 17 total_train_loss: 0.634545 Total_test_loss: 0.591168 Total_BCE_test_loss: 0.518371 Total_KLD_test_loss: 0.003491 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 18 total_train_loss: 0.636540 Total_test_loss: 0.590711 Total_BCE_test_loss: 0.517917 Total_KLD_test_loss: 0.003488 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 19 total_train_loss: 0.636434 Total_test_loss: 0.590334 Total_BCE_test_loss: 0.517542 Total_KLD_test_loss: 0.003488 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 20 total_train_loss: 0.636636 Total_test_loss: 0.590048 Total_BCE_test_loss: 0.517240 Total_KLD_test_loss: 0.003502 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 21 total_train_loss: 0.637174 Total_test_loss: 0.590333 Total_BCE_test_loss: 0.517502 Total_KLD_test_loss: 0.003526 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 22 total_train_loss: 0.641578 Total_test_loss: 0.589411 Total_BCE_test_loss: 0.516590 Total_KLD_test_loss: 0.003515 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 23 total_train_loss: 0.634014 Total_test_loss: 0.590163 Total_BCE_test_loss: 0.517355 Total_KLD_test_loss: 0.003501 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 24 total_train_loss: 0.637480 Total_test_loss: 0.589467 Total_BCE_test_loss: 0.516634 Total_KLD_test_loss: 0.003528 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 25 total_train_loss: 0.636223 Total_test_loss: 0.589244 Total_BCE_test_loss: 0.516423 Total_KLD_test_loss: 0.003515 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 26 total_train_loss: 0.637956 Total_test_loss: 0.590072 Total_BCE_test_loss: 0.517270 Total_KLD_test_loss: 0.003497 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 27 total_train_loss: 0.634785 Total_test_loss: 0.590411 Total_BCE_test_loss: 0.517595 Total_KLD_test_loss: 0.003510 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 28 total_train_loss: 0.639815 Total_test_loss: 0.590075 Total_BCE_test_loss: 0.517232 Total_KLD_test_loss: 0.003537 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 29 total_train_loss: 0.635544 Total_test_loss: 0.589938 Total_BCE_test_loss: 0.517099 Total_KLD_test_loss: 0.003534 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 30 total_train_loss: 0.636961 Total_test_loss: 0.589513 Total_BCE_test_loss: 0.516639 Total_KLD_test_loss: 0.003569 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 31 total_train_loss: 0.635186 Total_test_loss: 0.590823 Total_BCE_test_loss: 0.517966 Total_KLD_test_loss: 0.003552 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 32 total_train_loss: 0.637336 Total_test_loss: 0.589131 Total_BCE_test_loss: 0.516276 Total_KLD_test_loss: 0.003549 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 33 total_train_loss: 0.634900 Total_test_loss: 0.590941 Total_BCE_test_loss: 0.518106 Total_KLD_test_loss: 0.003530 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 34 total_train_loss: 0.634876 Total_test_loss: 0.590472 Total_BCE_test_loss: 0.517660 Total_KLD_test_loss: 0.003505 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 35 total_train_loss: 0.638096 Total_test_loss: 0.590225 Total_BCE_test_loss: 0.517440 Total_KLD_test_loss: 0.003479 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 36 total_train_loss: 0.635956 Total_test_loss: 0.590351 Total_BCE_test_loss: 0.517554 Total_KLD_test_loss: 0.003490 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 37 total_train_loss: 0.636129 Total_test_loss: 0.590054 Total_BCE_test_loss: 0.517290 Total_KLD_test_loss: 0.003458 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 38 total_train_loss: 0.637111 Total_test_loss: 0.589985 Total_BCE_test_loss: 0.517192 Total_KLD_test_loss: 0.003488 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 39 total_train_loss: 0.635938 Total_test_loss: 0.589801 Total_BCE_test_loss: 0.517032 Total_KLD_test_loss: 0.003467 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 40 total_train_loss: 0.638719 Total_test_loss: 0.589894 Total_BCE_test_loss: 0.517078 Total_KLD_test_loss: 0.003513 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 41 total_train_loss: 0.634884 Total_test_loss: 0.590452 Total_BCE_test_loss: 0.517676 Total_KLD_test_loss: 0.003471 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 42 total_train_loss: 0.636636 Total_test_loss: 0.590140 Total_BCE_test_loss: 0.517358 Total_KLD_test_loss: 0.003477 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 43 total_train_loss: 0.639327 Total_test_loss: 0.589545 Total_BCE_test_loss: 0.516767 Total_KLD_test_loss: 0.003475 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 44 total_train_loss: 0.634113 Total_test_loss: 0.589243 Total_BCE_test_loss: 0.516421 Total_KLD_test_loss: 0.003516 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 45 total_train_loss: 0.642473 Total_test_loss: 0.589028 Total_BCE_test_loss: 0.516240 Total_KLD_test_loss: 0.003486 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 46 total_train_loss: 0.636718 Total_test_loss: 0.588960 Total_BCE_test_loss: 0.516187 Total_KLD_test_loss: 0.003470 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 47 total_train_loss: 0.638055 Total_test_loss: 0.589182 Total_BCE_test_loss: 0.516383 Total_KLD_test_loss: 0.003495 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 48 total_train_loss: 0.639763 Total_test_loss: 0.589799 Total_BCE_test_loss: 0.516950 Total_KLD_test_loss: 0.003546 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 49 total_train_loss: 0.636866 Total_test_loss: 0.590126 Total_BCE_test_loss: 0.517291 Total_KLD_test_loss: 0.003530 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 50 total_train_loss: 0.637084 Total_test_loss: 0.589140 Total_BCE_test_loss: 0.516323 Total_KLD_test_loss: 0.003513 Total_CEP_test_loss: 0.069304\n",
      "5e-06\n",
      "====> Epoch: 1 total_train_loss: 0.635569 Total_test_loss: 0.588544 Total_BCE_test_loss: 0.515731 Total_KLD_test_loss: 0.003507 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 2 total_train_loss: 0.642373 Total_test_loss: 0.588888 Total_BCE_test_loss: 0.516068 Total_KLD_test_loss: 0.003515 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 3 total_train_loss: 0.636851 Total_test_loss: 0.589050 Total_BCE_test_loss: 0.516233 Total_KLD_test_loss: 0.003515 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 4 total_train_loss: 0.637391 Total_test_loss: 0.589415 Total_BCE_test_loss: 0.516584 Total_KLD_test_loss: 0.003526 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 5 total_train_loss: 0.635484 Total_test_loss: 0.590097 Total_BCE_test_loss: 0.517234 Total_KLD_test_loss: 0.003556 Total_CEP_test_loss: 0.069307\n",
      "====> Epoch: 6 total_train_loss: 0.634155 Total_test_loss: 0.590035 Total_BCE_test_loss: 0.517189 Total_KLD_test_loss: 0.003540 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 7 total_train_loss: 0.635841 Total_test_loss: 0.589758 Total_BCE_test_loss: 0.516924 Total_KLD_test_loss: 0.003529 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 8 total_train_loss: 0.633792 Total_test_loss: 0.589532 Total_BCE_test_loss: 0.516723 Total_KLD_test_loss: 0.003503 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 9 total_train_loss: 0.636403 Total_test_loss: 0.589871 Total_BCE_test_loss: 0.517062 Total_KLD_test_loss: 0.003503 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 10 total_train_loss: 0.639802 Total_test_loss: 0.589730 Total_BCE_test_loss: 0.516898 Total_KLD_test_loss: 0.003529 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 11 total_train_loss: 0.632406 Total_test_loss: 0.589395 Total_BCE_test_loss: 0.516555 Total_KLD_test_loss: 0.003537 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 12 total_train_loss: 0.632932 Total_test_loss: 0.590415 Total_BCE_test_loss: 0.517621 Total_KLD_test_loss: 0.003488 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 13 total_train_loss: 0.641827 Total_test_loss: 0.589844 Total_BCE_test_loss: 0.517047 Total_KLD_test_loss: 0.003492 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 14 total_train_loss: 0.636658 Total_test_loss: 0.589341 Total_BCE_test_loss: 0.516584 Total_KLD_test_loss: 0.003452 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 15 total_train_loss: 0.635676 Total_test_loss: 0.589816 Total_BCE_test_loss: 0.517050 Total_KLD_test_loss: 0.003461 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 16 total_train_loss: 0.638393 Total_test_loss: 0.589880 Total_BCE_test_loss: 0.517079 Total_KLD_test_loss: 0.003495 Total_CEP_test_loss: 0.069306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 17 total_train_loss: 0.638170 Total_test_loss: 0.589877 Total_BCE_test_loss: 0.517047 Total_KLD_test_loss: 0.003524 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 18 total_train_loss: 0.637261 Total_test_loss: 0.589696 Total_BCE_test_loss: 0.516883 Total_KLD_test_loss: 0.003508 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 19 total_train_loss: 0.639923 Total_test_loss: 0.588863 Total_BCE_test_loss: 0.516049 Total_KLD_test_loss: 0.003509 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 20 total_train_loss: 0.640611 Total_test_loss: 0.589226 Total_BCE_test_loss: 0.516386 Total_KLD_test_loss: 0.003538 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 21 total_train_loss: 0.636216 Total_test_loss: 0.589446 Total_BCE_test_loss: 0.516617 Total_KLD_test_loss: 0.003524 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 22 total_train_loss: 0.637791 Total_test_loss: 0.589744 Total_BCE_test_loss: 0.516932 Total_KLD_test_loss: 0.003508 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 23 total_train_loss: 0.634567 Total_test_loss: 0.588900 Total_BCE_test_loss: 0.516086 Total_KLD_test_loss: 0.003511 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 24 total_train_loss: 0.641284 Total_test_loss: 0.589856 Total_BCE_test_loss: 0.517013 Total_KLD_test_loss: 0.003538 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 25 total_train_loss: 0.641396 Total_test_loss: 0.589730 Total_BCE_test_loss: 0.516920 Total_KLD_test_loss: 0.003508 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 26 total_train_loss: 0.637708 Total_test_loss: 0.589230 Total_BCE_test_loss: 0.516396 Total_KLD_test_loss: 0.003530 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 27 total_train_loss: 0.635816 Total_test_loss: 0.589126 Total_BCE_test_loss: 0.516248 Total_KLD_test_loss: 0.003576 Total_CEP_test_loss: 0.069302\n",
      "====> Epoch: 28 total_train_loss: 0.632604 Total_test_loss: 0.589979 Total_BCE_test_loss: 0.517136 Total_KLD_test_loss: 0.003537 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 29 total_train_loss: 0.636112 Total_test_loss: 0.589300 Total_BCE_test_loss: 0.516431 Total_KLD_test_loss: 0.003564 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 30 total_train_loss: 0.634980 Total_test_loss: 0.590654 Total_BCE_test_loss: 0.517803 Total_KLD_test_loss: 0.003544 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 31 total_train_loss: 0.638345 Total_test_loss: 0.590467 Total_BCE_test_loss: 0.517639 Total_KLD_test_loss: 0.003523 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 32 total_train_loss: 0.636414 Total_test_loss: 0.589604 Total_BCE_test_loss: 0.516785 Total_KLD_test_loss: 0.003516 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 33 total_train_loss: 0.638048 Total_test_loss: 0.589632 Total_BCE_test_loss: 0.516790 Total_KLD_test_loss: 0.003538 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 34 total_train_loss: 0.639306 Total_test_loss: 0.589133 Total_BCE_test_loss: 0.516318 Total_KLD_test_loss: 0.003509 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 35 total_train_loss: 0.637821 Total_test_loss: 0.589376 Total_BCE_test_loss: 0.516570 Total_KLD_test_loss: 0.003502 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 36 total_train_loss: 0.634558 Total_test_loss: 0.589638 Total_BCE_test_loss: 0.516837 Total_KLD_test_loss: 0.003496 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 37 total_train_loss: 0.632698 Total_test_loss: 0.590057 Total_BCE_test_loss: 0.517253 Total_KLD_test_loss: 0.003500 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 38 total_train_loss: 0.639360 Total_test_loss: 0.589741 Total_BCE_test_loss: 0.516937 Total_KLD_test_loss: 0.003500 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 39 total_train_loss: 0.639664 Total_test_loss: 0.589174 Total_BCE_test_loss: 0.516316 Total_KLD_test_loss: 0.003554 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 40 total_train_loss: 0.638441 Total_test_loss: 0.588967 Total_BCE_test_loss: 0.516115 Total_KLD_test_loss: 0.003548 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 41 total_train_loss: 0.635803 Total_test_loss: 0.589344 Total_BCE_test_loss: 0.516519 Total_KLD_test_loss: 0.003520 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 42 total_train_loss: 0.633987 Total_test_loss: 0.589289 Total_BCE_test_loss: 0.516442 Total_KLD_test_loss: 0.003541 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 43 total_train_loss: 0.636714 Total_test_loss: 0.589887 Total_BCE_test_loss: 0.517098 Total_KLD_test_loss: 0.003483 Total_CEP_test_loss: 0.069306\n",
      "====> Epoch: 44 total_train_loss: 0.642050 Total_test_loss: 0.589505 Total_BCE_test_loss: 0.516677 Total_KLD_test_loss: 0.003525 Total_CEP_test_loss: 0.069304\n",
      "====> Epoch: 45 total_train_loss: 0.638564 Total_test_loss: 0.589900 Total_BCE_test_loss: 0.517062 Total_KLD_test_loss: 0.003533 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 46 total_train_loss: 0.637098 Total_test_loss: 0.590973 Total_BCE_test_loss: 0.518171 Total_KLD_test_loss: 0.003499 Total_CEP_test_loss: 0.069303\n",
      "====> Epoch: 47 total_train_loss: 0.636807 Total_test_loss: 0.590497 Total_BCE_test_loss: 0.517689 Total_KLD_test_loss: 0.003503 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 48 total_train_loss: 0.639594 Total_test_loss: 0.589385 Total_BCE_test_loss: 0.516560 Total_KLD_test_loss: 0.003521 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 49 total_train_loss: 0.635238 Total_test_loss: 0.589822 Total_BCE_test_loss: 0.517013 Total_KLD_test_loss: 0.003504 Total_CEP_test_loss: 0.069305\n",
      "====> Epoch: 50 total_train_loss: 0.636628 Total_test_loss: 0.589638 Total_BCE_test_loss: 0.516825 Total_KLD_test_loss: 0.003507 Total_CEP_test_loss: 0.069306\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "if model_tobe_trained:\n",
    "    lr=1e-2\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=100,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=200,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "    lr=1e-3\n",
    "    print(lr)\n",
    "    #obj.model_training(epochs=70,learning_rate=lr)\n",
    "\n",
    "    lr=5e-4\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    obj1.model_save(address=save_address+\".pt\")\n",
    "    obj1.save_residuals(address=save_address+'_residuals.pkl')\n",
    "\n",
    "    lr=1e-5\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n",
    "\n",
    "    lr=5e-6\n",
    "    print(lr)\n",
    "    obj1.model_training(epochs=50,learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5tsLzrBNa7E"
   },
   "source": [
    "# Save The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42YFEvnqbE9U",
    "outputId": "555826a8-f613-4331-b6a5-1acc5a1b82a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running the neural network\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "##########\n",
    "print(\"running the neural network\")\n",
    "#run(obj1,save_address)\n",
    "obj1.model_save(address=save_address+\".pt\")\n",
    "obj1.save_residuals(address=save_address+'_residuals.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUnPPyZ6NfDr"
   },
   "source": [
    "# Visualize Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "DStavVSXYRs5",
    "outputId": "047b7d45-2f98-4854-9ea2-17f3781d9842"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obj1.plot_residuals(init_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkjkH6AjN5pq"
   },
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DBW6HgBINu9v"
   },
   "outputs": [],
   "source": [
    "from ci_vae import ivae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "#import umap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqNZPHbPOkoh",
    "outputId": "35686f32-9d60-4ae3-b7b6-a634658bb1b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of the code\n"
     ]
    }
   ],
   "source": [
    "print(\"start of the code\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "##############################################################   \n",
    "##############################################################\n",
    "model_init=True\n",
    "model_tobe_trained=False\n",
    "\n",
    "model_init=True\n",
    "model_file_address='./bb.pt'\n",
    "save_address1=\"./\"\n",
    "\n",
    "df_XY=pd.read_csv('df_XY.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oDqQbsnOO6d"
   },
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "fHFjOqpnigK_"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'dict' as child module 'model' (torch.nn.Module or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d2346c1690b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_initialiaze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bb.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/ci_vae/example/ci_vae/ivae.py\u001b[0m in \u001b[0;36mmodel_load\u001b[0;34m(self, address)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;31m#self.model_initialiaze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m#self.model.load_state_dict(torch.load(address))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1298\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                     raise TypeError(\"cannot assign '{}' as child module '{}' \"\n\u001b[0m\u001b[1;32m   1301\u001b[0m                                     \u001b[0;34m\"(torch.nn.Module or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                                     .format(torch.typename(value), name))\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'dict' as child module 'model' (torch.nn.Module or None expected)"
     ]
    }
   ],
   "source": [
    "obj2 = ivae.IVAE(df_XY = df_XY,\n",
    "               reconst_coef = reconst_coef,\n",
    "               latent_size = 10,\n",
    "               kl_coef = kl_coef,\n",
    "               classifier_coef = classifier_coef,\n",
    "               test_ratio = 1)\n",
    "\n",
    "obj2.model_initialiaze()\n",
    "\n",
    "obj2.model_load(address=\"bb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOUIzmGTOTyG"
   },
   "source": [
    "## Print the Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-H3bybRp484",
    "outputId": "fce40859-ea30-4f75-b4b5-08284301f7cf"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bdbf25a41d0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "for param in obj2.model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi3yqTDIOoui"
   },
   "source": [
    "# Make Prediction of All Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHcer1BPikHd",
    "outputId": "f72b5c62-5fd7-438d-818f-47f5dddaf3a6"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4bbbafba90e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#obj1.load_residuals(address='bb_residuals.pkl')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "\n",
    "    obj2.load_residuals(address='bb_residuals.pkl')\n",
    "    print(\"model loaded\")\n",
    "    \n",
    "    obj2.generate_test_results()\n",
    "    print(\"test data generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qaKllEltPf16"
   },
   "source": [
    "# Comprehensive Checking of The Prediction Values vs. True Values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BK8l95VcvpJt",
    "outputId": "470c82e6-2b9a-4903-eaa1-732e46bd84c1"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IVAE' object has no attribute 'x_last'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4d1c66f17ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_last\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1266\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'IVAE' object has no attribute 'x_last'"
     ]
    }
   ],
   "source": [
    "print(obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9ZitDT26ZXW",
    "outputId": "de963c21-10fd-4515-b6b5-b9a06eb3bd46"
   },
   "outputs": [],
   "source": [
    "print(obj2.x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ENQ8rtRHInw",
    "outputId": "959af6be-4ab0-4150-e843-b0d2297efc30"
   },
   "outputs": [],
   "source": [
    "(np.abs(obj2.x_pred - obj2.x_last)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJJfodLhQmrD",
    "outputId": "1bc5cb45-60ba-4bc1-c812-fd51c64c41ef"
   },
   "outputs": [],
   "source": [
    "(obj2.x_pred-obj2.x_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3q3XVFEvsEQ",
    "outputId": "4add0237-b89d-4803-983f-2d35cad6599d"
   },
   "outputs": [],
   "source": [
    "print(obj2.y_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Opj3sBJX6cQH",
    "outputId": "daedae9e-1edb-4243-efa3-e54976aaee6c"
   },
   "outputs": [],
   "source": [
    "print(obj2.y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "N-n9_o8nQxBa",
    "outputId": "08c51321-bbdc-4302-8c5d-c97c8700b590"
   },
   "outputs": [],
   "source": [
    "df_XY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2Yzu9Eiis50",
    "outputId": "d909e107-e05b-41a0-f075-c48541287153"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    obj2.model.eval()\n",
    "    for x, y in obj2.testloader:\n",
    "      x = x.to(device)\n",
    "      print(x.size())\n",
    "      print(x)\n",
    "      # forward\n",
    "      x_hat,y_hat, mu, logvar,z = obj2.model(x)\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(x_hat.cpu().detach().numpy(), columns=obj2.df_XY.drop(columns=['Y']).columns)\n",
    "    print(df_reconstructed.shape)\n",
    "    df_latent=pd.DataFrame(z.cpu().detach().numpy())\n",
    "    \n",
    "    obj2.model.eval()\n",
    "    \n",
    "    df_reconstructed_decoder=pd.DataFrame(obj2.model.decoder(z).cpu().detach().numpy(), columns=obj2.df_XY.drop(columns=['Y']).columns)\n",
    "\n",
    "    df_reconstructed.to_csv('df_reconstructed.csv')\n",
    "    df_latent.to_csv('df_latent.csv')\n",
    "    df_reconstructed_decoder.to_csv('df_reconstructed_decoder.csv')\n",
    "    print(\"Full_data_reconstructed...\")\n",
    "    \n",
    "    print(\"========df_reconstructed========\")\n",
    "    print(df_reconstructed)\n",
    "    print(\"========df_reconstructed_decoder========\")\n",
    "    print(df_reconstructed_decoder)\n",
    "    print(\"========df_Original========\")\n",
    "    print(df_XY)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CciNZOW_Rc0n"
   },
   "source": [
    "# Checking Linear Separability of Data on Lower Dimensioanl Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHqfONTOlqSr",
    "outputId": "6a2fa0ee-b52d-49dd-f55a-f6cdae3ee20e"
   },
   "outputs": [],
   "source": [
    "print(\"regression analysis\")\n",
    "obj2.regression_analysis(obj2.zs,df_XY['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBQlR5KERlL-"
   },
   "source": [
    "# Visualize Data on Lower Dimensional Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SXZtQfoj93-"
   },
   "outputs": [],
   "source": [
    "print(\"calculate tsne_umap_pca\")\n",
    "tsne_mat,umap_mat,pca_mat,Y=obj2.calculate_lower_dimensions(obj2.zs,obj2.y_last,N=100)\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='3d',save_str='tsne3d.pdf')\n",
    "obj2.plot_lower_dimension(tsne_mat,Y,projection='2d',save_str='tsne2d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='3d',save_str='umap3d.pdf')\n",
    "obj2.plot_lower_dimension(umap_mat,Y,projection='2d',save_str='umap2d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='3d',save_str='pca3d.pdf')\n",
    "obj2.plot_lower_dimension(pca_mat,Y,projection='2d',save_str='pca2d.pdf')\n",
    "\n",
    "print(\"finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76ULT6UtRxU6"
   },
   "source": [
    "# Perform Interpolation across all groups (Y) and all features from YY=0 to YY=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dXtgbd1iJ0s",
    "outputId": "4710be7c-960f-4184-bdb5-931a5699dd5a"
   },
   "outputs": [],
   "source": [
    "ff = obj2.traversal_all_groups(traversal_step=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oyAzr8KSAgH"
   },
   "source": [
    "# See the interpolation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWzpnwK0g-8g"
   },
   "outputs": [],
   "source": [
    "with open('results_dict.pkl', 'rb') as f:\n",
    "    ff = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "p23aMsKEknL4",
    "outputId": "29141142-bd3d-4f66-c936-9479312bcba9"
   },
   "outputs": [],
   "source": [
    "ff['med']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hMMGIGAslEoz",
    "outputId": "52e9d86d-dcb8-46c1-dfbf-1866da50861b"
   },
   "outputs": [],
   "source": [
    "ff['mean']['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "T7sydB3ISVvE",
    "outputId": "36818219-d221-43b4-aa92-fb45eb31fb7a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "qHpnnAoxoy_r",
    "outputId": "cc5504d0-001b-4b5d-afc1-d01f408db930"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['mean']['1']['B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "kR4GHdxyqCxn",
    "outputId": "f9387ed5-758f-41db-bd4a-9043c31ccb4a"
   },
   "outputs": [],
   "source": [
    "plt.plot(ff['med']['0']['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "l92UJyGZSMYB",
    "outputId": "5632470a-2575-41d5-9877-1335947b8d45"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ff['mean']['0']['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHBvdSESFRV"
   },
   "source": [
    "# Generate Synthetic Data for a Given Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ_3NAgi2-6L",
    "outputId": "a11c8fd5-41de-40d3-dee2-91580cb322a9"
   },
   "outputs": [],
   "source": [
    "bb = obj2.synthetic_single_group(group_id=0,nr_of_synthetic=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E330szfA6BD7",
    "outputId": "a8687ace-e5b6-46d2-904d-e854e532d0d8"
   },
   "outputs": [],
   "source": [
    "bb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey_yS1WZ3Os2",
    "outputId": "dbeb2f28-2746-4a69-beeb-a934f19eb8a1"
   },
   "outputs": [],
   "source": [
    "bb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
